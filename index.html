<!DOCTYPE html>
<html lang="ko">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>Appocrypha</title>
  <meta name="description" content="store limitless knowledges">
  

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="https://hrmrzizon.github.io/">
  
  
  <link rel="alternate" type="application/rss+xml" title="Appocrypha" href="https://hrmrzizon.github.io/feed.xml">

  <meta name="google-site-verification" content="f8AsB97UC1pp_K4EwkRaJYJZVC0P4j36RK_TGOPPGAU" />


  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:title" content="Appocrypha">
  <meta name="twitter:description" content="store limitless knowledges">
  
  

  <script type="text/javascript">
  WebFontConfig = {
    google: { families: [ 'Bitter:400,700,400italic:latin' ] }
  };
  (function() {
    var wf = document.createElement('script');
    wf.src = ('https:' == document.location.protocol ? 'https' : 'http') +
      '://ajax.googleapis.com/ajax/libs/webfont/1/webfont.js';
    wf.type = 'text/javascript';
    wf.async = 'true';
    var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(wf, s);
  })();
</script>

  
  <!-- Google Analytics -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-93000374-1', 'auto');
    ga('send', 'pageview');

  </script>



</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Appocrypha</a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/">About</a>
      
        
        <a class="page-link" href="/archives/">Archives</a>
      
        
        <a class="page-link" href="/edu">Edu</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home">

  

  

  <ul class="post-list">
    
      
      

      <li>
        <header class="post-header">
          <h1 class="post-title">
            <a class="post-link" href="/2018/05/27/how-to-represent-transparency-object/">How To Represent Transparency Object</a>
          </h1>

          <p class="post-meta">May 27, 2018 • 
  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/rendering/">rendering</a>,
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/transparency/">transparency</a>
    
  

</p>
        </header>

        <div class="post-content">
          <p>오래전부터 게임같은 실시간으로 안정적인 성능을 뽑아내야하는 컴퓨터 그래픽에서의 투명한 물체는 항상 골칫거리였다. “투명” 하다보니 일반적으로 사용되는 최적화 방법도 사용할 수 없기 때문에 퍼포먼스의 문제가 있으며, 일반적으로 지원하는 <em>Alpha Blending</em> 을 사용할시에는 물체의 순서를 직접 소팅해주어야 했다. 투명한 물체를 그리는 일반적인 방법의 문제에 대해서 알아보고, 문제를 부분적으로 해결할 수 있는 몇가지 방법들을 적어보겠다.</p>

<p>일반적으로 아무런 방법없이 투명한 그리는 방법은 뒤에있는 물체부터 순서대로 그리는 것이다. 이를 형상화 시키면 아래 그림과 같다.</p>

<p><img src="/images/over_operator_draw_order.png" alt="over operator" class="center-image" /></p>

<p>그림에서는 아래부터 위쪽에 있는 문체를 순서대로 그린다. 파란색, 빨간색, 연두색 기준으로 그리게 된다. 그렇게 되면 이전에 그려진 결과와 오브젝트의 결과가 합쳐져서 표현되게 된다. 이는 다음과 같은 식으로 표현된다.</p>

<p><br />
<img src="/images/over-operator_polynomial.png" alt="over-operator polynomial" class="center-image" /></p>
<center>출처 : <a href="http://cwyman.org/supplement/oitContinuum/2016_HPG_ExpandingOITContinuum.pdf">"Exploring and Expanding the Continuum of OIT Algorithms," HPG 2016.</a>
</center>
<p><br /></p>

<p><em>a0,c0</em> 는 픽셀에서 출력될 <em>Alpha</em> 와 <em>Color</em> 를 의미한다. 그 뒤의 항에서 <em>a1,c1</em> 은 이전에 그려진 결과를 뜻한다. 이런 방식을 <em>“over”</em> 연산이라고 말한다. 이런 방식은 나중에 그려진 색이 조금 더 강조되고, 이전에 그려지는 색의 존재가 조금씩 흐려지는 것을 알 수 있다.</p>

<p>이 방법의 문제는 그려지는 순서를 직접 맞춰줘야 하는 것이다. 이는 굉장히 짜증나는 문제로, 단순한 정렬만으로는 세세한 부분의 문제를 해결할 수 없다. 아래 그림은 해당 문제를 잘 나타내어 준다.</p>

<p><br />
<img src="/images/shared-OIT-example-animation_lumberyard.gif" alt="Lumberyard : OIT example animation" class="center-image" /></p>
<center>출처 : <a href="https://docs.aws.amazon.com/ko_kr/lumberyard/latest/userguide/graphics-rendering-order-independent-transparency.html">lumberyard : OIT(Order Independent Transparency)</a>
</center>
<p><br /></p>

<p>상단 두개의 그림이 일반적인 <em>“over”</em> 연산을 한 방법이다. 정렬되지 않아 미리 정해진 순서대로 출력되며 물체가 회전하면 공간상의 순서가 바뀌므로 상단 두개의 그림같이 출력된다. 그래서 이런 <em>Depth</em> 의 정렬 문제 때문에 만들어진 용어가 있다.</p>

<h2>Order Independent Transparency(OIT)</h2>

<p><em>OIT</em>, 순서에 구애받지 않는 투명물체를 그리는 방법을 통칭하는 용어다. 대부분의 게임에서는 <em>OIT</em> 를 사용하여 투명물체를 그린다. 많이 알려진 방식은 두가지가 있다. <em>Depth Peeling</em> 과 <em>Weighted Blended OIT</em> 다.</p>

<p><em>Depth Peeling</em> 은 <em>Detph</em> 를 사용하여 카메라로부터 가장 가까운(<em>Depth</em> 값이 가장 작은) 표면의 색을 누적시키는 <em>OIT</em> 기법이다. 직역하자면 “<em>Depth</em> 껍질 벗기기” 인데, 조금 더 자세히 알아보자.</p>

<p><br />
<img src="/images/OIT_Lab_DepthPeeling.gif" alt="OIT-Lab DepthPeeling" class="center-image" /></p>
<center>출처 : <a href="https://github.com/candycat1992/OIT_Lab">Github : candycat1992 / OIT_Lab</a>
</center>
<p><br /></p>

<p><em>Depth Peeling</em> 은 총 3단계로 이루어진다. 맨 처음에는 <em>Alpha</em> 값에 관계없이 <em>Depth Test</em> 를 작거나 같은값에서 되게 해두고, <em>Depth Write</em> 가 가능하게 해준다. 그리고 그려준다. 그렇게 되면 카메라에서 가장 가까운 표면들이 처음에 그려지게 되고, 해당 상태에서 그려진 표면의 <em>Depth</em> 값들이 <em>Depth Buffer</em> 에 저장된다. 즉 처음에 그려진 결과를 가진 색들의 집합과 <em>Detph</em> 의 집합을 가지고 시작한다.</p>

<p>이제 비슷한 그리기를 계속 반복한다. 어떻게 반복하냐면, 두가지의 <em>Depth Test</em> 를 사용해야 한다. 일단 처음과 같은 작거나 같은(<em>LessEqual</em>) <em>Depth Test</em> 를 계속 해주고, 여기서 추가적으로 이전에 했던 <em>Depth Buffer</em> 의 값을 가져와 출력되는 <em>Depth</em> 값이 작거나 같을 때 픽셀의 결과를 누락시킨다.(<em>Discard</em>) 즉 이전에 출력한 표면을 <em>Depth Test</em> 로 누락시키고 그 뒤에서 가장 <em>Depth</em> 값이 작은 것들을 출력한다. 이때 <em>Depth</em> 값은 계속 기록해야한다. 아래 그림을 보자.</p>

<p><br />
<img src="/images/depthpeeling-crosssection.png" alt="Depth Peeling Cross-Section" class="center-image" /></p>
<center>출처 : <a href="http://developer.download.nvidia.com/assets/gamedev/docs/OrderIndependentTransparency.pdf">NVidia : Order-Independent Transparency</a>
</center>
<p><br /></p>

<p>맨 처음에 했던 결과가 <em>Layer 0</em> 의 결과이고, 두번째 과정을 반복하면 뒷부분을 계속 출력하게 된다. 그래서 임의의 횟수만큼 이와 같은 과정을 계속 반복해준다. 그러면 두번쨰 과정, <em>Depth Peeling</em> 은 끝이다.</p>

<p>다음은 임의의 횟수만큼 <em>Depth Peeling</em> 을 해서 출력된 색의 결과와 <em>Depth Buffer</em> 를 가지고 위에서 언급한 일반적인 <em>Alpha Blending</em> 의 과정인 <em>“over”</em> 연산을 해주면 된다. 순서는 가장 뒤에있는 결과, 마지막에 출력한 결과부터 처음 출력한 결과, 카메라에서 가장 가까운 결과까지 순서대로 <em>Layer</em> 를 <em>“over”</em> 연산을 통해 결합시켜주면 된다. 아래에는 <em>Layer</em> 의 크기를 점점 증가시킨 결과를 애니메이션으로 보여주는 그림이다.</p>

<p><br />
<img src="/images/depthpeeling.gif" alt="OIT-Lab DepthPeeling Anim" class="center-image" /></p>
<center>출처 : <a href="https://github.com/candycat1992/OIT_Lab">Github : candycat1992 / OIT_Lab</a>
</center>
<p><br /></p>

<p>다른 방법에 비해 <em>Depth Peeling</em> 은 그다지 좋은 방법은 아니다. 들어가는 비용이 너무 크다. <em>Depth Peeling</em> 은 <em>Layer</em> 가 크면 클수록 더 많은 것들을 표현할 수가 있다. 하지만 <em>Layer</em> 의 갯수에 맞춰서 들어가는 메모리가 굉장히 크다. 우선 화면 해상도를 기준으로 색과 <em>Depth</em> 를 저장하는 버퍼를 여러개 만들어야 한다. 그리고 <em>Layer</em> 의 갯수만큼 <em>Geometry Pass</em>(<em>Vertex Shader</em>, <em>Tesselation</em>, <em>Geometry Shader</em>)를 반복한다. 이는 정점 데이터가 많으면 많을수록 크리티컬하다. 이런 여러가지의 단점이 있기 때문에 <em>Depth Peeling</em> 은 잘 쓰이지 않는것으로 보인다. 또한 <em>Shader</em> 코드내에서 <em>Depth Test</em> 를 해주게 되면 GPU 의 <em>Early-Z Test</em> 가 비활성화가 되기 때문에 퍼포먼스 손실도 있을 것이라고 예측된다.</p>

<p>그래서 <em>Depth Peeling</em> 보다는 나은 성능을 가진 <em>Weighted Blended OIT</em> 라는 방법이 어느정도 쓰이는 것으로 보인다. <em>Weighted Blended OIT</em> 는 2013년에 논문을 발표하였으며, 요즘에도 쓰이는 방법이다. GDC2018 에서 Agent of Mayhem 의 제작사가 <em>Weighted Blended OIT</em> 에 대한 내용을 발표했다.(<a href="https://www.dsvolition.com/publications/rendering-technology-in-agents-of-mayhem/">링크</a>) <em>Weighted Blended OIT</em> 는 2013년에 고안된 기법으로 <em>Depth Peeling</em> 보다는 덜 오래된 방법이다. <em>Weighted Blended OIT</em> 는 물체의 색을 표현할 떄, 특정한 가중치(<em>weight</em>)를 구해 곱해서 표현한다. 그리고 이전에 출력된 결과에 그대로 색값을 더해준다. 이제 자세한 방법에 대해서 알아보자.</p>

<p><br />
<img src="/images/weightedblendedOIT_formula.png" alt="wboit formula" class="center-image" /></p>
<center>출처 : <a href="http://jcgt.org/published/0002/02/09/">jcgt : Weighted Blended Order-Independent Transparency</a>
</center>
<p><br /></p>

<p>위 그림은 최종으로 나온 픽셀이 가지고 있는 색을 보여준다. <em>n</em> 은 해당 픽셀에 그려지는 갯수이고, <em>α</em> 는 <em>Alpha</em>, <em>C</em> 는 <em>Color</em>, <em>Z</em> 는 <em>Depth</em>, <em>C0</em> 는 투명 오브젝트를 그리기 전에 그려진 불투명한 표면의 색이다. 식만 봐서는 자세히는 모르기 때문에 아래 그림을 보면서 알아보자.</p>

<p><br />
<img src="/images/weightedblendedOIT_formula_explain.png" alt="wboit formula" class="center-image" /></p>
<center>출처 : <a href="http://jcgt.org/published/0002/02/09/">jcgt : Weighted Blended Order-Independent Transparency</a>
</center>
<p><br /></p>

<p>일단 총 3가지 단계로 나뉜다. <em>RGB * Weight</em>, <em>Alpha * Weight</em> 를 <em>RGBA</em> 로 저장해주는 누적값(<em>accumulate</em>)을 하나의 렌더 타겟에 저장하고, 얼마나 픽셀의 색이 덮는지에 대한 여부가 아닌, 반대로 이전의 색이 얼마만큼 노출이 될 수 있는지에 대한 노출값(<em>revealage</em>)을 다른 하나의 렌더 타겟에 저장한다. 그 다음 그 결과를 가지고 위의 식의 값을 계산한 결과가 <em>Weighted, Blended OIT</em> 의 끝이다. 다만 <em>accumulate</em> 와 <em>revealage</em> 값들은 <em>MRT</em> 를 통해서 <em>DrawCall</em> 을 단축시킬 수 있을 것 같다.</p>

<p><em>Weight</em> 값을 계산하는 방법은 여러가지가 있다. 가장 중요한 것은 <em>Weight</em> 를 계산할 떄, <em>Depth</em>, <em>Alpha</em> 이 두가지를 사용하여 계산하는 것이 가장 중요한 점이다. <em>Depth</em> 는 카메라를 기준으로 앞 뒤를 판별하는 중요한 기준이며, <em>Alpha</em> 값은 투명한 정도를 나타내는 중요한 값이다. <em>weight</em> 를 구하는 공식은 아래 그림을 참고하자.</p>

<p><br />
<img src="/images/weightedblendedOIT_weight_formula.png" alt="wboit formula" class="center-image" /></p>
<center>출처 : <a href="http://jcgt.org/published/0002/02/09/">jcgt : Weighted Blended Order-Independent Transparency</a>
</center>
<p><br /></p>

<table>
  <tbody>
    <tr>
      <td>_0.1 ≤</td>
      <td>z</td>
      <td>≤ 500_ 를 가정하고 만든 식이다. 이중에 <em>d(z)</em> 라는 식이 있는데 이는 아래를 보면된다.</td>
    </tr>
  </tbody>
</table>

<p><br />
<img src="/images/weightedblendedOIT_weight_formula_additional.png" alt="wboit formula" class="center-image" /></p>
<center>출처 : <a href="http://jcgt.org/published/0002/02/09/">jcgt : Weighted Blended Order-Independent Transparency</a>
</center>
<p><br /></p>

<p>이는 <em>GLSL</em> 기준으로, <em>HLSL</em> 에서 취급하는 <em>Depth</em> 의 범위가 달라지면 바뀔수도 있다. 식을 선택할 떄 중요한 점은 커브가 어떤식으로 생성되는지 보아야 한다. 아래 그림을 보자.</p>

<p><br />
<img src="/images/weightedblendedOIT_weight_curve.png" alt="wboit formula" class="center-image" /></p>
<center>출처 : <a href="http://jcgt.org/published/0002/02/09/">jcgt : Weighted Blended Order-Independent Transparency</a>
</center>
<p><br /></p>

<p>논문에 실려있던 그림이며, 각 식에 따른 <em>weight</em> 의 커브를 잘 보여준다. 어느 정도 지식이 있다면 보여주고 싶은 컨텐츠의 특성에 따라서 식을 변형을 할 수도 있겠다.</p>

<p><em>WBOIT</em> 의 장점은 굉장히 전통적인 방법으로 구현이 가능하기 때문에 어떠한 기계에도 사용할 수 있다는 장점이 있다. <em>Depth Peeling</em> 은 <em>MRT</em> 를 사용해서 해야하므로 <em>DX10+</em> 를 지원하는 디바이스부터 가능한 것에 비하면 굉장한 장점이다. <em>Occlusion Culling</em> 은 안되지만 거의 <em>O(n)</em> 의 속도와 많아봐야 2개의 해상도에 비례하는(<em>RGBA</em>, <em>R</em>) 메모리만 있으면 되기 때문에 투명 물체를 엄청나게 많이 출력하지 않는다면 일반적으로 쓸 수 있다.</p>

<p><em>WBOIT</em> 또한 단점이 몇가지 존재한다. 첫번째로 불투명한 물체와 함께 <em>“over”</em> 연산을 한다면 앞의 물체에 의해 가려지게 된다. 하지만 <em>WBOIT</em> 는 물체가 다른색을 표현할 경우에는 불투명한 물체를 표현하지 못한다. 그리고 <em>Depth</em> 값의 정밀도에 따라서 표현할 수 있는 투명한 물체의 사이의 거리가 달라진다. 아무리 식을 변형시킨다고 하더라도 부동소수점 정밀도의 문제가 생길수가 있다. 마지막으로 모든 오브젝트가 카메라의 <em>Z</em> 축을 따라서 움직인다면 출력값은 달라질 것이다. 이는 <em>weight</em> 계산에 <em>Depth</em> 값이 들어가서 이러한 결과를 보여주는데, 해당 논문에는 그다지 신경쓰지 않았다고 적혀있다.</p>

<p>하지만 위의 단점은 충분히 안고갈만한 단점이기 때문에 투명한 물체를 그릴 때는 <em>Weighted Blended OIT</em> 가 가장 나은 방법이라고 본다. 아래 <em>Alpha Blend, Detph Peeling, _Weighted Blended OIT</em> 이 세가지 방법에 대한 비교 영상이 있다.</p>

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class="embed-container">    <iframe title="YouTube video player" width="640" height="390" src="//www.youtube.com/embed/JVa9xXddgbM" frameborder="0" allowfullscreen=""></iframe></div>

<p>위에서 언급한 <a href="https://www.dsvolition.com/publications/rendering-technology-in-agents-of-mayhem/">Rendering Technology in ‘Agents of Mayhem’</a> PT 에서는 기존의 <em>WBOIT</em> 에서 <em>Emmisive</em> 한 물체를 위해 추가적인 화면 해상도에 비례하는 버퍼를 사용하여 빛나는 투명 물체를 표현하는 방법을 서술해놓았다. 또한 해당 논문의 저자가 1년전에 언급한 <a href="https://www.youtube.com/watch?v=jWe5Ae22Ffs&amp;t=555s"><em>Phenomenological Transparency</em></a> 라는 방법도 있다.</p>

<h2>참조</h2>

<ul>
  <li><a href="http://cwyman.org/papers.html">cwyman.org : “Exploring and Expanding the Continuum of OIT Algorithms,” HPG 2016.</a></li>
  <li><a href="http://developer.download.nvidia.com/assets/gamedev/docs/OrderIndependentTransparency.pdf">NVidia : Order-Independent Transparency</a></li>
  <li><a href="http://developer.download.nvidia.com/SDK/10.5/opengl/src/dual_depth_peeling/doc/DualDepthPeeling.pdf">NVidia : Order Independent Transparency with Dual Depth Peeling</a></li>
  <li><a href="http://jcgt.org/published/0002/02/09/">jcgt : Weighted Blended Order-Independent Transparency</a></li>
  <li><a href="https://github.com/candycat1992/OIT_Lab">Github : candycat1992 / OIT_Lab</a></li>
  <li><a href="https://www.dsvolition.com/publications/rendering-technology-in-agents-of-mayhem/">GDC2018 : Rendering Technology in ‘Agents of Mayhem’</a></li>
  <li><a href="https://www.youtube.com/watch?v=jWe5Ae22Ffs&amp;t=555s">Youtube : Phenomenological Transparency</a></li>
</ul>

        </div>

        
      </li>
    
      
      

      <li>
        <header class="post-header">
          <h1 class="post-title">
            <a class="post-link" href="/2018/05/22/opaque-as-alpha-test/">Opaque As Alpha Test</a>
          </h1>

          <p class="post-meta">May 22, 2018 • 
  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/rendering/">rendering</a>,
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/alphatest/">alphatest</a>,
    
  
    
  

  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/shader/">shader</a>
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

</p>
        </header>

        <div class="post-content">
          <p><em>Shader</em> 에서 샘플링하는 <em>Texutre</em> 에서 <em>Alpha</em> 값을 가지고 있어, <em>Alpha</em> 을 참조해서 실제 픽셀에 출력을 하는지 안하는지를 결정하는 것을 <em>Alpha Test</em> 라고 한다. 이런 <em>Material</em> 이나 <em>Texture</em> 를  <em>Cutout</em> 이라고 통칭하는 경우가 많다.</p>

<p>보통 게임에서의 <em>Alpha Test</em> 를 사용하는 것들은 나무, 풀 같은 식생들(<em>Vegetation</em>)이 있고, 중간에 구멍이 뚫린 펜스같은 것들도 존재한다. 자연을 배경으로하는 게임의 경우에는 식생들이 굉장히 많기 때문에 <em>Alpha Test</em> 를 사용하는 <em>Shader</em> 가 굉장히 많이 사용될 것이다.</p>

<p><br />
<img src="/images/1_8EKqWSOOPXaTrDHVFTACJg.png" alt="Wikipedia : Single-precision floating-point format" class="center-image" /></p>
<center>출처 : <a href="https://medium.com/@bgolus/anti-aliased-alpha-test-the-esoteric-alpha-to-coverage-8b177335ae4f">Anti-aliased Alpha Test: The Esoteric Alpha To Coverage</a>
</center>
<p><br /></p>

<p>하지만 <em>Alpha Test</em> 는 굉장히 큰 단점이 있다. 고정된 화면 해상도에서 물체가 작게 표현되면 물체를 표현할 수 있는 픽셀의 숫자가 많이 작아진다. 물체를 표현하는 픽셀의 수가 작아지게 되면 일반적으로 해당 넓이에 맞게 생성된 <em>Texture</em> 의 <em>Mip-level</em> 에 접근한다. 중간의 <em>Alpha Test</em> 그림을 보면된다.</p>

<p><br />
<img src="/images/1_zNbZFiJXjcqqyTkM9eEt7w.gif" alt="Wikipedia : Single-precision floating-point format" class="center-image" /></p>
<center>출처 : <a href="https://medium.com/@bgolus/anti-aliased-alpha-test-the-esoteric-alpha-to-coverage-8b177335ae4f">Anti-aliased Alpha Test: The Esoteric Alpha To Coverage</a>
</center>
<p><br /></p>

<p>실제로는 양 옆의 물체들처럼 자연스럽게 표현이 되야하지만 일반적인 <em>Alpha Test</em> 를 사용하게 되면 위와 같은 현상에 마주치게 된다. 이는 굉장히 끔찍한 현상이다. 실제 게임을 해보거나, 만들어본 사람이라면 안다. 대부분의 픽셀에 나무가 표현되고, 잎사귀들이 저런식으로 자글자글 거린다면 약간의 불쾌함이 느껴진다. VR 이라면 더욱..</p>

<p>그래서 급하게 대처방안으로 나온 것이 위 그림의 오른쪽에 나오는 <em>Alpha to Coverage</em> 라는 방법이다. 이는 하드웨어 <em>MSAA</em> 를 픽셀 쉐이더의 결과를 통해 자동으로 해주는것으로, <em>MSAA</em> 의 퍼포먼스와 비례한다. <em>MSAA</em> 는 성능이 영 좋지않아 안쓰는 경우가 꽤 많이 존재하기 때문에 <em>Alpha to Coverage</em> 는 절대적으로 사용할 수 있는 방법은 아니다. 게다가 엄청나게 많은 나무를 <em>Alpha to Coverage</em> 를 쓴다면.. 성능은 안봐도 뻔하다.</p>

<p>앞서 말한 <em>Alpha Test</em> 은 <em>Material</em>, <em>Shader</em> 별로 고정된 <em>Alpha</em> 값을 설정해 그 이하가 되면 <em>Pixel Shader</em> 에서 결과를 내놓지 않게 하는(<em>Discard</em>) 방법이였다. <em>Alpha Test</em> 의 문제는 샘플링한 <em>Alpha</em> 값이 가끔 극단적으로 낮아서 <em>Discard</em> 되는 것인데, 이를 간단하게 해결하기 위해 요상한 방법이 등장했다.</p>

<p>바로 <em>Stochastic test</em> 라는 방법이다.</p>

<p><br />
<img src="/images/stochastic_sampling.png" alt="NVidia deverloper : Hashed Alpha Testing" class="center-image" /></p>
<center>출처 : <a href="https://developer.download.nvidia.com/assets/gameworks/downloads/regular/GDC17/RealTimeRenderingAdvances_HashedAlphaTesting_GDC2017_FINAL.pdf?pUIX8DXxfad7mL4zB3GOthX3r5IgGao9UWxYuYb3q9h10RXrQeYko-dEuJXJxt1hhsI9J_9KJDcCYGeWWksxlaHTrXSE825D_3izja7LUFOtzhaeBUqpn7qbwXaaGlLdbipjE3PeI3e2IMn45mQAA3OV2PD-kG2y9cecTaWE2uum2uwdHgyn0nhYiLOvlOsrUzewbK5REH7vAm3-lNWzxehw_5Tphg">NVidia developer : Hashed Alpha Testing</a>
</center>
<p><br /></p>

<p>위 그림에서 위쪽에 있는 것이 일반적인 <em>Alpha Test</em> 인데, <em>color.a</em> 는 텍스쳐에서 샘플링한 <em>Alpha</em> 값, <em>ατ</em> 는 <em>Alpha Test</em> 를 위한 고정된 <em>Alpha Threshold</em>(<em>알파한계</em>)다. 밑의 코드에서 <em>drand48</em> 이 나타내는 것은 단순한 0 ~ 1 사이의 랜덤값이다. 즉 랜덤하게 <em>Alpha Threshold</em> 를 설정해주어 물체가 멀어져서 평균 <em>Alpha</em> 값이 낮아질 때도 픽셀이 <em>Discard</em> 되지 않도록 하는 것이다. 하지만 이는 굉장한 눈아픔? 반짝거림? 을 유발한다. 범위를 지정해주지 않았기 때문에 이전 프레임에서 출력된 픽셀이 다음 프레임에서는 출력되지 않을 수도 있다. 이렇게 각 프레임마다 상황이 달라서 생기는 현상앞에 <em>Temporal</em> 을 붙인다. <em>Stochastic Alpha Test</em> 의 문제는 <em>Temporal Flickering</em> 이라고 할 수 있겠다.</p>

<p><em>Temporal Flickering</em> 이 없는, <em>Temporal Stability</em>(임시적 안정성) 을 확보하기 위해서는 <em>Alpha Threshold</em> 를 이러저리 튀지 않게해야 했고, 이를 위해 특정 값에 따라서 <em>Hash</em> 값을 생성하는 방법이 고안되었다. 이 방법은 <em>Hashed Alpha Test</em> 라는 이름으로 작년에 공개되었다.</p>

<h2>Hashed Alpha Testing</h2>

<p>기본적으로 랜덤 값(난수) 생성은 제대로된 난수생성이 아닌, 특수한 식을 사용해서 의사 난수 생성 방법을 이용하는데, <em>Hash</em> 를 이용한 난수생성은 일반적으로 많이 쓰인다고 한다. <em>Hashed Alpha Testing</em> 은 <em>Hash</em> 를 생성하기 위한 <em>Key</em> 값을 선정하는데 조심스러웠다고 한다.</p>

<p><em>Key</em> 로 선정될 수 있는 후보는 <em>Texture Coordinate</em>, <em>World-Space Coordinate</em>, <em>Ojbect-Space Coordinate</em> 이 세가지 였다고 한다. <em>Texture Coordinate</em> 는 가끔 없는 경우가 있어 제외하였고, <em>World-Space Coordinate</em> 는 정적 물체에는 원하는대로 동작하지만, 동적 물체의 경우에는 문제가 있었다고 한다. 결국 남은건 <em>Ojbect-Space Coordinate</em> 가 남게 되었다.</p>

<p><em>Ojbect-Space Coordinate</em> 의 <em>X,Y,Z</em> 세 좌표를 모두 이용하게 되는데, 이는 <em>X,Y</em> 두개만 이용하게 되면 <em>Hash</em> 값이 <em>Screen-Space</em> 에서 생성되어 다른 물체와 겹치게 되면 <em>Alpha to Coverge</em> 같은 효과를 내게되어 3가지 좌표 모두 <em>Hash</em> 생성에 사용된다고 한다.</p>

<p>마지막으로 중요한 포인트는 <em>Temporal Stability</em> 를 확보하는 것이다. 이해하기 쉽게 설명하자면, 아래와 같은 각 픽셀을 나타내는 그리드안에 점이 있다고 가정해보자. 이 점들이 조금씩 움직여서 계속 픽셀안에 있다면, 같은 <em>Hash</em> 값을 사용하여 같은 <em>Alpha Threshold</em> 값을 만들어줘야 한다.</p>

<p><img src="/images/subpixel_0.png" alt="Subpixel 0" class="center-image" /></p>

<p>아래 두 그림의 빨간 점의 위치처럼 원래의 픽셀위치를 벗어나게 된다면 새로운 <em>Alpha Threshold</em> 를 생성해야 하겠지만, 위치가 많이 바뀌지 않는다면 같은 <em>Alpha Threshold</em> 를 사용해 <em>Flickering</em> 을 최대한 줄여야 한다.</p>

<p><img src="/images/subpixel_2.png" alt="Subpixel 2" class="center-image" /></p>

<p>이러한 맥락으로 <em>Hashed ALpha Testing</em> 은 <em>Temporal Stability</em> 를 조금 확보하게 된다. 물론 위의 그림은 이해를 돕기위한 용도로, 실제 코드상에서는 다른 방법을 통해 계산된다. 아래 코드를 보자.</p>

<p><br />
<img src="/images/hat_codesnippet_screenxy.png" alt="Hashed Alpha Testing" class="center-image" /></p>
<center>출처 : <a href="http://cwyman.org/papers/tvcg17_hashedAlphaExtended.pdf">Cwyman.org : Hashed Alpha Test(Extended)</a>
</center>
<p><br /></p>

<p>위 코드는 픽셀이 가지고 있는 <em>Object-Space Coordinate</em> 의 옆 픽셀과의 차이, 세로에 있는 픽셀과의 차이를 통한 값으로 계산한다. (dFdX, dFdY 의 자세한 내용은 찾아보거나 <a href="/2018/03/04/what-is-ddx-and-ddy/">What is ddx and ddy</a> 에서 볼 수 있다.) 픽셀별로 값의 차이, 즉 근접한 픽셀의 위치 차이값에 따른값(미분값)과 그 값을 이용해 <em>Object-Space Coordinate</em> 값에 곱한 값을 <em>Key</em> 로 두어서 <em>Alpha Threshold</em> 를 계산한다.</p>

<p>마지막에 <em>Alpha Threshold</em> 를 구하는 코드를 보면, <em>Floor</em> 하는, 올림을 해주어 <em>discrete value</em> 로 <em>Key</em> 값을 넣어준다. <em>Floor</em> 가 의미하는 것은, 선형적인 데이터가 아닌 뚝뚝 끊기는 데이터로 만들어 특정한 값을 넘어야 <em>Key</em> 값이 바뀌게 하여 <em>Hash</em> 를 유지해 <em>Flickering</em> 을 방지하는 것이다. 아래 그림은 <em>floor(x)</em> 의 그래프다. 즉 코드의 <em>pixScale</em> 이 크면 클수록 <em>Hash</em> 의 값은 픽셀의 변화에 따라서 빠르게 바뀌고, 작으면 작을수록(0에 가까워질수록) 픽셀의 변화에 따라서 <em>Hash</em> 값이 느리게 바뀔 것이다.</p>

<p><br />
<img src="/images/wolframalpha_floor.gif" alt="Woflram Alpha : Floor Graph" class="center-image" /></p>
<center>출처 : <a href="http://www.wolframalpha.com/input/?i=floor">Wolframalpha</a>
</center>
<p><br /></p>

<p>이러한 방법은 <em>View-Space</em> 를 기준으로 <em>X,Y</em> 좌표가 조금씩 바뀔때는 픽셀끼리의 차이를 계산하기 때문에 안정적이다. 하지만 <em>Z(Depth)</em> 값이 바뀔때는 많은 <em>Flickering</em> 을 일으킬 것이다. 이를 해결하기 위해 아래 코드를 보자.</p>

<p><br />
<img src="/images/hat_codesnippet_screenz0.png" alt="Hashed Alpha Testing" class="center-image" /></p>
<center>출처 : <a href="http://cwyman.org/papers/tvcg17_hashedAlphaExtended.pdf">Cwyman.org : Hashed Alpha Test(Extended)</a>
</center>
<p><br /></p>

<p>위치의 픽셀별 차이 벡터의 크기를 <em>discrete</em> 시키는 방법도 좋은 아이디어중 하나다. 하지만 이는 빌보드처럼 큰 크기의 판이 다가오게 된다면 끝부분의 <em>discontinuity</em> 를 유발하게 된다.</p>

<p><br />
<img src="/images/hat_codesnippet_screenz1.png" alt="Hashed Alpha Testing" class="center-image" />
<img src="/images/hat_codesnippet_lerpscale.png" alt="Hashed Alpha Testing" class="center-image" /></p>
<center>출처 : <a href="http://cwyman.org/papers/tvcg17_hashedAlphaExtended.pdf">Cwyman.org : Hashed Alpha Test(Extended)</a>
</center>
<p><br /></p>

<p>그래서 위 코드와 같 <em>discretize</em> 시킨 올림처리한 값과, 내림처리한 값을 사용한 두 <em>Hash</em> 값 사이의 보간을 통해서 <em>Alpha Threshold</em> 를 구해준다. 하지만 이 코드는 아직 문제점이 존재한다. 만약 <em>maxDeriv</em> 의 값이 0 ~ 1 사이라면 내림값이 반드시 0이 되기 때문에 보간할 값 중 한개의 값이 고정되게 된다. 그래서 아래와 같은 코드를 사용한다.</p>

<p><br />
<img src="/images/hat_codesnippet_exp2.png" alt="Hashed Alpha Testing" class="center-image" /></p>
<center>출처 : <a href="http://cwyman.org/papers/tvcg17_hashedAlphaExtended.pdf">Cwyman.org : Hashed Alpha Test(Extended)</a>
</center>
<p><br /></p>

<p><em>pixScale</em> 을 그냥 계산하는 대신, <em>discretize</em> 된 두개의 스케일값을 2의 지수로 표현하여 값이 0으로 되는 것을 막는다. 이렇게 보간된 값을 사용하여 <em>Alpha Threshold</em> 를 정해주면 약간의 문제가 생긴다. 보간을 함으로써 균일하지 않게 랜덤값이 분포되었기 때문이다. 그래서 아래와 같은 식을 사용하여 다시 값을 분포시켜준다.</p>

<p><br />
<img src="/images/hat_codesnippet_cdf.png" alt="Hashed Alpha Testing" class="center-image" /></p>
<center>출처 : <a href="http://cwyman.org/papers/tvcg17_hashedAlphaExtended.pdf">Cwyman.org : Hashed Alpha Test(Extended)</a>
</center>
<p><br /></p>

<p>위의 식을 적용하면 모든 값들이 균일하게 분포되어 진정한 랜덤값의 <em>Alpha Threshold</em> 가 생성된다고 한다. 아래는 전체 코드다.</p>

<p><br />
<img src="/images/hat_codesnippet_whole.png" alt="Hashed Alpha Testing" class="center-image" /></p>
<center>출처 : <a href="http://cwyman.org/papers/tvcg17_hashedAlphaExtended.pdf">Cwyman.org : Hashed Alpha Test(Extended)</a>
</center>
<p><br /></p>

<p>자세한 사항은 논문에서 확인할 수 있다(<a href="http://cwyman.org/papers/tvcg17_hashedAlphaExtended.pdf">[Cwyman17]</a>). 결과는 아래 유튜브 영상에서 확인할 수 있다.</p>

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class="embed-container">    <iframe title="YouTube video player" width="640" height="390" src="//www.youtube.com/embed/p4TYf5DDpbQ" frameborder="0" allowfullscreen=""></iframe></div>

<p>이를 통해 전보다 훨씬 나은 <em>Alpha Test</em> 품질을 얻을 수 있게 되었다. 하지만 <em>Hashed Alpha Testing</em> 의 결과는 <em>Stochastic Test</em> 처럼 픽셀이 흩뿌려진 느낌을 지울 수 없다. 어느정도의 랜덤값에서 생성이되니 이는 어쩔 수 없는 결과다.</p>

<h2>Alpha Distribution</h2>

<p><em>Alpha Test</em> 의 구린 품질을 좀 더 개선할 수 있는 방법이 또 있다. 이번년도 <em>I3D</em> 에 제출된 <em>Alpha Distribution</em> 이라는 논문이 있는데, 이는 <em>Hashed Alpha Testing</em> 처럼 런타임에 계산을 하지않고 각 <em>Mip-level</em> 의 텍스쳐를 미리 처리해놓는 방법 중에 하나다. 미리 계산된 <em>Texture</em> 들을 사용하여 일반적인 <em>Alpha Test</em> 를 그대로 사용하기만 하면 된다. 아직 직접 사용한 예시는 없어 검증되지는 않았지만, 이 방법이 그대로 사용될 수 있다면 <em>Alpha Test</em> 부분에서는 거의 끝판왕이 될 것 같다.</p>

<p><em>Alpha Distribution</em> 일반적인 <em>Alpha Test</em> 를 기준으로 <em>Alpha Threshold</em> 가 고정되어 있다는 것을 가정한다. 그렇게 되면 <em>Alpha Threshold</em> 에 따라서 픽셀에 출력이 되냐, 안되냐로  따질 수가 있다.(<em>Binary Visibility</em>) <em>Binary Visibility</em> 를 각 <em>Mip-level</em> 에 맞춰서 고르게 분산(<em>Distribution</em>)시키는게 <em>Alpha Distribution</em> 의 목적이다.</p>

<p><em>Alpha Distribution</em> 은 두가지 분산방법을 사용한다. <em>Error Diffusion</em> 과 <em>Alpha Pyramid</em> 이라는 방법을 사용한다. 하나씩 알아보자.</p>

<p><em>Error Diffusion</em> 은 하나하나의 픽셀을 순회하면서, 각 픽셀의 <em>Binary Visibility</em> 에 해당하는 값(0 아니면 1)과 이미지가 가지고 있는 <em>Alpha</em> 값을 비교해 그 오차(<em>Quantization Error</em>)를 다른 픽셀에 나누어준다. <em>Binary Visibility</em> 는 다음과 같이 정해진다.</p>

<blockquote>
  <p>αˆi = αi &gt;= ατ : 1, αi &lt; ατ : 0</p>
</blockquote>

<p>αi 는 이미지가 가지고 있는 이산화된 <em>Alpha</em> 값이고, ατ 는 <em>Alpha Threshold</em>, 한계값을 뜻한다. αˆi 는 해당 픽셀의 <em>Binary Visibility</em> 를 뜻한다. 이것을 가지고 <em>Quantization Error</em> 를 계산한다.</p>

<blockquote>
  <p>ϵi = αi − αˆi</p>
</blockquote>

<p>ϵi 는 <em>Quantization Error</em> 를 뜻하고 픽셀이 보이게 된다면 <em>~1 &lt;= ϵi &lt; 0</em> 의 값을 가지게 되고 픽셀이 보이지 않는다면 <em>0 &lt; ϵi &lt;= 1</em> 의 값을 가지게 된다. 이런 <em>Quantization Error</em> 는 인근 픽셀로 분포된다. 아래 그림을 보자.</p>

<p><img src="/images/ad_error_diffusion.png" alt="Error Diffusion" class="center-image" /></p>

<p>그림에서 ϵi 가 들어가 있는 부분이 현재 처리중인 픽셀이며, ϵi 의 값은 인근 픽셀로 고정된 비율로 <em>Alpha</em> 값에 더해진다. (x+1,y) 는 7/16, (x-1,y+1) 은 3/16, (x,y+1) 은 5/16, (x+1,y+1) 은 1/16 비율로 분포된다. 이런 방법으로 각 픽셀을 순회하면서 처리하면 <em>Error Diffusion</em> 은 간단하게 끝난다. 오차 확산이라는 이름이 굉장히 직관적이다.</p>

<p><em>Error Diffusion</em> 은 픽셀과 픽셀사이의 <em>Alpha</em> 값을 고르게 분포시킨다. 하지만 약간의 문제가 존재한다. 보이게 되던, 안보이게 되던 <em>Alpha</em> 값이 0.3 ~ 0.7 정도로 중간값을 가지고 있다면, 한 픽셀은 강조되고, 옆의 픽셀은 보이지 않게 된다. 이러한 방법은 아래 이미지와 비슷한 결과를 만든다.</p>

<p><br />
<img src="/images/Michelangelo's_David_-_Floyd-Steinberg.png" alt="Michelangelo's_David_-_Floyd-Steinberg" class="center-image" /></p>
<center>출처 : <a href="https://en.wikipedia.org/wiki/Dither">Wikipedia : Dither</a>
</center>
<p><br /></p>

<p><em>Error Diffusion</em> 의 문제는 위 그림처럼 비슷한 색 영역에 있어도 분산된 영향을 받아서 각 픽셀이 부드럽게 보이지 않는 현상이 발생한다. 이러한 특징을 <em>Dithering</em> 이라고 부른다. 그래서 <em>Alpha Distribution</em> 논문에서는 이보다 나은 품질을 위해 <em>Alpha Pyramid</em> 라는 다른 방법이 소개된다.</p>

<p><br />
<img src="/images/alpha_pyramid.png" alt="Cemyuksel : Alpha Distribution" class="center-image" /></p>
<center>출처 : <a href="http://www.cemyuksel.com/research/alphadistribution/">Cemyuksel : Alpha Distribution</a>
</center>
<p><br /></p>

<p><em>Alpha Pyramid</em> 은 <em>Error Diffusion</em> 보다는 좀 더 복잡한 방식이다. <em>Alpha Pyramid</em> 라는 밉맵같은 개념의 텍스쳐들을 생성하고, 그 <em>Alpha Pyramid</em> 를 사용해서 <em>Alpha</em> 값들을 분산시키는 방법이다. <em>Alpha Pyramid</em> 를 만들고 값을 다루는 방법에 대해서 알아보자.</p>

<p><em>Alpha Pyramid</em> 은 각각의 <em>mip-level</em> 마다 하나씩 생성된다. 즉 이미지 한개씩만 처리한다.</p>

<p><em>Alpha Pyramid</em> 를 만들떄 맨 처음에 <em>Mip-Map</em> 과 비슷한 방식으로 <em>Sub-level</em> 들을 만든다. 맨 처음에 생성되는 <em>level</em> 의 해상도는 이미지 해상도의 1/4(1/2*1/2) 을 곱한 해상도로, 2의 지수가 아니여도 된다. 이렇게 생성된 <em>level 1</em> 은 약 원본 이미지의 해상도가 1/4이 되는데, <em>level 0</em> 의 각 픽셀들에 적어도 원본 이미지의 픽셀 4개의 알파값들을 더해서 저장한다. 원본 이미지의 해상도가 2의 지수가 아니라면 <em>level 0</em> 세로와 가로 끝부분의 픽셀들은 픽셀 6개의 알파값들을 더해서 저장하고, 가장 모서리의 픽셀 하나는 픽셀 9개의 알파값을 더해서 저장한다. 위의 이미지는 그리드로 원본의 해상도를 표시하고, 색으로 해당 레벨의 실질적인 픽셀들을 표시한다.</p>

<p>원본 이미지와 <em>level 1</em> 의 관계는 <em>level 1</em> 생성한 후 다음 <em>Level 2</em> 을 생성할 때 <em>level 1</em> 과 <em>level 2</em> 의 관계와 같다. 즉 같은 방법을 2x2 해상도가 될떄까지 계속 반복한다. 이렇게 생성된 <em>Alpha Pyramid</em> 의 각 레벨의 텍셀은 하위 레벨의 연관된 <em>Alpha</em> 값들의 합을 가지고 있는다. 이를 누적된 알파값(<em>Accumulated Alpha</em>)라고 부르겠다.</p>

<p>다음은 각각의 <em>Accumulated Alpha</em> 을 가지고 각각의 픽셀의 보여주는 여부를 결정하는 <em>Visibility Value</em> 를 구해야 한다. 처음에는 <em>Alpha Pyramid</em> 의 최상위 레벨의 각각의 <em>Accumulated Alpha</em> 값들의 합을 구하여 올림을 해준 값을 가지고 있는다. (이 값은 입력으로 들어온 이미지의 보여지는 픽셀을 정하는 값으로, 이 값이 하위 층으로 한층한층 분산되면서 결국 이미지의 보여지는 픽셀을 결정하게 된다. 논문에서는 텍스쳐 전체의 <em>Visibility Value</em> 라고도 한다.) <em>Alpha Pyramid</em> 의 최상위 레벨의 각각의 <em>Accumulated Alpha</em> 의 정수부의 값만(일반적으로 <em>Alpha</em> 값은 0 ~ 1 사이의 소수.) <em>Visibility Value</em> 에 저장한다. 그리고 각 <em>Accumulated Alpha</em> 의 소수부 값들 중 큰 값들만 <em>Visibility Value</em> 를 1씩 나누어준다. 이러면 모든 <em>Visibility Value</em> 가 분산된다. 이렇게 최상위 층의 처리가 끝난다.</p>

<p>그 다음 각각의 층들이 처리가 되야한다. 최상위 층은 각 <em>Accumulated Alpha</em> 을 가지고 있는 윗층이 없어 직접 구했지만, 아랫층부터는 상위 층들이 <em>Accumulated Alpha</em> 합을 구해서 가지고 있다. 이 값을 가지고 위에서 언급한 <em>Visibility Value</em> 를 계산하여 계속 구한다. <em>level 1</em> 까지 이 과정을 반복하면 <em>Alpha Pyramid</em> 내부에서 처리하는 과정은 끝난다.</p>

<p>다음은 마지막으로 계산된 <em>Alpha Pyramid</em> 의 최하층 <em>level 1</em> 의 <em>Visibility Value</em> 들과 맨 처음 입력으로 들어온 이미지를 처리한다. (위에서 <em>Binary Visibility</em> 에 대한 언급을 했었다. <em>Alpha Test</em> 의 이미지는 결국 보이냐, 안보이냐의 차이이기 때문에 <em>Alpha Pyramid</em> 도 <em>level 1</em> 의 <em>Visibility Value</em> 를 통해 이미지의 <em>Binary Visibility</em> 를 처리해준다.) <em>level 1</em> 에 관련된 2x2,2x3,3x2,3x3 픽셀의 <em>Binary Visibility</em> 를 처리한다. <em>level 1</em> 의  <em>Visibility Value</em> 의 값을 기존 이미지가 가지고 있는 <em>Alpha</em> 값이 큰 순서대로 1씩 나누어준다.(보이게 처리한다.) 그렇게 <em>Visibility Value</em> 를 다 쓰게되고 남아있는 픽셀들은 안보이게 처리한다.</p>

<p><em>Alpha Pyramid</em> 는 순서대로 텍스쳐의 층을 쌓은 후 최상층에서 다시 차례대로 내려오면서 <em>Visibility Value</em> 를 분산시키고, 마지막으로 이미지의 각 픽셀의 <em>Binary Visibility</em> 를 정해주는 방법이다.</p>

<p>이 글에서의 <em>Alpha Distribution</em> 에 대한 내용은 논문 뿐만 아니라 논문의 저자가 제공한 코드까지 참조하여 썼다. 근데 논문의 내용중 여기서 언급하지 않은 내용이 있다. 저자가 <a href="https://github.com/cemyuksel/cyCodeBase/blob/master/cyAlphaDistribution.h">제공한 코드</a>에서는 <em>Visibility Value</em> 를 분산시킬 떄 소수부의 값이 큰 기준으로 분산시킨다. 하지만 논문에서는 이를 랜덤하게 처리한다고 한다. 왜냐하면 균일한 패턴의 생성을 막기 위해서라고 한다. 하지만 제공되는 코드에서는 랜덤하게 설정하는 부분은 없었다. 또한 코드에서는 <em>Alpha Threshold</em> 를 0.5 로 가정하여 코드를 짜놓아서</p>

<p>방법만 봐도 여러가지 이유로 <em>Alpha Pyramid</em> 이 <em>Error Diffusion</em> 보다는 더 나은 <em>Alpha Test</em> 를 제공할 것 같다는 생각이 든다. 논문에서도 실제로 더 나은 품질을 보여준다고 한다.</p>

<style>.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }</style>
<div class="embed-container">    <iframe title="YouTube video player" width="640" height="390" src="//www.youtube.com/embed/cjHfPi9lQik" frameborder="0" allowfullscreen=""></iframe></div>

<p>절대적으로 좋은 결과를 내게하는 방법은 없다. <em>Alpha Distribution</em> 역시 단점을 가지고 있다. 미리 처리를 하기 때문에 이미지가 고정되어 타일링처럼 보일 수가 있다. 또한 확대시 아무것도 처리하지 않고, <em>Bilinear Filtering</em> 만 걸은 것보다 안좋은 결과를 보여줄 수도 있다.</p>

<p><br />
<img src="/images/ad_cons.png" alt="Cemyuksel : Alpha Distribution" class="center-image" /></p>
<center>출처 : <a href="http://www.cemyuksel.com/research/alphadistribution/">Cemyuksel : Alpha Distribution</a>
</center>
<p><br /></p>

<p>또 <em>Alpha Threshold</em> 가 고정되어 있다고 가정하기 때문에 값이 바뀌면 다시 계산해야한다. 직접 구현해서 붙이는 경우에는 계산하는 코드를 넣어주어야 하는데, 만약 <em>Texture Compression</em> 이 적용되어 있으면 굉장히 귀찮을 것이다. 거기에 상용엔진에 <em>Intergration</em> 할려면 더욱더 심할것이다.</p>

<p>그에 비해 <em>Hashed Alpha Testing</em> 은 구현하기엔 쉬운편이다. 코드는 <em>Shader</em> 에 붙여넣기만 하면 된다. 하지만 약간의 퍼포먼스를 잡아먹고, 뭉개진 가루처럼 보이는 현상이 존재하기 때문에 무조건 좋다고하기에는 무리가 있다.</p>

<p>조금더 시간을 가지고 지켜봐야 될것 같다는 생각이 든다.</p>

<h2>참조</h2>

<ul>
  <li><a href="https://medium.com/@bgolus/anti-aliased-alpha-test-the-esoteric-alpha-to-coverage-8b177335ae4f">Anti-aliased Alpha Test: The Esoteric Alpha To Coverage</a></li>
  <li><a href="https://developer.download.nvidia.com/assets/gameworks/downloads/regular/GDC17/RealTimeRenderingAdvances_HashedAlphaTesting_GDC2017_FINAL.pdf?pUIX8DXxfad7mL4zB3GOthX3r5IgGao9UWxYuYb3q9h10RXrQeYko-dEuJXJxt1hhsI9J_9KJDcCYGeWWksxlaHTrXSE825D_3izja7LUFOtzhaeBUqpn7qbwXaaGlLdbipjE3PeI3e2IMn45mQAA3OV2PD-kG2y9cecTaWE2uum2uwdHgyn0nhYiLOvlOsrUzewbK5REH7vAm3-lNWzxehw_5Tphg">NVidia developer : Hashed Alpha Testing</a></li>
  <li><a href="http://cwyman.org/papers/tvcg17_hashedAlphaExtended.pdf">Cwyman.org : Hashed Alpha Test(Extended)</a></li>
  <li><a href="http://www.cemyuksel.com/research/alphadistribution/">Cemyuksel : Alpha Distribution</a></li>
  <li><a href="https://github.com/cemyuksel/cyCodeBase/blob/master/cyAlphaDistribution.h">Github : cyAlphaDistribution.h</a></li>
</ul>

        </div>

        
      </li>
    
      
      

      <li>
        <header class="post-header">
          <h1 class="post-title">
            <a class="post-link" href="/2018/05/18/IEEE-754-floaing-number/">Ieee 754 Floaing Number</a>
          </h1>

          <p class="post-meta">May 18, 2018 • 
  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/math/">math</a>,
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/floating-point/">floating_point</a>
    
  
    
  
    
  

</p>
        </header>

        <div class="post-content">
          <p>코딩을 하던 도중, 비트로 나타내어진 2 byte floating number(half-precision) 데이터를 일반적인 4byte floating number(single-precision) 으로 나타내야 할 일이 있었다. 그래서 귀찮아서 알아보지 않았던 컴퓨터의 소수를 표현하는 방법에 대해서 알아보았다. 이 글에서는 간략하게 어떤식으로 표현되는지에 대해서만 적어보기로 하겠다.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>134.75
</code></pre></div></div>

<p>이와 같은 소수가 있다. 이는 10진법으로 나타낸 소숫점으로, 2진법으로 나타내면 다음과 같다.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>134.75(demical) = 10000110.11(binary)
</code></pre></div></div>

<p>이를 소수부와 정수부를 나누면 다음과 같다.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>134(demical) = 10000110(binary)
0.75(demical) = 0.11(binary)
</code></pre></div></div>

<p>정수부는 오른쪽부터 2^0 인 1부터 나란히 2^¹, 2^², 2^³, 2^⁴, … 2^ⁿ 로 구성되고(n은 자리의 끝), 소수부는 점 이하인 숫자부터 2^-1, 2^-2, … 2^-n 으로 구성된다. 0.75 는 2^-1 * 1 + 2^-2 * 2 가 되니 위의 경우처럼 굉장히 쉽게 표현이 가능하다. 하지만 다음과 같은 숫자는 어떻게 표현할까?</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.9999... = 0.1111...
</code></pre></div></div>

<p>이런식으로 표기는 가능할 것이다. 하지만 컴퓨터는 유한한 데이터만을 다루기 때문에 한계가 있다.</p>

<h2>IEEE 754</h2>

<p>컴퓨터에서는 숫자를 다루기 위해 여러가지 기준이 정해져 있다. 그 중에서도 소수를 나타내기 위한 기준은 IEEE 754 로 알려져 있다. 대부분의 언어에서 사용하는 <em>float</em> 은 IEEE 754 single-precision 을 사용하여 계산된다.(<em>double</em> 또한 마찬가지.) 데이터를 어떻게 저장하는지, 그 데이터의 표현방식은 어떻게 되는지에 대하여 간략하게 알아보자.</p>

<p><br />
<img src="/images/General_floating_point_ko.png" alt="위키백과 : IEEE 754" class="center-image" /></p>
<center>출처 : <a href="https://ko.wikipedia.org/wiki/IEEE_754">위키백과 : IEEE 754
</a>
</center>
<p><br /></p>

<p>데이터의 저장 방식은 다음과 같다. 일반적으로 들어가는 부호를 위한 1bit, 그리고 우리가 아직 살펴보지 않은 지수(<em>exponent</em>), 가수(<em>fraction</em>) 부분으로 나뉘어져 있다. single-precision 을 예시로 보며 설명해보겠다.</p>

<p><br />
<img src="/images/single-precision_example.png" alt="Wikipedia : Single-precision floating-point format" class="center-image" /></p>
<center>출처 : <a href="https://en.wikipedia.org/wiki/Single-precision_floating-point_format">Wikipedia : Single-precision floating-point format</a>
</center>
<p><br /></p>

<p>각각의 부분이 어떠한 숫자를 저장하는지만 알면 된다. <em>fraction</em> 은 말 그대로 소숫점 아래의 숫자만 나타내는 부분이다. 가장 왼쪽의 비트(22번쨰 비트)는 2^-1 을 저장하고 오른쪽으로 2^-2, 2^-3 이런식의 숫자에 대한 정보를 기록한다. 이렇게 실질적인 소수부와 나머지인 <em>exponent</em> 부분이 남아 있다. <em>exponent</em> 는 <em>fraction</em> 숫자를 얼마나 곱하는지 나타내는 숫자다. 아래의 그림을 보자.</p>

<p><br />
<img src="/images/single-precision_formatted.svg" alt="Wikipedia : Single-precision floating-point format" class="center-image" /></p>
<center>출처 : <a href="https://en.wikipedia.org/wiki/Single-precision_floating-point_format">Wikipedia : Single-precision floating-point format</a>
</center>
<p><br /></p>

<p>여기서 이해가 안되는 부분이 많을 것이다. 첫번째로 맨 오른쪽의 식은 2의 지수가 음수가 되는 부분의 데이터를 나타내는 식인데, 전부다 더한 이후에 1을 더한다. 이는 숫자의 표현을 위해 넣은 부분이다. 그 다음은 중간 2의 지수가 들어가는 식에서 2^(e-127) 인데, 이는 지수를 양수, 음수로 표현하기 위한 수단이다. 양수가 된다면 가수부가 나타내는 숫자보다 큰 숫자를 나타낼 것이며, 음수가 된다면 가수부가 나타내던 숫자보다 더 작은 수를 표현할 것이다. 즉 표현하는 숫자의 범위는 굉장히 큰 것을 알 수 있다. 또한 정밀도는 보장하지 못한다는 것을 알 수 있다.</p>

<h2>참조</h2>

<ul>
  <li><a href="https://ko.wikipedia.org/wiki/IEEE_754">위키피디아(한글) : IEEE 754</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Single-precision_floating-point_format">Wikipedia : Single-precision floating-point format</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Half-precision_floating-point_format">Wikipedia : Half-precision floating-point format</a></li>
</ul>

        </div>

        
      </li>
    
      
      

      <li>
        <header class="post-header">
          <h1 class="post-title">
            <a class="post-link" href="/2018/05/13/anisotropic-filtering/">Anisotropic Filtering</a>
          </h1>

          <p class="post-meta">May 13, 2018 • 
  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/anisotropic/">anisotropic</a>,
    
  
    
  
    
  
    
  
    
  

  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/filtering/">filtering</a>
    
  
    
  
    
  
    
  

</p>
        </header>

        <div class="post-content">
          <p>보통 사용되는 <em>Texture Fltering</em> 들은 <em>Axis Align</em> 된 방향을 기준으로 추가적인 샘플링을 하는 방법들이 대부분이다.(bilinear, bicubic, etc..) 하지만 특이한 것이 하나 있다. 바로 <em>Anisotropic Filtering</em> 이다.</p>

<p><em>Anisotropic Filtering</em> 은 원거리에 있는 물체들을 선명하게 보이게 하기위해서 쓰여지는 <em>Fiterling</em> 으로, 말보다는 아래 그림을 보는게 훨씬 직관적으로 이해할 수 있다.</p>

<p><br />
<img src="/images/aniso_pixel_to_texel.png" alt="Real-time Rendering 3rd" class="center-image" /></p>
<center>출처 : Real-time Rendering 3rd&lt;/a&gt;
</center>
<p><br /></p>

<p>위의 그림과 같이 <em>Texture-Space</em> 에서 픽셀안에 있는 텍스쳐를 여러번 샘플링하여 평균을 구하는 방식인듯하다. 그런데 아주 중요한 것이 하나 남아있다.</p>

<p><br />
<img src="/images/img034.gif" alt="Unsolved Problems and Opportunities for High-quality, High-perfornmance 3D Graphics on a PC Platform : Anisotropic Filtering" class="center-image" /></p>
<center>출처 : <a href="http://www.graphicshardware.org/previous/www_1998/presentations/kirk/sld030.htm">Unsolved Problems and Opportunities for High-quality, High-perfornmance 3D Graphics on a PC Platform : Anisotropic Filtering</a>
</center>
<p><br /></p>

<p>위의 그림을 보면 알겠지만 <em>bilinear filtering</em> 과 함께 쓸 경우 엄청난 샘플링 부하가 생길 것이라는 것을 예상할 수 있다.</p>

<p><em>Anisotropic Filtering</em> 을 처음 접했을 떄, 가장 이해가 가지 않았던 것은 결국 내부에서 샘플링을 해야할 텐데 어떤 방식으로 방향을 구할지 가장 이해가 안됬었다. 지금 다시 생각해보면, <em>ddx</em> 키워드를 <em>uv</em> 좌표에 쓰듯이 <em>Texutre-Space</em> 의 차이 벡터를 쉽게 구할 수 있을 듯 하다.</p>

<h2>참조</h2>

<ul>
  <li>Real-Time Rendering 3rd</li>
  <li><a href="http://www.graphicshardware.org/previous/www_1998/presentations/kirk/sld030.htm">Unsolved Problems and Opportunities for High-quality, High-perfornmance 3D Graphics on a PC Platform : Anisotropic Filtering</a></li>
</ul>

        </div>

        
      </li>
    
      
      

      <li>
        <header class="post-header">
          <h1 class="post-title">
            <a class="post-link" href="/2018/03/13/recommandation-of-gpu-skinning-in-github/">Recommandation Of Gpu Skinning In Github</a>
          </h1>

          <p class="post-meta">Mar 13, 2018 • 
  
  
    
  
    
  
    
      <a href="/categories/unity/">unity</a>,
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/gpu-skinning/">gpu-skinning</a>
    
  
    
  
    
  
    
  
    
  
    
  

</p>
        </header>

        <div class="post-content">
          <p>최근 급하게 어떤 프로젝트에 투입되서 작업을 하고 있다. 다른 회사와 같이 일을 하고 있는데, 다른 회사에서 해놓은 것들이 너무 느려서 최적화를 해야했다. 결국 일반적인 잔머리로는 도저히 해결할 수 없는 상황에 봉착했다. 1년 전의 필자였다면 포기하고 안된다고 했었겠지만 다행히 약간의 노하우를 통해 해결할 수 있었다.</p>

<h2>상황과 문제?</h2>

<p>요즘 한국에서는 VR 게임/컨텐츠들이 굉장히 많이 개발되고 있다. 필자도 그 와중에 떨어진 프로젝트를 하나 받아진행하게 되었다. 개발은 주변 3D 환경을 다른 회사에서 해주고, 게임이 돌아가는 코어 시스템을 필자의 회사에서 작업하기로 되어 있었다. 게임에서의 코어 시스템은 개발을 하였으나 <em>Unity</em> 에 의존적인 코딩은 거의 진행되지 않은 상태에서 한달전 프로젝트에 투입되었다.</p>

<p>그렇게 하나하나 작업을 하면서 코어 시스템과 3D 환경을 결합하는 도중, 터무니 없는 경우를 만났다. 바로 <strong>SkinnedMeshRenderer</strong> 와 <em>Animator</em> 가 약 800 개 정도되는 상황에 부딫쳤다. 여기서는 두가지의 큰 부하가 있었다. 절대적인 <em>Vertex Skinning</em> 부하와 <em>Animator</em> 가 800개가 한꺼번에 계산되는 부하였다. <em>Vertex</em> 숫자는 <em>LOD</em> 베이커를 구해서 어떻게든 해결이 되었으나, 800개의 <em>Animator</em> 부하는 우회방법이 없었다. 즉 이는 <em>Vertex Shader</em> 나 <em>Compute Shader</em> 안에서 <em>AnimationClip</em> 의 정보들을 처리하는 정공법이 필요했다.</p>

<p>하지만 기한도 얼마 남지않아 급한 와중에 저것들을 직접 코딩할 여유는 없었다. 게다가 처음 건드려보는 부분이라서 헤멜 코스트까지 합하면 굉장히 암울했다. 혹시 오픈소스가 있나 싶어서 생각을 해보았는데 예전에 <em>SkinRenderer</em> 를 직접 구현하면서 찾아본 오픈소스 레포지토리가 하나 있었다.</p>

<h2>Github : <a href="https://github.com/chengkehan/GPUSkinning">GPUSkinning</a></h2>

<p>한 중국인이 개발한 스키닝 툴이다. 이는 두가지의 큰 기능을 담고있다. 하나는 <em>Vertex Shader</em> 에서 <em>Skinning</em> 처리를 해주는 기능과, 하나는 <em>AnimationClip</em> 들을 직접 샘플링해 바이너리 파일로 저장해 GPU 메모리에 텍스쳐의 형태로 올려두어 사용하는 기능이 있다.</p>

<p><em>Compute Shader</em> 나 <em>Vetex Shader</em> 를 사용해 <em>Skinning</em> 을 구현하는 것은 크게 어려운 것은 아니다. <em>Deformation</em>  하지만 제일 시간이 오래걸리는 부분은 <em>AnimationClip</em> 을 가공하는 부분이 제일 오래 걸리는 부분 중 하나다. 해당 레포지토리에는 그 오래걸리는 부분을 만들어 놓았다. 사실 이 부분으로만으로 꽤 큰 가치가 있다. 부가적으로 원하는 <em>bone</em> 의 위치와 회전값을 <strong>Transform</strong> 으로 만들어 <em>Hierarchy</em> 상에서 컨트롤이 가능하다.</p>

<p>조금 불편한 점도 몇가지 있다. 한번에 여러개의 샘플링이 안되고, 에디터에서 필수적으로 지원해야할 멀티 에디팅이 안된다. 후자는 간단히 코드를 수정하면 되지만 전자는 샘플링 과정에서 플레이를 해야되기 때문에 직접 수정하기엔 조금 부담스럽다. <em>Vertex Shader</em> 에 스키닝이 물려있기 때문에 <em>Skinning</em> 을 수정하려면 <em>Shader</em> 부분도 바꿔야하고, <em>LOD</em> 기능도 어설프게 들어있어 조금 애매하다.</p>

<p>더 기능을 생각하자면 IK 나 특정 본을 타겟으로 회전을 시키는 기능이 없다. 이는 <em>mecanim</em> 에서 <em>Humanoid</em> 를 타겟으로 지원하는 기능으로 이 기능까지 만들어 놓았으면 엄청 유용 했을것 같다. 다만 구현이 <em>Vertex Shader</em> 에서 되어서 다른 기능을 끼워넣기는 조금 부담스러운 것으로 생각된다.</p>

<p>굳이 단점을 생각하지 않아도 샘플링 코드와 구현만으로도 충분히 가치있는 레포지토리로 생각된다. <em>ComputeShader</em> 를 사용해서 구현했으면 더 좋았을 것 같다는 생각이 문득든다.</p>

<p>써보면서 이게 본격적으로 쓰려고 만들어진 코드는 아닌 것 같다는 생각이 들었다. 기본적인 캐싱도 안되있어서 약간의 삽질을 했었다.</p>

<p>가장 큰 문제는 <em>AnimationClip</em> 이 많으면 많을수록 에디터, 런타임 로드시에 엄청나게 로딩이 걸린다. 아마 ScriptableObject 에 Serialization 으로 저장한 정보들이 많아서 그런듯 하다. 이는 따로 텍스쳐든 뭐든 Unity 에서 직접 관리하는 리소스로 바꾸어야 겠다.</p>

<h2>참조</h2>

<ul>
  <li><a href="https://github.com/chengkehan/GPUSkinning">Github : GPUSkining</a></li>
</ul>

        </div>

        
      </li>
    
      
      

      <li>
        <header class="post-header">
          <h1 class="post-title">
            <a class="post-link" href="/2018/03/06/dualized-asset-management-of-unity/">Dualized Asset Management Of Unity</a>
          </h1>

          <p class="post-meta">Mar 6, 2018 • 
  
  
    
  
    
  
    
      <a href="/categories/unity/">unity</a>
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

</p>
        </header>

        <div class="post-content">
          <p><em>Unity</em> 에서는 모든 사용자의 작업물을 <em>Assets</em> 폴더에 저장한다. 그리고 <em>Assets</em> 폴더안의 파일의 변경이 발생할 시 안의 파일들을 재가공하여 다시 로드한다. 보통 파일의 변경은 <em>assetDatabaseX</em> 바이너리 파일로 들어가게 되며, 스크립트, 바이너리의 변경은 다시 컴파일을 함으로써 현재 변경사항을 프로젝트에 적용시킨다.</p>

<p>이러한 시스템을 위해 <em>Unity</em> 에서는 모든 파일, 디렉토리에 <em>meta</em> 파일을 생성한다. 파일별 <em>meta</em> 파일에는 해당 파일의 순수한 정보가 아닌 메타 정보가 들어간다. 중요한 정보는 두개로 나뉜다.</p>

<p>하나는 <em>Unity</em> 프로젝트상에서 파일을 처음 감지했을 때, 파일의 <em>GUID</em> 를 생성한다. <em>GUID</em> 란 고유의 16진수 32글자로 이루어지는 총 512비트로 이루어지는 <em>ID</em> 로써 자동으로 생성되는 알고리즘을 가지고 있으며 겹칠 염려는 거의 없는 <em>ID</em> 알고리즘이다. 그래서 생성된 <em>GUID</em> 는 다른 곳에서 해당 파일을 참조할떄 쓰인다. 즉 파일이 삭제되서 같은 것으로 다시 생성한다고 해도 <em>GUID</em> 가 랜덤으로 결정되기 때문에 다시 연결을 해주어야 한다. 이는 <em>Unity</em> 내부에서 파일 링크를 <em>GUID</em> 로 한다는 추측을 할 수 있게 해준다. 또한 <em>Edit -&gt; Project Setting -&gt; Editor</em> 에서 <em>Asset Serialization</em> 모드가 <em>Force Text</em> 로 되어있을 시에는 <em>meta</em> 파일들을 직접 텍스트 에디터로 확인이 가능하다.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fileFormatVersion: 2
guid: 5d44a238286f6904198ab78e914c229d
MonoImporter:
  serializedVersion: 2
  defaultReferences: []
  executionOrder: 0
  icon: {instanceID: 0}
  userData:
  assetBundleName:
</code></pre></div></div>

<p>어떤 스크립트에 딸린 <em>meta</em> 파일의 내용이다. 두번째 줄에 생성된 <em>guid</em> 가 존재한다. 이는 <em>Library/metadata</em> 디렉토리에 쓰여진 이름들과 매칭된다.</p>

<p>두번째는 바로 해당 파일의 <em>Importer</em> 정보가 들어있다. 위의 <em>meta</em> 파일은 스크립트이기 때문에 3번째 줄에 <em>MonoImporter</em> 라고 쓰여져 있으며, 파일의 성질에 따라서 <em>built-in importer</em> 가 달라진다. 바이너리 파일들은 <em>NativeImporter</em>, 텍스쳐 파일들은 <em>TextureImporter</em>, 3D 모델 파일들은 <em>ModelImporter</em> 로 자동으로 매칭된다.</p>

<p>이러한 <em>Importer</em> 정보들은 보통 해당 <em>Asset</em> 의 옵션을 세팅할 떄 쓰인다. 또한 <em>2017</em> 버젼에서는 파일의 확장자를 사용자가 직접 지정해 <em>Importer</em> 를 사용할 수도 있게 해두었다.(<a href="/2018/01/11/unity-scripted-importer/">링크</a>)</p>

<p>즉 <em>Unity</em> 에서는 새로운 파일을 감지했을 때, <em>GUID</em> 를 생성하고 파일의 확장자에 따라 <em>Importer</em> 정보를 갱신한 후, 정보를 <em>Library/metadata</em> 에 갱신하는 것으로 볼 수 있다. <em>Library/metadata</em> 에서는 <em>GUID</em> 로 된 파일과 (해당 <em>GUID</em>).info 로 파일이 구성되어 있다. 각각의 파일은 파일의 유형별로 다른 것으로 보인다.</p>

        </div>

        
      </li>
    
      
      

      <li>
        <header class="post-header">
          <h1 class="post-title">
            <a class="post-link" href="/2018/03/04/what-is-ddx-and-ddy/">What Is Ddx And Ddy</a>
          </h1>

          <p class="post-meta">Mar 4, 2018 • 
  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/shader/">shader</a>,
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/hlsl/">hlsl</a>
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

</p>
        </header>

        <div class="post-content">
          <p><em>HLSL</em> 에는 <em>ddx</em> 와 <em>ddy</em> <em>intrisic</em> 이 <em>Shader Model 2.0</em> 부터 존재했다. 필자는 이를 이해하기 위해 자료를 찾아보았지만 쉽게 이해되는 것들은 거의 없었다. 이해한 것을 정리하기 위해 이글을 쓴다.</p>

<p>예전부터 <em>Pixel Shader</em> 를 처리할 때 픽셀 단위로 하나하나 처리하는게 아닌 적어도 2x2 개의 픽셀들을 한꺼번에 처리했다고 한다. 그래서 이러한 아키텍쳐를 이용한 키워드가 <em>ddx</em> 와 <em>ddy</em> 다. 기본적으로 쉐이더는 병렬로 처리되기 때문에, 4개의 <em>Pixel Shader</em> 가 한꺼번에 실행되는 것으로 생각할 수 있다. 아래 코드를 보면서 생각해보자.</p>

<pre><code class="language-hlsl">    half3 dpdx = ddx(position);
    half3 dpdy = ddy(position);
</code></pre>

<p>4개의 픽셀 쉐이더가 첫번째 라인을 실행할 때 ddx 는 들어온 파라미터의 x축, 가로의 픽셀들의 파라미터의 차이를 구해 반환한다. 이는 <em>δ/δx</em> 의 의미와 같다. 즉 x 를 기준으로 편미분을 한것이라고 한다. 마찬가지로 ddy 는 y축을 기준으로 차이를 계산해 반환하는 키워드로 생각하면 된다.</p>

<p><em>Shader Model 5.0</em> 부터는 <em>ddx_coarse/ddy_coarse</em> 와 <em>ddx_fine/ddy_fine</em> 으로 키워드가 나뉜다. 기존의 <em>ddx/ddy</em> 는 <em>ddx_coarse/ddy_coarse</em> 와 같다고 한다. <em>fine</em> 과 <em>coarse</em> 의 차이는 간단하다. 4개의 픽셀을 기준으로 각각의 차이를 전부 구하는게 <em>fine</em>, 한쪽의 차이만 구하는게 <em>coarse</em> 라고 한다. 자세한 것은 아래 참조에서 보는 것을 추천한다.</p>

<h2>참조</h2>
<ul>
  <li><a href="https://msdn.microsoft.com/en-us/library/windows/desktop/bb509588.aspx">MSDN HLSL Intrisic : ddx</a></li>
  <li><a href="https://gamedev.stackexchange.com/questions/62648/what-does-ddx-hlsl-actually-do">gamedev.stackexchange.net : What does ddx (hlsl) actually do?</a></li>
  <li><a href="https://fgiesen.wordpress.com/2011/07/10/a-trip-through-the-graphics-pipeline-2011-part-8/#comment-1990">The ryg blog : A trip through the Graphics Pipeline 2011, part 8</a></li>
  <li><a href="https://msdn.microsoft.com/en-us/library/windows/desktop/hh446950.aspx">MSDN Shader Model Assembly 5.0 : deriv_rtx_fine</a></li>
  <li><a href="https://msdn.microsoft.com/en-us/library/windows/desktop/hh446948.aspx">MSDN Shader Model Assembly 5.0 : deriv_rtx_coarse </a></li>
</ul>

        </div>

        
      </li>
    
      
      

      <li>
        <header class="post-header">
          <h1 class="post-title">
            <a class="post-link" href="/2018/02/19/gpu-branching-and-divergence/">Gpu Branching And Divergence</a>
          </h1>

          <p class="post-meta">Feb 19, 2018 • 
  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/gpu/">gpu</a>,
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/gpgpu/">gpgpu</a>
    
  
    
  
    
  
    
  
    
  
    
  
    
  

</p>
        </header>

        <div class="post-content">
          <p>요즘은 꽤나 많은 것들을 GPU 로 처리할 수 있다. GPGPU 기술이 나온지 10년이 넘어가는 이 시점에서 꽤나 많은 것들이 GPU 로 처리되고 있다. 그 중에서도 GPGPU 를 다뤄볼 사람이라면 필수적인 상식하나가 있다. 아래 그림을 보자.</p>

<p><br />
<img src="/images/prior_simt.png" alt="" class="center-image" /></p>
<center>출처 : <a href="http://composter.com.ua/documents/Volta-Architecture-Whitepaper.pdf">NVidia : GV100 Whitepaper</a>
</center>
<p><br /></p>

<p>위 그림은 분기가 나뉘어져 있는 코드를 여러개의 스레드가 실행하는 것을 보여준다. 즉 GPU 에서의 코드 실행 모습이다. 왼쪽은 스레드의 번호로 나뉘는 간단한 분기 코드다. 이는 CPU 에서는 크게 문제가 없다. 하지만 분기가 있던 말던 모든 명령어들을(분기안의 코드들) 전부 실행시키는 것이다. 오른쪽의 그림에서 설명하는 것은 이를 실행하는 쓰레드의 모습을 나타낸다. 딱 봐도 이 그림은 처리가 비효율적일 것처럼 보인다.</p>

<p>CPU 는 한 쓰레드에서 하나의 <em>Program Conter</em> 를 가지기 때문에 분기가 나오면 조건에 맞게 단순히 포인터를 증가시키기만 한다. 하지만 <em>SIMT</em>(<em>Single Instruction Multiple Threads</em>) 의 구조를 가진 GPU 에서의 분기는 조금 다르다. 여태까지는 여러개의 쓰레드를 가진 그룹 하나당 <em>Program Counter</em> 를 가지는게 일반적이였다. 그래서 위와같이 동시에 활성화된 쓰레드들 끼리만 실행하게 되는 것이다.</p>

<p><em>Volta</em> 아키텍쳐에서는 이를 개선시켜 한 <em>Thread</em> 당 하나의 <em>Program Counter</em> 와 <em>Call-Stack</em> 을 두므로써 각 <em>Thread</em> 를 독립적으로 실행시키게 해준다고 한다. <em>SIMT</em> 의 <em>Concurrency</em> 를 고려하여 전부 한꺼번에 실행시키지는 못하지만 각각의 <em>Thread</em> 를 같은 명령별로 그룹지어 실행시키거나, 한번에 실행시키는게 아니라 각 그룹의 실행을 클럭이나 시분할로 쪼개어 실행하는 것들을 지원한다고 한다. 아래 그림을 보자.</p>

<p><br />
<img src="/images/interleaved_execution.png" alt="" class="center-image" /></p>
<center>출처 : <a href="http://composter.com.ua/documents/Volta-Architecture-Whitepaper.pdf">NVidia : GV100 Whitepaper</a>
</center>
<p><br /></p>

<p>하지만 이는 그림에서 나와 있다시피 그다지 효율적인 실행은 아니다. 결국 아직은 분기의 사용은 최소화해야될 것으로 보인다. 다만 이 기능들은 <em>Graphic Processing Unit</em> 들이 <em>Accelerator</em> 로 바뀌는 하나의 과정으로 볼 수 있을듯 하다.</p>

<h2>참조</h2>

<ul>
  <li><a href="http://composter.com.ua/documents/Volta-Architecture-Whitepaper.pdf">NVidia : GV100 Whitepaper</a></li>
</ul>

        </div>

        
      </li>
    
      
      

      <li>
        <header class="post-header">
          <h1 class="post-title">
            <a class="post-link" href="/2018/01/14/frustum-traced-shadow-with-irrelgular-z-buffer-2/">Frustum Traced Shadow With Irrelgular Z Buffer 2</a>
          </h1>

          <p class="post-meta">Jan 14, 2018 • 
  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/shader/">shader</a>,
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/shadow/">shadow</a>,
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/rendering/">rendering</a>,
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/fts/">fts</a>
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

</p>
        </header>

        <div class="post-content">
          <p><a href="/2018/01/13/frustum-traced-shadow-with-irrelgular-z-buffer-1/">frustum Traced Shadow with Irregular Z-Buffer 1</a> 에서 포괄적인 전체 시스템과 복잡도에 대하여 알아보았다. 이번 글에서는 시스템 구현에 관한 디테일한 사항들을 알아볼 것이다.</p>

<p>첫번째로는 <em>Irregular Z-Buffer</em> 와 <em>Sampling Rate</em> 간의 최적화다. 논문의 저자는 기본적으로 <em>32spp</em> (<em>sampling per pixel</em>) 를 제안했다. 정확히 짚자면, <em>Light-Space</em> 에서 <em>Occluder Geometry</em> 를 <em>Conservative Rasterization</em> 을 하면 <em>Visibility Test</em> 를 계산하는 것이, 한번 <em>Visibility Test</em> 를 할때 32번을 하는게 가장 신경쓰이는 부분이다. 이의 결과를 저장하기 위해 두가지 방법이 있다고 한다. 하나는 <em>μQuad</em> 를 <em>Light-Space</em> 에서 <em>IZB</em> 를 만들 떄 <em>Rasterize</em> 하는 것, 다른 방법은 32 번의 <em>Visibility Test</em> 샘플링 결과를 <em>IZB</em> 에 저장하는 것이다. 전자는 비용이 크기 때문에 안쓰고, 후자를 선택했다고 한다. 이를 <em>Sample-based insertion</em> 이라고 명명했다. 그래서 이 방식으로 <em>Prototype</em> 을 만들어 보니, <em>IZB</em> 의 중복을 위한 최적화를 했음에도 불구하고 한 픽셀당 8개 이상의 <em>IZB Node</em> 가 생성되었다고 한다.</p>

<p>그래서 고안해낸 간단한 근사(<em>approximate</em>)하는 방법을 언급한다. <em>μQuad</em> 의 <em>Normal</em> 벡터와 <em>View Ray</em>(<em>Eye Direction</em>) 벡터의 내적 값이 0에 가까워질수록(90도에 가까워질수록) <em>μQuad</em> 를 늘리는 것이다. 아래 그림의 왼쪽 그림을 보면 쉽게 이해할 수 있다.
그래서 고안해낸 간단한 근사(<em>approximate</em>)하는 방법을 언급한다. <em>μQuad</em> 의 <em>Normal</em> 벡터와 <em>View Ray</em>(<em>Eye Direction</em>) 벡터의 내적 값이 0에 가까워질수록(90도에 가까워질수록) <em>μQuad</em> 를 늘리는 것이다. 아래 그림의 왼쪽 그림을 보면 쉽게 이해할 수 있다.</p>

<p><br />
<img src="/images/fts_microquad_elongate.png" alt="" class="center-image" /></p>
<center>출처 : <a href="http://cwyman.org/papers/tvcg16_ftizbExtended.pdf">Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows</a>
</center>
<p><br /></p>

<p>보다 정확하게 계산 방법을 설명하자면, <em>μQuad</em> 의 넓이는 표면의 방향과 상관없이 상수로 정해주고 <em>View Ray</em> 의 앞뒤 방향으로 <em>μQuad</em> 의 길이가 늘어나고, 그 방향을 따라서 1줄로 샘플링을 한다. 1차원이라고도 할 수 있겠다. 1 ~ 8 개의 샘플링을 해준다고한다. 위 그림의 오른쪽 그림을 보면 쉽게 이해할 수 있다.</p>

<p>다만 이는 단지 <em>Irregular Z-Buffer</em> 를 <em>Approximate</em>(근사) 하는 것이기 때문에 오차가 생길 수 있다. <em>μQuad</em> 가 커질수록 <em>IZB Node</em> 를 넣는 것을 놓치고, <em>Light Leak</em> 을 발생시킬 수 있다. 보통 가리는 물체가 작거나, 멀리있는 경우에 해당된다. <em>Light Leak</em> 을 없에는 방법은 몇가지가 존재하는데, 가장 쉬운 방법은 <em>IZB</em> 의 기본적인 모토인 1:1 샘플링을 맞춰주는 것이다. 하지만 이는 정확히 해주기에 어려운 경우가 있다고 한다. 그래서 다른 방법을 제시한다. <em>Conservative Rasterization</em> 은 보통 결과에서 0.5 픽셀을 늘려준다. 하지만 1 픽셀 팽창(<em>dilation</em>)을 해주는 <em>Conservative Rasterization</em> 을 사용하면 <em>Light Leak</em> 을 막을 수 있다. 원래 <em>μQuad</em> 2차원으로 샘플링을 했었으나 기준을 1차원으로 줄이면서 각각의 폴리곤의 넓이을 늘리는 방식으로 보완한 것이라고 생각하면 되겠다. 아래 그림에 적용을 한 사례가 있다.</p>

<p><br />
<img src="/images/fts_approx-insert_vs_over-conserv-raster.png" alt="" class="center-image" /></p>
<center>출처 : <a href="http://cwyman.org/papers/tvcg16_ftizbExtended.pdf">Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows</a>
</center>
<p><br /></p>

<p>이전 글에서 언급한 복잡도는 O(<em>La</em> * <em>F</em>) 이다. 우선 평균적인 리스트의 길이는 줄어든다고 한다. 샘플링을 하는 횟수가 최소 1/4 정도 줄었기 때문이다.(8 / 32) 그리고 더 넓은 <em>Conservative Rasterization</em> 의 결과로 <em>Fragment</em> 의 갯수는 최대 60% 증가했다고 한다. 이는 성능상 엄청난 이득을 가져온다.</p>

<p>하지만 이 방법은 <em>Approximate</em> 하는 방법이란 것을 알아야 한다. 아주 극성맞은 경우와 안좋은 파라미터 설정에는 <em>Light Leak</em> 이 발생할 수 있다고 한다. 아래 그림에서 그 경우를 볼 수 있다.</p>

<p><br />
<img src="/images/fts_lightleaks.png" alt="" class="center-image" /></p>
<center>출처 : <a href="http://cwyman.org/papers/tvcg16_ftizbExtended.pdf">Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows</a>
</center>
<p><br /></p>

<p>왼쪽의 그림은 정상적으로 그림자가 보일 때, 두번째는 정말 안좋은 경우들이 겹친 <em>Light Leak</em> 이 발생하는 경우, 세번째 경우는 세팅값을 맞춰주어 <em>Light Leak</em> 을 없엔 장면이다. 하지만 결과를 잘 모르는 경우에는 이 결과들이 맞는지 아닌지 쉽게 구별할 수 있는 정도는 아니다. 즉 아주 정확한 결과를 원하는게 아니라면 그냥 써도 된다는 말이다.</p>

<p>두번째는 데이터 구조와 메모리 레이아웃 최적화다. 가장 맨처음 이를 구현할 때는 링크드 리스트의 2D 그리드의 형태로 만들었다고 한다. 각각의 리스트의 노드는 다음노드를 가리키는 포인터와 <em>G-Buffer</em> 를 참조하기 위한 명시적인<sup id="fnref:C2"><a href="#fn:C2" class="footnote">1</a></sup> 인덱스로 구성되었다고 한다. 하지만 이 구조는 <em>GPU</em> 에서의 두가지 쓰레드 동기화를 필요로 했다. 하나는 <em>Global Node Pool</em> 에서 비어있는 노드를 찾기위한 <em>Global</em> 동기화<sup id="fnref:C1"><a href="#fn:C1" class="footnote">2</a></sup>, 나머지는 헤드 포인터(<em>Light-Space Data</em>)를 업데이트하기 위한 <em>Per-Texel</em> 동기화<sup id="fnref:C1:1"><a href="#fn:C1" class="footnote">2</a></sup>였다. 동기화를 많이 걸면 걸수록 성능상으로는 그다지 좋지않다. 그렇기 때문에 데이터 구조를 바꾸었다고 한다.</p>

<p>여러 시도 끝에 가장 성공적인 결과는 리스트의 각 노드의 크기를 줄이는 것이였다. 이를 위한 준비는 노드를 저장하기 위한 <em>Screen-Space Grid</em> 버퍼를 미리 할당한다. 그리고 각 노드들은 자신을 기준으로한 다음 노드의 오프셋을 저장한다. 이는 <em>Linked-List</em> 의 기준으로 보자면 <em>Next Pointer</em> 가 된다. 이를 논문에서는 간접적인(<em>Implcit</em>) <em>G-Buffer</em> 인덱스라고 부른다. 이렇게 계속 픽셀의 노드 정보를 참조하면서 픽셀의 위치를 <em>Linked-List</em> 의 형태로 나타낼 수 있는 것이다. 아래 중간의 그림의 노란색 화살표는 이를 간단하게 나타냈다.</p>

<p><br />
<img src="/images/fts_SimpleLayout.png" alt="" class="center-image" /></p>
<center>출처 : <a href="http://cwyman.org/papers/tvcg16_ftizbExtended.pdf">Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows</a>
</center>
<p><br /></p>

<p>이는 <em>IZB Node</em> 의 크기도 반으로 줄이고, 위에서 언급한 두가지의 동기화 중 <em>Global</em> 동기화를 안할 수 있게 되었다. 32spp 그림자를 보여주기 위해서는 적어도 픽셀별로 8개의 노드가 필요했다. 이렇게 반으로 노드의 크기를 줄임으로써 큰 퍼포먼스 향상을 얻게 되었다.</p>

<p>세번째는 헤드 포인터를 가지고 있는 <em>Light-Space Buffer</em> 의 해상도다. 일반적인 <em>Shadow Mapping</em> 기법의 <em>Shadow Map</em> 의 해상도는 보여지는 정도를 결정하지만, 여기서의 해상도는 퍼포먼스를 결정한다.(<em>La</em>) 1920 x 1080 을 기준으로 추천하는 해상도는 1400 ~ 2500 사이라고 한다.</p>

<p>네번째로는 기존의 <em>Shadow Mapping</em> 의 잘 알려진 기법인 <em>Cascaded Shadow Mapping</em><sup id="fnref:P1"><a href="#fn:P1" class="footnote">3</a></sup> 을 이 기법에 적용시키는 것이다. 이 기법의 원리는 <em>View frustum</em> 을 원하는 갯수대로 쪼갠 후, 쪼개진 <em>frustum</em> 안의 오브젝트들의 <em>Shadow</em> 를 계산한다. 논문에서 쪼개는 방법은 <em>Sample Distribution Shadow Map</em> 과 <em>Logarithm Partitioning</em> 을 언급했다. 여기서는 쪼개진 <em>frustum</em> 마다 전부 <em>IZB</em> 를 생성한다. 이때 각각의 쪼개진 <em>frustum</em> 의 끝부분이 잘 맞도록 신경써야줘야 한다고 언급했다. 논문의 저자는 구현할때 <em>2D Texture Array</em> 를 사용하여 <em>IZB</em> 를 저장하고, 병렬로 각각의 <em>Detph Texture</em> 마다 <em>Light-Space Culling Prepass</em> 를 넣어줬다고 한다. 일반적으로 각각의 <em>Cascade</em> 를 계산할때는 한개당 하나의 <em>Pass</em> 를 사용하여 계산하는데, 여기서는 1 Pass 로 적절히 프리미티브를 나누어 성능 향상을 고려했다고 한다.</p>

<p><em>Cascade</em> 의 적용은 <em>Occluder Geometry</em> 의 <em>Rasterize</em> 퍼포먼스를 안고 가면서 <em>Thread Divergence</em> 의 시간을 줄여준다. 이는 사용시 적절한 타협점을 찾아야 한다는 뜻으로, 보통은 두개의 <em>Cascade</em> 를 사용하고, 복잡한 게임에서는 3개나 4개의 <em>Cascade</em> 를 사용하여 상황에 따라 뜀뛰는 시간을 최소화 시킨다. 아래 그림은 <em>Cascade</em> 의 효과를 증명해준다.</p>

<p><br />
<img src="/images/fts_cascaded-izb.png" alt="" class="center-image" /></p>
<center>출처 : <a href="http://cwyman.org/papers/tvcg16_ftizbExtended.pdf">Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows</a>
</center>
<p><br /></p>

<p>다섯번째로는 <em>N dot L Culling</em> 이다. 일반적으로 <em>N dot L</em> 의 값이 음수가 되는 경우에는 0으로 클램핑하여 사용한다. 이 말은 값이 음수나 0 인 경우에는 무조건 <em>Shadow</em> 가 비춘다는 말이다. 이때는 <em>La</em> : 평균적인 <em>IZB</em> 리스트의 길이를 줄여 성능 향상을 해줄 수 있다고 한다. 보통은 10 ~ 15% 의 성능향상을 해준다고 한다.</p>

<p>여섯번째로는 <em>Early-Z</em> 의 개념을 응용한 <em>Early-Out</em> 이다. <em>Visibility Test</em> 에서 한 픽셀을 완전하게 그림자를 드리우는 경우, 다음 후속으로 같은 픽셀에 <em>Node</em> 가 추가될 필요가 없다. 그러므로 완전히 그림자 처리가 되는 부분은 <em>Node</em> 를 지워준다. 이때 <em>atomic</em> 연산을 사용하지 않는데, 최악에 경우에는 <em>Visibility Test</em> 를 다시할 수도 있다. <em>Early-Out</em> 은 추가적인 시간과 메모리를 요구함에도 불구하고 10 ~ 15% 의 성능향상을 보인다.</p>

<p>일곱번째로는 <em>Unchanged Mask</em> 를 이용한 메모리 동기화다. <em>Visibility Test</em> 는 메모리 대역폭, 처리량, 동기화로 인하여 병목이 일어난다. 픽셀 각각의 <em>Visibility Mask</em> 를 사용해 동기화를 한다. 정확히 말하면 각각의 폴리곤들이 픽셀의 가시성을 테스트 할때 <em>Race Condition</em> 을 피하기 위하여 동기화를 하여 <em>Visibility</em> 를 기록한다. 그러므로 <em>Visibility Mask</em> 는 반드시 폴리곤이 기존의 <em>Visibility</em> 를 바꿀 때만 업데이트된다. 바뀌는지 비교를 하기위해 이전에 사용한 마스크를 써야하지만, 이는 최고 14% 의 성능 향상을 보여준다고 한다.</p>

<p>마지막으로는 코드를 통한 <em>Latency Hiding</em> 이다. 단계가 복잡하여 <em>Memory Latency</em> 가 꽤나 긴편인데, GPU 에서는 이 <em>Latency</em> 를 감출 방법이 없다. 다행히 사전에 루프를 돌면서 <em>G-Buffer</em> 좌표를 계산하여 <em>Latency Hiding</em> 이 가능하다고 한다. 이는 5 ~ 15% 성능 향상을 보였다고 한다.</p>

<p>시스템 구현에 대한 디테일한 사항은 여기까지가 끝이다.</p>

<p>마지막으로 <em>Transparency Geometry</em> 를 처리하는 방법에 대해서 써보겠다. 이 기법의 <em>per-pixel</em> 테스트는 <em>Visibility Mask Buffer</em> 에 결과가 저장된다. <em>Visibility Mask Buffer</em> 의 효율적인 사용을 위해 항상 각 픽셀의 여러개의 32bit 데이터를 저장해준다고 한다. 이정도의 크기라면 단지 <em>Visibility</em> 만을 사용하는게 아니라 <em>Opacity</em> 또한 저장이 가능하다. 통상적인 가시성을 위한 투명 오브젝트의 처리 방법은 <em>Alpha to Coverage</em><sup id="fnref:C3"><a href="#fn:C3" class="footnote">4</a></sup> 를 쓴다고 한다. 그리고 여기에서도 비슷한 방법을 사용할 수 있다고한다.</p>

<p>처음에는 <em>Coverage</em> 를 계산하기 위해 <em>Visibility Test</em> 를 해준다. 그리고 해당 알파가 저장된 텍스쳐를 참조하여 투명도를 가져오고, 해당 투명도를 사용하여 <em>Alpha to Coverage</em> 를 실행하여 투명도 마스크를 얻는다. 이를 비트 AND 연산으로 합쳐서 <em>Coverage</em> 를 <em>Visibility Buffer</em> 에 저장한다.</p>

<p>이 기법에서 알파 데이터를 처리하는 방법은 두가지로 나뉜다. 적은 비용으로 <em>Aliasing</em> 을 생기게 하는 방법과 높은 비용으로 완벽하게 구현하는 방법으로 나뉜다. 적은 비용의 방식은 <em>Alpha</em> 텍스쳐의 값을 <em>IZB Node</em> 를 순회하기 전에 가져와서 계산하는 방식이다. <em>Light-Space Texel</em> 을 기준으로 계산하므로 <em>Aliasing</em> 이 생길 것으롸 예상된다. 하지만 이 논문의 저자는 구현물을 이 방식으로 구현했다고 한다. 나머지 한개의 방식은 <em>IZB Node</em> 를 하나하나 순회하면서 <em>Alpha</em> 텍스쳐의 값을 가져와 계산하는 것이다. 이는 일반적으로 생각되는 텍스쳐 샘플링의 부하와 <em>Varing</em> 부하를 생기게 한다. 이는 꽤나 큰 비용이라고 한다.</p>

<p>여기까지가 끝이다. 논문에 그 다음 내용들은 전부 퍼포먼스들의 분석밖에 없다. 다음으로 쓸 내용은 <em>HFTS</em> 에 대한 내용이다.</p>

<h2>참조</h2>
<ul>
  <li><a href="http://cwyman.org/papers/tvcg16_ftizbExtended.pdf">Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows</a></li>
  <li><a href="https://software.intel.com/en-us/articles/sample-distribution-shadow-maps">Intel Developer Zone : Sample Distribution Shadow Maps</a></li>
</ul>

<div class="footnotes">
  <ol>
    <li id="fn:C2">
      <p>명시적이란 뜻은 바로 넣어서 계산할 수 있는 절대적인 위치의 텍셀 인덱스를 뜻한다. <a href="#fnref:C2" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:C1">
      <p>여기서의 동기화는 <em>atomic</em> 의 개념을 말한다. 성능상의 단점은 다른 쓰레드에서 선점하는 경우에는 기다리는 것이다. <a href="#fnref:C1" class="reversefootnote">&#8617;</a> <a href="#fnref:C1:1" class="reversefootnote">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:P1">
      <p>이 블로그에서 <em>Cascaded Shadow Mapping</em> 에 대한 내용을 다루었었다. <a href="/2017/12/17/cascaded-shadow-mapping/">여기</a>에서 볼 수 있다. <a href="#fnref:P1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:C3">
      <p><a href="https://medium.com/@bgolus/anti-aliased-alpha-test-the-esoteric-alpha-to-coverage-8b177335ae4f">https://medium.com/@bgolus/anti-aliased-alpha-test-the-esoteric-alpha-to-coverage-8b177335ae4f</a> <a href="#fnref:C3" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

        </div>

        
      </li>
    
      
      

      <li>
        <header class="post-header">
          <h1 class="post-title">
            <a class="post-link" href="/2018/01/13/frustum-traced-shadow-with-irrelgular-z-buffer-1/">Frustum Traced Shadow With Irrelgular Z Buffer 1</a>
          </h1>

          <p class="post-meta">Jan 13, 2018 • 
  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/shader/">shader</a>,
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/shadow/">shadow</a>,
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/rendering/">rendering</a>,
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

  
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <a href="/categories/fts/">fts</a>
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  

</p>
        </header>

        <div class="post-content">
          <p><a href="/2018/01/13/frustum-traced-shadow-with-irrelgular-z-buffer-0/">frustum Traced Shadow with Irregular Z-Buffer 0</a> 에서 기법의 아이디어를 둘러봄으러써 대강 이 알고리즘이 무엇인지 살펴보았다. 이번 글에서는 논문에 수록된 포괄적인 전체 시스템과 복잡도에 대하여 알아볼 것이다.</p>

<h3>전체 시스템</h3>

<p><a href="/2018/01/13/frustum-traced-shadow-with-irrelgular-z-buffer-0/">이전 글</a>에서 두가지 단계에 대해서 자세한 설명을 했었다. <em>Irregular Z-Buffer</em> 를 생성하고 <em>Visibility Test</em> 를 하는 것이였다. 실제 구현된 단계는 총 6개의 단계로 이루어진다고 한다.</p>

<p>첫번째로는 <em>Eye-Space Z-Prepass</em> 를 해준다. 요즘의 엔진들이나 큰 규모가 아닌 게임이더라도 <em>Z-Prepass</em><sup id="fnref:L1"><a href="#fn:L1" class="footnote">1</a></sup> 는 거의 대부분 해준다. <em>Geometry Pass</em> 가 두번 걸리기는 하지만 <em>Fill Rate</em> 의 부하가 <em>Geometry Pass</em> 의 부하보다 많이 커서 그런 듯하다. 중요한건 단순히 언급한 <em>Eye-Space Z-Prepass</em> 를 뜻하는게 아니다. 이전에 언급한 <em>μQuad</em> 의 빠른 계산을 위해 <em>G-Buffer</em> 에 3개의 실수 값들을 넣는다. 이 3개의 실수는 실제 그려지는 카메라의 위치와 <em>Tangent Plane</em> 의 4개의 코너중에 3개의 거리를 나타낸다. 이는 <em>μQuad</em> 를 다시 계산하기에 충분하다고 한다.</p>

<p>이 방법은 <em>Visibility Test</em> 의 속도를 빠르게 하는데 도움이 되지만, 당연히 <em>G-Buffer</em> 의 공간이 부족한 경우에는 쓰지 못한다. AAA 급의 게임들은 <em>G-Buffer</em> 를 bit 단위로 최대한 아껴쓰기 경우가 많기 때문에 이와 같은 상황이 일어날 수도 있다. 이런 경우에는 명시적으로 <em>Visibility Test</em> 를 할때 <em>μQuad</em> 를 계산한다고 한다. 아래 그림은 2009년에 발매된 <em>KillZone 2</em> 의 <em>G-Buffer</em> 사용을 나타내는 PT의 한 부분이다.</p>

<p><br />
<img src="/images/killzone2_g-buffer.png" alt="" class="center-image" /></p>
<center>출처 : <a href="https://www.slideshare.net/guerrillagames/deferred-rendering-in-killzone-2-9691589">Deferred Rendering in Killzone 2</a>
</center>
<p><br /></p>

<p>두번째로는 씬의 경계를 설정해주는 것이다. <em>Shadow Mapping</em> 에서 <em>Light-Space Projection</em> 행렬은 씬에 딱 맞게 해주어야 한다.<sup id="fnref:C1"><a href="#fn:C1" class="footnote">2</a></sup> 딱 맞지 않는 경우에는 <em>IZB</em> 에 쓸모없는 텍셀을 생기게 하기 때문이다. 그래서 <em>Light-Space Projection</em> 행렬을 계산하기 위해 <em>Compute Shader</em> 와 <em>Z-Buffer</em> 를 사용하여 <em>Bounding Box</em> 를 계산한다. 이 계산은 화면의 해상도에 따라서 달라진다. 하지만 논문의 저자는 이 비용이 병목의 큰 원인이 아니기 때문에 특별한 해결 방법을 제시하지는 않는다.</p>

<p>세번째는 <em>Irregular Z-Buffer</em> 를 만드는 것이다. 이전 글에서 <em>IZB</em> 에 대한 대략적인 아이디어는 언급했었다. 더 디테일하게 이를 말해보면, 우선 <em>Eye-Space Z Buffer</em> 를 참조해 <em>Light-Space</em> 로 변환한 후에 <em>Light-Space A-Buffer</em> 의 텍셀의 <em>Linked-List</em> 에 넣는다.</p>

<p><em>IZB</em> 의 발상은 <em>Eye-Space</em> 의 픽셀과 <em>Light-Space</em> 의 텍셀이 1:1 로 매칭되지 않고 하나의 텍셀이 참조당하는 횟수가 1을 넘을때 <em>allasing</em> 이 발생하는 것에서 시작되었다. 그래서 여기서 구현된 <em>IZB</em> 는 텍셀에 <em>Linked-List</em> 의 개념을 도입하여 보다 정확히 계산할 수 있게 하였다.</p>

<p>아래 그림은 <em>IZB</em> 의 데이터를 나타낸다.</p>

<p><br />
<img src="/images/fts_depth_length_cull.png" alt="" class="center-image" /></p>
<center>출처 : <a href="http://cwyman.org/papers/tvcg16_ftizbExtended.pdf">Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows</a>
</center>
<p><br /></p>

<p>왼쪽의 그림은 일반적인 <em>Shadow Map</em> 을 나타내고, 중간의 그림은 <em>Linked-List</em> 의 크기를 나타낸다. 흰색은 <em>Linked-List</em> 의 크기가 0인 텍셀을 나타내고, 검은색에 가까워질수록 <em>Linked-List</em> 의 크기가 점점 커지는 것을 나타낸다. 오른쪽의 그림은 필요없는 부분을 0으로 나타내고, 나머지 부분은 0 이상의 숫자를 나타내는 방식이다. 이는 아래에서 언급할 <em>Light-Space Culling Prepass</em> 에서 쓰인다.</p>

<p>픽셀별로 여러개의 가시성(가려진 정도)를 나타내는 픽셀은 여러개의 <em>IZB Node</em> 를 필요로 한다. 일반적인 <em>Shadow Map</em> 과는 다르게 <em>μQuad</em> 는 다른 <em>Light-Space Texel</em> 에 <em>Projection</em> 이 가능하다.</p>

<p>대충 생각해보면, <em>N</em> 개의 픽셀별 쉐도우가 필요하면, <em>N</em> 개의 <em>IZB Node</em> 가 필요하다. 하지만 쓸데없는 데이터를(<em>Geometry</em> 가 없는 경우) 넣지 않을 수 있으므로 <em>M</em> 개의 <em>Light-Space Texel</em> 에 <em>μQuad</em> 를 투영한다면, 우리는 min(<em>N</em>, <em>M</em>)개의 <em>IZB Node</em> 가 필요하다고 알 수 있다.</p>

<p>아래 그림은 <em>IZB</em> 를 사용하는 디테일한 구조를 나타낸다.</p>

<p><br />
<img src="/images/fts_SimpleLayout.png" alt="" class="center-image" /></p>
<center>출처 : <a href="http://cwyman.org/papers/tvcg16_ftizbExtended.pdf">Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows</a>
</center>
<p><br /></p>

<p><em>IZB</em> 의 데이터 구조는 <em>Light-Space A-Buffer</em> : <em>Eye-Space A-Buffer</em> 의 <em>Node</em> 를 찾아가기 위한 데이터, <em>Eye-Space A-Buffer</em> : 리스트의 크기를 나타내는 버퍼와 같다. 출력되는 결과 : <em>Visibility Buffer</em> 또한 따로 존재한다. 이게 최종적으로 그려질때 사용된다.</p>

<p>다음은 <em>Light-Space Culling Prepass</em> 다. <em>Visibility Test</em> 를 위해 <em>Geometry</em> 의 <em>Light-Space Conservative Rasterization</em> 을 하게 되는데, GPU 에서 <em>Early-Z</em> 기능을 제공한다면 쓸데없는 부분(아무것도 없는 텍셀, 가장 끝)을 컬링할 수 있다.</p>

<p>하지만 <em>Early-Z</em> 하드웨어를 사용할때는 <em>Light-Space Z-Buffer</em> 를 필요로 한다. 이는 조금 당황스러운 상황을 만든다. 그래서 이 단계에서는 반드시 스텐실 버퍼를 만들어야 한다. 위에서 언급한 쓸데없는 부분(아무것도 없는 텍셀, 가장 끝)의 <em>Depth</em> 를 0으로 세팅한다. 나머지는 0 이상의 숫자로 세팅한다.</p>

<p>논문의 저자에 따르면, 엄청 큰 씬을 제외하고는 30% ~ 50% 의 성능 향상을 보였다고 한다. 이는 <em>Conservative Rasterization</em> 으로 생성된 <em>Fragment</em> 의 절대적인 숫자를 줄이고, 아무것도 없는 리스트를 스킵하면서 길이의 다양성을 없엔 효과다.</p>

<p>컬링을 된 이후에는 픽셀별로 <em>Visibility Test</em> 를 해준다. 이때 각각의 폴리곤들은 임의의 텍셀 집합을 가리기 된다. 그리고 각각의 텍셀이 가진 리스트를 순회하면서 <em>Visibility Test</em> 를 한다. 이때 <em>atomic OR</em> 을 사용하여 <em>Visibility Buffer</em> 에 기록한다. 여기서 가장 병목이 되는 구간은 각각의 폴리곤이 임의의 서로다른 길이를 가진 텍셀의 리스트를 커버링하여 각각의 쓰레드별로 실행되는 시간이 제각각이 된다. 문제는 시간이 제각각인 경우, 가장 느린 시간을 소모한 쓰레드를 기준으로 <em>divergence</em> 하여 각각의 텍셀의 리스트의 길이 중 가장 긴 길이의 시간으로 맞춰진다. 이는 최악의 상황을 유발할 수 있다.</p>

<p>마지막으로 <em>Visibility Buffer</em> 를 사용하여 실제 오브젝트들을 렌더링하면 된다.</p>

<h3>복잡도 계산</h3>

<p>위에서도 언급햇다시피 이 기법은 <em>N</em> 번의 <em>Visibility Test</em> 한다면, 시간 복잡도는 O(<em>N</em>) 과 같다. 여기서 <em>N</em> 을 분해하면 다음과 같다. O(<em>La</em> * <em>F</em>), <em>F</em> 는 <em>Light-Space</em> 의 <em>Fragment</em> 갯수이고, <em>La</em> 는 <em>Linked-List</em> 의 평균 길이다.</p>

<p><em>La</em> 또한 다른 요소로 나타낼 수 있다. <em>Visibility Test</em> 는 <em>Eye-Space</em> 에서 한다. 그래서 <em>IZB Node</em> 의 갯수는 <em>Eye-Space</em> 의 해상도에 비례한다. 그리고 <em>Eye-Space</em> 의 데이터는 전부 <em>Light-Space</em> 에 기록되므로 <em>La</em> ≈ (<em>Eye-Space Resolution</em>) / (<em>Light-Space Resolution</em>) 으로 계산될 수 있다.</p>

<p>하지만 위에서 언급했던 것과 같이 <em>SIMT</em> 기반의 GPU 에서는 O(<em>La</em> * <em>F</em>) 가 아닌 O(<em>Lm</em> * <em>F</em>) 이 될 수 밖에 없다. <em>Lm</em> 은 <em>Linked-List</em> 의 최대 길이다. 결국 퍼포먼스를 내기 위해선 <em>Lm</em> 의 길이를 줄이는 것이 가장 중요하다는 것이 된다.</p>

<p>이번 글에서는 전체 시스템과 복잡도에 대해서 알아보았다. 다음은 더 디테일한 구현 부분의 내용을과 <em>Alpha</em> 처리에 대한 부분을 알아볼것이다.</p>

<h2>참조</h2>
<ul>
  <li><a href="http://cwyman.org/papers/tvcg16_ftizbExtended.pdf">Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows</a></li>
</ul>

<div class="footnotes">
  <ol>
    <li id="fn:L1">
      <p>https://ypchoi.gitbooks.io/rendering-techniques/content/z_prepass.html <a href="#fnref:L1" class="reversefootnote">&#8617;</a></p>
    </li>
    <li id="fn:C1">
      <p>Cascaded Shadow Map 의 Crop Matrix 를 떠올리면 된다. <a href="#fnref:C1" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

        </div>

        
      </li>
    
  </ul>

  
  <div class="pagination">
    
      <a class="previous" href="/posts/2/">&laquo; Older</a>
    

    
  </div>



</div>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <p>
      

Content on this site is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a><br />
&copy; 2015&ndash;2018 - <a href="/about/">Su-Hyeok Kim</a> - Powered by <a href="https://jekyllrb.com">Jekyll</a> &amp; <a href="https://github.com/yous/whiteglass">whiteglass</a> - Subscribe via <a href="https://hrmrzizon.github.io/feed.xml">RSS</a>

    </p>

  </div>

</footer>


  </body>

</html>

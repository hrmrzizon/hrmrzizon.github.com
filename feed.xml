<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="ko">
  <generator uri="http://jekyllrb.com" version="3.8.2">Jekyll</generator>
  
  
  <link href="https://hrmrzizon.github.io/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://hrmrzizon.github.io/" rel="alternate" type="text/html" hreflang="ko" />
  <updated>2018-05-25T15:14:47+00:00</updated>
  <id>https://hrmrzizon.github.io//</id>

  
    <title type="html">Appocrypha</title>
  

  
    <subtitle>store limitless knowledges</subtitle>
  

  
    <author>
        <name>Su-Hyeok Kim</name>
      
      
    </author>
  

  
  
    <entry>
      
      <title type="html">Opaque As Alpha Test</title>
      
      <link href="https://hrmrzizon.github.io/2018/05/22/opaque-as-alpha-test/" rel="alternate" type="text/html" title="Opaque As Alpha Test" />
      <published>2018-05-22T00:00:00+00:00</published>
      <updated>2018-05-22T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2018/05/22/opaque-as-alpha-test</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2018/05/22/opaque-as-alpha-test/">&lt;p&gt;&lt;em&gt;Shader&lt;/em&gt; 에서 샘플링하는 &lt;em&gt;Texutre&lt;/em&gt; 에서 &lt;em&gt;Alpha&lt;/em&gt; 값을 가지고 있어, &lt;em&gt;Alpha&lt;/em&gt; 을 참조해서 실제 픽셀에 출력을 하는지 안하는지를 결정하는 것을 &lt;em&gt;Alpha Test&lt;/em&gt; 라고 한다. 이런 &lt;em&gt;Material&lt;/em&gt; 이나 &lt;em&gt;Texture&lt;/em&gt; 를  &lt;em&gt;Cutout&lt;/em&gt; 이라고 통칭하는 경우가 많다.&lt;/p&gt;

&lt;p&gt;보통 게임에서의 &lt;em&gt;Alpha Test&lt;/em&gt; 를 사용하는 것들은 나무, 풀 같은 식생들(&lt;em&gt;Vegetation&lt;/em&gt;)이 있고, 중간에 구멍이 뚫린 펜스같은 것들도 존재한다. 자연을 배경으로하는 게임의 경우에는 식생들이 굉장히 많기 때문에 &lt;em&gt;Alpha Test&lt;/em&gt; 를 사용하는 &lt;em&gt;Shader&lt;/em&gt; 가 굉장히 많이 사용될 것이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/1_8EKqWSOOPXaTrDHVFTACJg.png&quot; alt=&quot;Wikipedia : Single-precision floating-point format&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://medium.com/@bgolus/anti-aliased-alpha-test-the-esoteric-alpha-to-coverage-8b177335ae4f&quot;&gt;Anti-aliased Alpha Test: The Esoteric Alpha To Coverage&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;하지만 &lt;em&gt;Alpha Test&lt;/em&gt; 는 굉장히 큰 단점이 있다. 고정된 화면 해상도에서 물체가 작게 표현되면 물체를 표현할 수 있는 픽셀의 숫자가 많이 작아진다. 물체를 표현하는 픽셀의 수가 작아지게 되면 일반적으로 해당 넓이에 맞게 생성된 &lt;em&gt;Texture&lt;/em&gt; 의 &lt;em&gt;Mip-level&lt;/em&gt; 에 접근한다. 중간의 &lt;em&gt;Alpha Test&lt;/em&gt; 그림을 보면된다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/1_zNbZFiJXjcqqyTkM9eEt7w.gif&quot; alt=&quot;Wikipedia : Single-precision floating-point format&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://medium.com/@bgolus/anti-aliased-alpha-test-the-esoteric-alpha-to-coverage-8b177335ae4f&quot;&gt;Anti-aliased Alpha Test: The Esoteric Alpha To Coverage&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;실제로는 양 옆의 물체들처럼 자연스럽게 표현이 되야하지만 일반적인 &lt;em&gt;Alpha Test&lt;/em&gt; 를 사용하게 되면 위와 같은 현상에 마주치게 된다. 이는 굉장히 끔찍한 현상이다. 실제 게임을 해보거나, 만들어본 사람이라면 안다. 대부분의 픽셀에 나무가 표현되고, 잎사귀들이 저런식으로 자글자글 거린다면 약간의 불쾌함이 느껴진다. VR 이라면 더욱..&lt;/p&gt;

&lt;p&gt;그래서 급하게 대처방안으로 나온 것이 위 그림의 오른쪽에 나오는 &lt;em&gt;Alpha to Coverage&lt;/em&gt; 라는 방법이다. 이는 하드웨어 &lt;em&gt;MSAA&lt;/em&gt; 를 픽셀 쉐이더의 결과를 통해 자동으로 해주는것으로, &lt;em&gt;MSAA&lt;/em&gt; 의 퍼포먼스와 비례한다. &lt;em&gt;MSAA&lt;/em&gt; 는 성능이 영 좋지않아 안쓰는 경우가 꽤 많이 존재하기 때문에 &lt;em&gt;Alpha to Coverage&lt;/em&gt; 는 절대적으로 사용할 수 있는 방법은 아니다. 게다가 엄청나게 많은 나무를 &lt;em&gt;Alpha to Coverage&lt;/em&gt; 를 쓴다면.. 성능은 안봐도 뻔하다.&lt;/p&gt;

&lt;p&gt;앞서 말한 &lt;em&gt;Alpha Test&lt;/em&gt; 은 &lt;em&gt;Material&lt;/em&gt;, &lt;em&gt;Shader&lt;/em&gt; 별로 고정된 &lt;em&gt;Alpha&lt;/em&gt; 값을 설정해 그 이하가 되면 &lt;em&gt;Pixel Shader&lt;/em&gt; 에서 결과를 내놓지 않게 하는(&lt;em&gt;Discard&lt;/em&gt;) 방법이였다. &lt;em&gt;Alpha Test&lt;/em&gt; 의 문제는 샘플링한 &lt;em&gt;Alpha&lt;/em&gt; 값이 가끔 극단적으로 낮아서 &lt;em&gt;Discard&lt;/em&gt; 되는 것인데, 이를 간단하게 해결하기 위해 요상한 방법이 등장했다.&lt;/p&gt;

&lt;p&gt;바로 &lt;em&gt;Stochastic test&lt;/em&gt; 라는 방법이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/stochastic_sampling.png&quot; alt=&quot;NVidia deverloper : Hashed Alpha Testing&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://developer.download.nvidia.com/assets/gameworks/downloads/regular/GDC17/RealTimeRenderingAdvances_HashedAlphaTesting_GDC2017_FINAL.pdf?pUIX8DXxfad7mL4zB3GOthX3r5IgGao9UWxYuYb3q9h10RXrQeYko-dEuJXJxt1hhsI9J_9KJDcCYGeWWksxlaHTrXSE825D_3izja7LUFOtzhaeBUqpn7qbwXaaGlLdbipjE3PeI3e2IMn45mQAA3OV2PD-kG2y9cecTaWE2uum2uwdHgyn0nhYiLOvlOsrUzewbK5REH7vAm3-lNWzxehw_5Tphg&quot;&gt;NVidia developer : Hashed Alpha Testing&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위 그림에서 위쪽에 있는 것이 일반적인 &lt;em&gt;Alpha Test&lt;/em&gt; 인데, &lt;em&gt;color.a&lt;/em&gt; 는 텍스쳐에서 샘플링한 &lt;em&gt;Alpha&lt;/em&gt; 값, &lt;em&gt;ατ&lt;/em&gt; 는 &lt;em&gt;Alpha Test&lt;/em&gt; 를 위한 고정된 &lt;em&gt;Alpha Threshold&lt;/em&gt;(&lt;em&gt;알파한계&lt;/em&gt;)다. 밑의 코드에서 &lt;em&gt;drand48&lt;/em&gt; 이 나타내는 것은 단순한 0 ~ 1 사이의 랜덤값이다. 즉 랜덤하게 &lt;em&gt;Alpha Threshold&lt;/em&gt; 를 설정해주어 물체가 멀어져서 평균 &lt;em&gt;Alpha&lt;/em&gt; 값이 낮아질 때도 픽셀이 &lt;em&gt;Discard&lt;/em&gt; 되지 않도록 하는 것이다. 하지만 이는 굉장한 눈아픔? 반짝거림? 을 유발한다. 범위를 지정해주지 않았기 때문에 이전 프레임에서 출력된 픽셀이 다음 프레임에서는 출력되지 않을 수도 있다. 이렇게 각 프레임마다 상황이 달라서 생기는 현상앞에 &lt;em&gt;Temporal&lt;/em&gt; 을 붙인다. &lt;em&gt;Stochastic Alpha Test&lt;/em&gt; 의 문제는 &lt;em&gt;Temporal Flickering&lt;/em&gt; 이라고 할 수 있겠다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Temporal Flickering&lt;/em&gt; 이 없는, &lt;em&gt;Temporal Stability&lt;/em&gt;(임시적 안정성) 을 확보하기 위해서는 &lt;em&gt;Alpha Threshold&lt;/em&gt; 를 이러저리 튀지 않게해야 했고, 이를 위해 특정 값에 따라서 &lt;em&gt;Hash&lt;/em&gt; 값을 생성하는 방법이 고안되었다. 이 방법은 &lt;em&gt;Hashed Alpha Test&lt;/em&gt; 라는 이름으로 작년에 공개되었다.&lt;/p&gt;

&lt;h2&gt;Hashed Alpha testing&lt;/h2&gt;

&lt;p&gt;기본적으로 랜덤 값(난수) 생성은 제대로된 난수생성이 아닌, 특수한 식을 사용해서 의사 난수 생성 방법을 이용하는데, &lt;em&gt;Hash&lt;/em&gt; 를 이용한 난수생성은 일반적으로 많이 쓰인다고 한다. &lt;em&gt;Hashed Alpha Testing&lt;/em&gt; 은 &lt;em&gt;Hash&lt;/em&gt; 를 생성하기 위한 &lt;em&gt;Key&lt;/em&gt; 값을 선정하는데 조심스러웠다고 한다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Key&lt;/em&gt; 로 선정될 수 있는 후보는 &lt;em&gt;Texture Coordinate&lt;/em&gt;, &lt;em&gt;World-Space Coordinate&lt;/em&gt;, &lt;em&gt;Ojbect-Space Coordinate&lt;/em&gt; 이 세가지 였다고 한다. &lt;em&gt;Texture Coordinate&lt;/em&gt; 는 가끔 없는 경우가 있어 제외하였고, &lt;em&gt;World-Space Coordinate&lt;/em&gt; 는 정적 물체에는 원하는대로 동작하지만, 동적 물체의 경우에는 문제가 있었다고 한다. 결국 남은건 &lt;em&gt;Ojbect-Space Coordinate&lt;/em&gt; 가 남게 되었다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Ojbect-Space Coordinate&lt;/em&gt; 의 &lt;em&gt;X,Y,Z&lt;/em&gt; 세 좌표를 모두 이용하게 되는데, 이는 &lt;em&gt;X,Y&lt;/em&gt; 두개만 이용하게 되면 &lt;em&gt;Hash&lt;/em&gt; 값이 &lt;em&gt;Screen-Space&lt;/em&gt; 에서 생성되어 다른 물체와 겹치게 되면 &lt;em&gt;Alpha to Coverge&lt;/em&gt; 같은 효과를 내게되어 3가지 좌표 모두 &lt;em&gt;Hash&lt;/em&gt; 생성에 사용된다고 한다.&lt;/p&gt;

&lt;p&gt;마지막으로 중요한 포인트는 &lt;em&gt;Temporal Stability&lt;/em&gt; 를 확보하는 것이다. 이해하기 쉽게 설명하자면, 아래와 같은 각 픽셀을 나타내는 그리드안에 점이 있다고 가정해보자. 이 점들이 조금씩 움직여서 계속 픽셀안에 있다면, 같은 &lt;em&gt;Hash&lt;/em&gt; 값을 사용하여 같은 &lt;em&gt;Alpha Threshold&lt;/em&gt; 값을 만들어줘야 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/subpixel_0.png&quot; alt=&quot;Subpixel 0&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;아래 두 그림의 빨간 점의 위치처럼 원래의 픽셀위치를 벗어나게 된다면 새로운 &lt;em&gt;Alpha Threshold&lt;/em&gt; 를 생성해야 하겠지만, 위치가 많이 바뀌지 않는다면 같은 &lt;em&gt;Alpha Threshold&lt;/em&gt; 를 사용해 &lt;em&gt;Flickering&lt;/em&gt; 을 최대한 줄여야 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/subpixel_2.png&quot; alt=&quot;Subpixel 2&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이러한 맥락으로 &lt;em&gt;Hashed ALpha Testing&lt;/em&gt; 은 &lt;em&gt;Temporal Stability&lt;/em&gt; 를 조금 확보하게 된다. 물론 위의 그림은 이해를 돕기위한 용도로, 실제 코드상에서는 다른 방법을 통해 계산된다. 아래 코드를 보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/hat_codesnippet_screenxy.png&quot; alt=&quot;Hashed Alpha Testing&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://cwyman.org/papers/tvcg17_hashedAlphaExtended.pdf&quot;&gt;Cwyman.org : Hashed Alpha Test(Extended)&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위 코드는 픽셀이 가지고 있는 &lt;em&gt;Object-Space Coordinate&lt;/em&gt; 의 옆 픽셀과의 차이, 세로에 있는 픽셀과의 차이를 통한 값으로 계산한다. (dFdX, dFdY 의 자세한 내용은 찾아보거나 &lt;a href=&quot;/2018/03/04/what-is-ddx-and-ddy/&quot;&gt;What is ddx and ddy&lt;/a&gt; 에서 볼 수 있다.) 픽셀별로 값의 차이, 즉 근접한 픽셀의 위치 차이값에 따른값(미분값)과 그 값을 이용해 &lt;em&gt;Object-Space Coordinate&lt;/em&gt; 값에 곱한 값을 &lt;em&gt;Key&lt;/em&gt; 로 두어서 &lt;em&gt;Alpha Threshold&lt;/em&gt; 를 계산한다.&lt;/p&gt;

&lt;p&gt;마지막에 &lt;em&gt;Alpha Threshold&lt;/em&gt; 를 구하는 코드를 보면, &lt;em&gt;Floor&lt;/em&gt; 하는, 올림을 해주어 &lt;em&gt;discrete value&lt;/em&gt; 로 &lt;em&gt;Key&lt;/em&gt; 값을 넣어준다. &lt;em&gt;Floor&lt;/em&gt; 가 의미하는 것은, 선형적인 데이터가 아닌 뚝뚝 끊기는 데이터로 만들어 특정한 값을 넘어야 &lt;em&gt;Key&lt;/em&gt; 값이 바뀌게 하여 &lt;em&gt;Hash&lt;/em&gt; 를 유지해 &lt;em&gt;Flickering&lt;/em&gt; 을 방지하는 것이다. 아래 그림은 &lt;em&gt;floor(x)&lt;/em&gt; 의 그래프다. 즉 코드의 &lt;em&gt;pixScale&lt;/em&gt; 이 크면 클수록 &lt;em&gt;Hash&lt;/em&gt; 의 값은 픽셀의 변화에 따라서 빠르게 바뀌고, 작으면 작을수록(0에 가까워질수록) 픽셀의 변화에 따라서 &lt;em&gt;Hash&lt;/em&gt; 값이 느리게 바뀔 것이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/wolframalpha_floor.gif&quot; alt=&quot;Woflram Alpha : Floor Graph&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://www.wolframalpha.com/input/?i=floor&quot;&gt;Wolframalpha&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이러한 방법은 &lt;em&gt;View-Space&lt;/em&gt; 를 기준으로 &lt;em&gt;X,Y&lt;/em&gt; 좌표가 조금씩 바뀔때는 픽셀끼리의 차이를 계산하기 때문에 안정적이다. 하지만 &lt;em&gt;Z(Depth)&lt;/em&gt; 값이 바뀔때는 많은 &lt;em&gt;Flickering&lt;/em&gt; 을 일으킬 것이다. 이를 해결하기 위해 아래 코드를 보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/hat_codesnippet_screenz0.png&quot; alt=&quot;Hashed Alpha Testing&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://cwyman.org/papers/tvcg17_hashedAlphaExtended.pdf&quot;&gt;Cwyman.org : Hashed Alpha Test(Extended)&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위치의 픽셀별 차이 벡터의 크기를 &lt;em&gt;discrete&lt;/em&gt; 시키는 방법도 좋은 아이디어중 하나다. 하지만 이는 빌보드처럼 큰 크기의 판이 다가오게 된다면 끝부분의 &lt;em&gt;discontinuity&lt;/em&gt; 를 유발하게 된다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/hat_codesnippet_screenz1.png&quot; alt=&quot;Hashed Alpha Testing&quot; class=&quot;center-image&quot; /&gt;
&lt;img src=&quot;/images/hat_codesnippet_lerpscale.png&quot; alt=&quot;Hashed Alpha Testing&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://cwyman.org/papers/tvcg17_hashedAlphaExtended.pdf&quot;&gt;Cwyman.org : Hashed Alpha Test(Extended)&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;그래서 위 코드와 같 &lt;em&gt;discretize&lt;/em&gt; 시킨 올림처리한 값과, 내림처리한 값을 사용한 두 &lt;em&gt;Hash&lt;/em&gt; 값 사이의 보간을 통해서 &lt;em&gt;Alpha Threshold&lt;/em&gt; 를 구해준다. 하지만 이 코드는 아직 문제점이 존재한다. 만약 &lt;em&gt;maxDeriv&lt;/em&gt; 의 값이 0 ~ 1 사이라면 내림값이 반드시 0이 되기 때문에 보간할 값 중 한개의 값이 고정되게 된다. 그래서 아래와 같은 코드를 사용한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/hat_codesnippet_exp2.png&quot; alt=&quot;Hashed Alpha Testing&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://cwyman.org/papers/tvcg17_hashedAlphaExtended.pdf&quot;&gt;Cwyman.org : Hashed Alpha Test(Extended)&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;pixScale&lt;/em&gt; 을 그냥 계산하는 대신, &lt;em&gt;discretize&lt;/em&gt; 된 두개의 스케일값을 2의 지수로 표현하여 값이 0으로 되는 것을 막는다. 이렇게 보간된 값을 사용하여 &lt;em&gt;Alpha Threshold&lt;/em&gt; 를 정해주면 약간의 문제가 생긴다. 보간을 함으로써 균일하지 않게 랜덤값이 분포되었기 때문이다. 그래서 아래와 같은 식을 사용하여 다시 값을 분포시켜준다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/hat_codesnippet_cdf.png&quot; alt=&quot;Hashed Alpha Testing&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://cwyman.org/papers/tvcg17_hashedAlphaExtended.pdf&quot;&gt;Cwyman.org : Hashed Alpha Test(Extended)&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위의 식을 적용하면 모든 값들이 균일하게 분포되어 진정한 랜덤값의 &lt;em&gt;Alpha Threshold&lt;/em&gt; 가 생성된다고 한다. 아래는 전체 코드다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/hat_codesnippet_whole.png&quot; alt=&quot;Hashed Alpha Testing&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://cwyman.org/papers/tvcg17_hashedAlphaExtended.pdf&quot;&gt;Cwyman.org : Hashed Alpha Test(Extended)&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;자세한 사항은 논문에서 확인할 수 있다(&lt;a href=&quot;http://cwyman.org/papers/tvcg17_hashedAlphaExtended.pdf&quot;&gt;[Cwyman17]&lt;/a&gt;). 결과는 아래 유튜브 영상에서 확인할 수 있다.&lt;/p&gt;

&lt;style&gt;.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }&lt;/style&gt;
&lt;div class=&quot;embed-container&quot;&gt;    &lt;iframe title=&quot;YouTube video player&quot; width=&quot;640&quot; height=&quot;390&quot; src=&quot;//www.youtube.com/embed/p4TYf5DDpbQ&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;p&gt;이를 통해 전보다 훨씬 나은 &lt;em&gt;Alpha Test&lt;/em&gt; 품질을 얻을 수 있게 되었다. 하지만 &lt;em&gt;Hashed Alpha Testing&lt;/em&gt; 의 결과는 &lt;em&gt;Stochastic Test&lt;/em&gt; 처럼 픽셀이 흩뿌려진 느낌을 지울 수 없다. 어느정도의 랜덤값에서 생성이되니 이는 어쩔 수 없는 결과다.&lt;/p&gt;

&lt;h2&gt;Alpha Distribution&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Alpha Test&lt;/em&gt; 의 구린 품질을 좀 더 개선할 수 있는 방법이 또 있다. 이번년도 &lt;em&gt;I3D&lt;/em&gt; 에 제출된 &lt;em&gt;Alpha Distribution&lt;/em&gt; 이라는 논문이 있는데, 이는 &lt;em&gt;Hashed Alpha Testing&lt;/em&gt; 처럼 런타임에 계산을 하지않고 각 &lt;em&gt;Mip-level&lt;/em&gt; 의 텍스쳐를 미리 처리해놓는 방법 중에 하나다. 미리 계산된 &lt;em&gt;Texture&lt;/em&gt; 들을 사용하여 일반적인 &lt;em&gt;Alpha Test&lt;/em&gt; 를 그대로 사용하기만 하면 된다. 아직 직접 사용한 예시는 없어 검증되지는 않았지만, 이 방법이 그대로 사용될 수 있다면 &lt;em&gt;Alpha Test&lt;/em&gt; 부분에서는 거의 끝판왕이 될 것 같다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Alpha Distribution&lt;/em&gt; 일반적인 &lt;em&gt;Alpha Test&lt;/em&gt; 를 기준으로 &lt;em&gt;Alpha Threshold&lt;/em&gt; 가 고정되어 있다는 것을 가정한다. 그렇게 되면 &lt;em&gt;Alpha Threshold&lt;/em&gt; 에 따라서 픽셀에 출력이 되냐, 안되냐로  따질 수가 있다.(&lt;em&gt;Binary Visibility&lt;/em&gt;) &lt;em&gt;Binary Visibility&lt;/em&gt; 를 각 &lt;em&gt;Mip-level&lt;/em&gt; 에 맞춰서 고르게 분산(&lt;em&gt;Distribution&lt;/em&gt;)시키는게 &lt;em&gt;Alpha Distribution&lt;/em&gt; 의 목적이다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Alpha Distribution&lt;/em&gt; 은 두가지 분산방법을 사용한다. &lt;em&gt;Error Diffusion&lt;/em&gt; 과 &lt;em&gt;Alpha Pyramid&lt;/em&gt; 이라는 방법을 사용한다. 하나씩 알아보자.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Error Diffusion&lt;/em&gt; 은 하나하나의 픽셀을 순회하면서, 각 픽셀의 &lt;em&gt;Binary Visibility&lt;/em&gt; 에 해당하는 값(0 아니면 1)과 이미지가 가지고 있는 &lt;em&gt;Alpha&lt;/em&gt; 값을 비교해 그 오차(&lt;em&gt;Quantization Error&lt;/em&gt;)를 다른 픽셀에 나누어준다. &lt;em&gt;Binary Visibility&lt;/em&gt; 는 다음과 같이 정해진다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;αˆi = αi &amp;gt;= ατ : 1, αi &amp;lt; ατ : 0&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;αi 는 이미지가 가지고 있는 이산화된 &lt;em&gt;Alpha&lt;/em&gt; 값이고, ατ 는 &lt;em&gt;Alpha Threshold&lt;/em&gt;, 한계값을 뜻한다. αˆi 는 해당 픽셀의 &lt;em&gt;Binary Visibility&lt;/em&gt; 를 뜻한다. 이것을 가지고 &lt;em&gt;Quantization Error&lt;/em&gt; 를 계산한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;ϵi = αi − αˆi&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;ϵi 는 &lt;em&gt;Quantization Error&lt;/em&gt; 를 뜻하고 픽셀이 보이게 된다면 &lt;em&gt;~1 &amp;lt;= ϵi &amp;lt; 0&lt;/em&gt; 의 값을 가지게 되고 픽셀이 보이지 않는다면 &lt;em&gt;0 &amp;lt; ϵi &amp;lt;= 1&lt;/em&gt; 의 값을 가지게 된다. 이런 &lt;em&gt;Quantization Error&lt;/em&gt; 는 인근 픽셀로 분포된다. 아래 그림을 보자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/ad_error_diffusion.png&quot; alt=&quot;Error Diffusion&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그림에서 ϵi 가 들어가 있는 부분이 현재 처리중인 픽셀이며, ϵi 의 값은 인근 픽셀로 고정된 비율로 &lt;em&gt;Alpha&lt;/em&gt; 값에 더해진다. (x+1,y) 는 7/16, (x-1,y+1) 은 3/16, (x,y+1) 은 5/16, (x+1,y+1) 은 1/16 비율로 분포된다. 이런 방법으로 각 픽셀을 순회하면서 처리하면 &lt;em&gt;Error Diffusion&lt;/em&gt; 은 간단하게 끝난다. 오차 확산이라는 이름이 굉장히 직관적이다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Error Diffusion&lt;/em&gt; 은 픽셀과 픽셀사이의 &lt;em&gt;Alpha&lt;/em&gt; 값을 고르게 분포시킨다. 하지만 약간의 문제가 존재한다. 보이게 되던, 안보이게 되던 &lt;em&gt;Alpha&lt;/em&gt; 값이 0.3 ~ 0.7 정도로 중간값을 가지고 있다면, 한 픽셀은 강조되고, 옆의 픽셀은 보이지 않게 된다. 이러한 방법은 아래 이미지와 비슷한 결과를 만든다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/Michelangelo's_David_-_Floyd-Steinberg.png&quot; alt=&quot;Michelangelo's_David_-_Floyd-Steinberg&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://en.wikipedia.org/wiki/Dither&quot;&gt;Wikipedia : Dither&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Error Diffusion&lt;/em&gt; 의 문제는 위 그림처럼 비슷한 색 영역에 있어도 분산된 영향을 받아서 각 픽셀이 부드럽게 보이지 않는 현상이 발생한다. 이러한 특징을 &lt;em&gt;Dithering&lt;/em&gt; 이라고 부른다. 그래서 이보다 나은 품질을 위해 &lt;em&gt;Alpha Pyramid&lt;/em&gt; 라는 다른 방법이 소개된다.&lt;/p&gt;

&lt;h2&gt;참조&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@bgolus/anti-aliased-alpha-test-the-esoteric-alpha-to-coverage-8b177335ae4f&quot;&gt;Anti-aliased Alpha Test: The Esoteric Alpha To Coverage&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://developer.download.nvidia.com/assets/gameworks/downloads/regular/GDC17/RealTimeRenderingAdvances_HashedAlphaTesting_GDC2017_FINAL.pdf?pUIX8DXxfad7mL4zB3GOthX3r5IgGao9UWxYuYb3q9h10RXrQeYko-dEuJXJxt1hhsI9J_9KJDcCYGeWWksxlaHTrXSE825D_3izja7LUFOtzhaeBUqpn7qbwXaaGlLdbipjE3PeI3e2IMn45mQAA3OV2PD-kG2y9cecTaWE2uum2uwdHgyn0nhYiLOvlOsrUzewbK5REH7vAm3-lNWzxehw_5Tphg&quot;&gt;NVidia developer : Hashed Alpha Testing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://cwyman.org/papers/tvcg17_hashedAlphaExtended.pdf&quot;&gt;Cwyman.org : Hashed Alpha Test(Extended)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cemyuksel.com/research/alphadistribution/&quot;&gt;Cemyuksel : Alpha Distribution&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="rendering" />
      
        <category term="alphatest" />
      
        <category term="shader" />
      

      

      
        <summary type="html">Shader 에서 샘플링하는 Texutre 에서 Alpha 값을 가지고 있어, Alpha 을 참조해서 실제 픽셀에 출력을 하는지 안하는지를 결정하는 것을 Alpha Test 라고 한다. 이런 Material 이나 Texture 를 Cutout 이라고 통칭하는 경우가 많다. 보통 게임에서의 Alpha Test 를 사용하는 것들은 나무, 풀 같은 식생들(Vegetation)이 있고, 중간에 구멍이 뚫린 펜스같은 것들도 존재한다. 자연을 배경으로하는 게임의 경우에는 식생들이 굉장히 많기 때문에 Alpha Test 를 사용하는 Shader 가 굉장히 많이 사용될 것이다. 출처 : Anti-aliased Alpha Test: The Esoteric Alpha To Coverage 하지만 Alpha Test 는 굉장히 큰 단점이 있다. 고정된 화면 해상도에서 물체가 작게 표현되면 물체를 표현할 수 있는 픽셀의 숫자가 많이 작아진다. 물체를 표현하는 픽셀의 수가 작아지게 되면 일반적으로 해당 넓이에 맞게 생성된 Texture 의 Mip-level 에 접근한다. 중간의 Alpha Test 그림을 보면된다. 출처 : Anti-aliased Alpha Test: The Esoteric Alpha To Coverage 실제로는 양 옆의 물체들처럼 자연스럽게 표현이 되야하지만 일반적인 Alpha Test 를 사용하게 되면 위와 같은 현상에 마주치게 된다. 이는 굉장히 끔찍한 현상이다. 실제 게임을 해보거나, 만들어본 사람이라면 안다. 대부분의 픽셀에 나무가 표현되고, 잎사귀들이 저런식으로 자글자글 거린다면 약간의 불쾌함이 느껴진다. VR 이라면 더욱.. 그래서 급하게 대처방안으로 나온 것이 위 그림의 오른쪽에 나오는 Alpha to Coverage 라는 방법이다. 이는 하드웨어 MSAA 를 픽셀 쉐이더의 결과를 통해 자동으로 해주는것으로, MSAA 의 퍼포먼스와 비례한다. MSAA 는 성능이 영 좋지않아 안쓰는 경우가 꽤 많이 존재하기 때문에 Alpha to Coverage 는 절대적으로 사용할 수 있는 방법은 아니다. 게다가 엄청나게 많은 나무를 Alpha to Coverage 를 쓴다면.. 성능은 안봐도 뻔하다. 앞서 말한 Alpha Test 은 Material, Shader 별로 고정된 Alpha 값을 설정해 그 이하가 되면 Pixel Shader 에서 결과를 내놓지 않게 하는(Discard) 방법이였다. Alpha Test 의 문제는 샘플링한 Alpha 값이 가끔 극단적으로 낮아서 Discard 되는 것인데, 이를 간단하게 해결하기 위해 요상한 방법이 등장했다. 바로 Stochastic test 라는 방법이다. 출처 : NVidia developer : Hashed Alpha Testing 위 그림에서 위쪽에 있는 것이 일반적인 Alpha Test 인데, color.a 는 텍스쳐에서 샘플링한 Alpha 값, ατ 는 Alpha Test 를 위한 고정된 Alpha Threshold(알파한계)다. 밑의 코드에서 drand48 이 나타내는 것은 단순한 0 ~ 1 사이의 랜덤값이다. 즉 랜덤하게 Alpha Threshold 를 설정해주어 물체가 멀어져서 평균 Alpha 값이 낮아질 때도 픽셀이 Discard 되지 않도록 하는 것이다. 하지만 이는 굉장한 눈아픔? 반짝거림? 을 유발한다. 범위를 지정해주지 않았기 때문에 이전 프레임에서 출력된 픽셀이 다음 프레임에서는 출력되지 않을 수도 있다. 이렇게 각 프레임마다 상황이 달라서 생기는 현상앞에 Temporal 을 붙인다. Stochastic Alpha Test 의 문제는 Temporal Flickering 이라고 할 수 있겠다. Temporal Flickering 이 없는, Temporal Stability(임시적 안정성) 을 확보하기 위해서는 Alpha Threshold 를 이러저리 튀지 않게해야 했고, 이를 위해 특정 값에 따라서 Hash 값을 생성하는 방법이 고안되었다. 이 방법은 Hashed Alpha Test 라는 이름으로 작년에 공개되었다. Hashed Alpha testing 기본적으로 랜덤 값(난수) 생성은 제대로된 난수생성이 아닌, 특수한 식을 사용해서 의사 난수 생성 방법을 이용하는데, Hash 를 이용한 난수생성은 일반적으로 많이 쓰인다고 한다. Hashed Alpha Testing 은 Hash 를 생성하기 위한 Key 값을 선정하는데 조심스러웠다고 한다. Key 로 선정될 수 있는 후보는 Texture Coordinate, World-Space Coordinate, Ojbect-Space Coordinate 이 세가지 였다고 한다. Texture Coordinate 는 가끔 없는 경우가 있어 제외하였고, World-Space Coordinate 는 정적 물체에는 원하는대로 동작하지만, 동적 물체의 경우에는 문제가 있었다고 한다. 결국 남은건 Ojbect-Space Coordinate 가 남게 되었다. Ojbect-Space Coordinate 의 X,Y,Z 세 좌표를 모두 이용하게 되는데, 이는 X,Y 두개만 이용하게 되면 Hash 값이 Screen-Space 에서 생성되어 다른 물체와 겹치게 되면 Alpha to Coverge 같은 효과를 내게되어 3가지 좌표 모두 Hash 생성에 사용된다고 한다. 마지막으로 중요한 포인트는 Temporal Stability 를 확보하는 것이다. 이해하기 쉽게 설명하자면, 아래와 같은 각 픽셀을 나타내는 그리드안에 점이 있다고 가정해보자. 이 점들이 조금씩 움직여서 계속 픽셀안에 있다면, 같은 Hash 값을 사용하여 같은 Alpha Threshold 값을 만들어줘야 한다. 아래 두 그림의 빨간 점의 위치처럼 원래의 픽셀위치를 벗어나게 된다면 새로운 Alpha Threshold 를 생성해야 하겠지만, 위치가 많이 바뀌지 않는다면 같은 Alpha Threshold 를 사용해 Flickering 을 최대한 줄여야 한다. 이러한 맥락으로 Hashed ALpha Testing 은 Temporal Stability 를 조금 확보하게 된다. 물론 위의 그림은 이해를 돕기위한 용도로, 실제 코드상에서는 다른 방법을 통해 계산된다. 아래 코드를 보자. 출처 : Cwyman.org : Hashed Alpha Test(Extended) 위 코드는 픽셀이 가지고 있는 Object-Space Coordinate 의 옆 픽셀과의 차이, 세로에 있는 픽셀과의 차이를 통한 값으로 계산한다. (dFdX, dFdY 의 자세한 내용은 찾아보거나 What is ddx and ddy 에서 볼 수 있다.) 픽셀별로 값의 차이, 즉 근접한 픽셀의 위치 차이값에 따른값(미분값)과 그 값을 이용해 Object-Space Coordinate 값에 곱한 값을 Key 로 두어서 Alpha Threshold 를 계산한다. 마지막에 Alpha Threshold 를 구하는 코드를 보면, Floor 하는, 올림을 해주어 discrete value 로 Key 값을 넣어준다. Floor 가 의미하는 것은, 선형적인 데이터가 아닌 뚝뚝 끊기는 데이터로 만들어 특정한 값을 넘어야 Key 값이 바뀌게 하여 Hash 를 유지해 Flickering 을 방지하는 것이다. 아래 그림은 floor(x) 의 그래프다. 즉 코드의 pixScale 이 크면 클수록 Hash 의 값은 픽셀의 변화에 따라서 빠르게 바뀌고, 작으면 작을수록(0에 가까워질수록) 픽셀의 변화에 따라서 Hash 값이 느리게 바뀔 것이다. 출처 : Wolframalpha 이러한 방법은 View-Space 를 기준으로 X,Y 좌표가 조금씩 바뀔때는 픽셀끼리의 차이를 계산하기 때문에 안정적이다. 하지만 Z(Depth) 값이 바뀔때는 많은 Flickering 을 일으킬 것이다. 이를 해결하기 위해 아래 코드를 보자. 출처 : Cwyman.org : Hashed Alpha Test(Extended) 위치의 픽셀별 차이 벡터의 크기를 discrete 시키는 방법도 좋은 아이디어중 하나다. 하지만 이는 빌보드처럼 큰 크기의 판이 다가오게 된다면 끝부분의 discontinuity 를 유발하게 된다. 출처 : Cwyman.org : Hashed Alpha Test(Extended) 그래서 위 코드와 같 discretize 시킨 올림처리한 값과, 내림처리한 값을 사용한 두 Hash 값 사이의 보간을 통해서 Alpha Threshold 를 구해준다. 하지만 이 코드는 아직 문제점이 존재한다. 만약 maxDeriv 의 값이 0 ~ 1 사이라면 내림값이 반드시 0이 되기 때문에 보간할 값 중 한개의 값이 고정되게 된다. 그래서 아래와 같은 코드를 사용한다. 출처 : Cwyman.org : Hashed Alpha Test(Extended) pixScale 을 그냥 계산하는 대신, discretize 된 두개의 스케일값을 2의 지수로 표현하여 값이 0으로 되는 것을 막는다. 이렇게 보간된 값을 사용하여 Alpha Threshold 를 정해주면 약간의 문제가 생긴다. 보간을 함으로써 균일하지 않게 랜덤값이 분포되었기 때문이다. 그래서 아래와 같은 식을 사용하여 다시 값을 분포시켜준다. 출처 : Cwyman.org : Hashed Alpha Test(Extended) 위의 식을 적용하면 모든 값들이 균일하게 분포되어 진정한 랜덤값의 Alpha Threshold 가 생성된다고 한다. 아래는 전체 코드다. 출처 : Cwyman.org : Hashed Alpha Test(Extended) 자세한 사항은 논문에서 확인할 수 있다([Cwyman17]). 결과는 아래 유튜브 영상에서 확인할 수 있다. 이를 통해 전보다 훨씬 나은 Alpha Test 품질을 얻을 수 있게 되었다. 하지만 Hashed Alpha Testing 의 결과는 Stochastic Test 처럼 픽셀이 흩뿌려진 느낌을 지울 수 없다. 어느정도의 랜덤값에서 생성이되니 이는 어쩔 수 없는 결과다. Alpha Distribution Alpha Test 의 구린 품질을 좀 더 개선할 수 있는 방법이 또 있다. 이번년도 I3D 에 제출된 Alpha Distribution 이라는 논문이 있는데, 이는 Hashed Alpha Testing 처럼 런타임에 계산을 하지않고 각 Mip-level 의 텍스쳐를 미리 처리해놓는 방법 중에 하나다. 미리 계산된 Texture 들을 사용하여 일반적인 Alpha Test 를 그대로 사용하기만 하면 된다. 아직 직접 사용한 예시는 없어 검증되지는 않았지만, 이 방법이 그대로 사용될 수 있다면 Alpha Test 부분에서는 거의 끝판왕이 될 것 같다. Alpha Distribution 일반적인 Alpha Test 를 기준으로 Alpha Threshold 가 고정되어 있다는 것을 가정한다. 그렇게 되면 Alpha Threshold 에 따라서 픽셀에 출력이 되냐, 안되냐로 따질 수가 있다.(Binary Visibility) Binary Visibility 를 각 Mip-level 에 맞춰서 고르게 분산(Distribution)시키는게 Alpha Distribution 의 목적이다. Alpha Distribution 은 두가지 분산방법을 사용한다. Error Diffusion 과 Alpha Pyramid 이라는 방법을 사용한다. 하나씩 알아보자. Error Diffusion 은 하나하나의 픽셀을 순회하면서, 각 픽셀의 Binary Visibility 에 해당하는 값(0 아니면 1)과 이미지가 가지고 있는 Alpha 값을 비교해 그 오차(Quantization Error)를 다른 픽셀에 나누어준다. Binary Visibility 는 다음과 같이 정해진다. αˆi = αi &amp;gt;= ατ : 1, αi &amp;lt; ατ : 0 αi 는 이미지가 가지고 있는 이산화된 Alpha 값이고, ατ 는 Alpha Threshold, 한계값을 뜻한다. αˆi 는 해당 픽셀의 Binary Visibility 를 뜻한다. 이것을 가지고 Quantization Error 를 계산한다. ϵi = αi − αˆi ϵi 는 Quantization Error 를 뜻하고 픽셀이 보이게 된다면 ~1 &amp;lt;= ϵi &amp;lt; 0 의 값을 가지게 되고 픽셀이 보이지 않는다면 0 &amp;lt; ϵi &amp;lt;= 1 의 값을 가지게 된다. 이런 Quantization Error 는 인근 픽셀로 분포된다. 아래 그림을 보자. 그림에서 ϵi 가 들어가 있는 부분이 현재 처리중인 픽셀이며, ϵi 의 값은 인근 픽셀로 고정된 비율로 Alpha 값에 더해진다. (x+1,y) 는 7/16, (x-1,y+1) 은 3/16, (x,y+1) 은 5/16, (x+1,y+1) 은 1/16 비율로 분포된다. 이런 방법으로 각 픽셀을 순회하면서 처리하면 Error Diffusion 은 간단하게 끝난다. 오차 확산이라는 이름이 굉장히 직관적이다. Error Diffusion 은 픽셀과 픽셀사이의 Alpha 값을 고르게 분포시킨다. 하지만 약간의 문제가 존재한다. 보이게 되던, 안보이게 되던 Alpha 값이 0.3 ~ 0.7 정도로 중간값을 가지고 있다면, 한 픽셀은 강조되고, 옆의 픽셀은 보이지 않게 된다. 이러한 방법은 아래 이미지와 비슷한 결과를 만든다. 출처 : Wikipedia : Dither Error Diffusion 의 문제는 위 그림처럼 비슷한 색 영역에 있어도 분산된 영향을 받아서 각 픽셀이 부드럽게 보이지 않는 현상이 발생한다. 이러한 특징을 Dithering 이라고 부른다. 그래서 이보다 나은 품질을 위해 Alpha Pyramid 라는 다른 방법이 소개된다. 참조 Anti-aliased Alpha Test: The Esoteric Alpha To Coverage NVidia developer : Hashed Alpha Testing Cwyman.org : Hashed Alpha Test(Extended) Cemyuksel : Alpha Distribution</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Ieee 754 Floaing Number</title>
      
      <link href="https://hrmrzizon.github.io/2018/05/18/IEEE-754-floaing-number/" rel="alternate" type="text/html" title="Ieee 754 Floaing Number" />
      <published>2018-05-18T00:00:00+00:00</published>
      <updated>2018-05-18T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2018/05/18/IEEE-754-floaing-number</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2018/05/18/IEEE-754-floaing-number/">&lt;p&gt;코딩을 하던 도중, 비트로 나타내어진 2 byte floating number(half-precision) 데이터를 일반적인 4byte floating number(single-precision) 으로 나타내야 할 일이 있었다. 그래서 귀찮아서 알아보지 않았던 컴퓨터의 소수를 표현하는 방법에 대해서 알아보았다. 이 글에서는 간략하게 어떤식으로 표현되는지에 대해서만 적어보기로 하겠다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;134.75
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이와 같은 소수가 있다. 이는 10진법으로 나타낸 소숫점으로, 2진법으로 나타내면 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;134.75(demical) = 10000110.11(binary)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이를 소수부와 정수부를 나누면 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;134(demical) = 10000110(binary)
0.75(demical) = 0.11(binary)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;정수부는 오른쪽부터 2^0 인 1부터 나란히 2^¹, 2^², 2^³, 2^⁴, … 2^ⁿ 로 구성되고(n은 자리의 끝), 소수부는 점 이하인 숫자부터 2^-1, 2^-2, … 2^-n 으로 구성된다. 0.75 는 2^-1 * 1 + 2^-2 * 2 가 되니 위의 경우처럼 굉장히 쉽게 표현이 가능하다. 하지만 다음과 같은 숫자는 어떻게 표현할까?&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;0.9999... = 0.1111...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이런식으로 표기는 가능할 것이다. 하지만 컴퓨터는 유한한 데이터만을 다루기 때문에 한계가 있다.&lt;/p&gt;

&lt;h2&gt;IEEE 754&lt;/h2&gt;

&lt;p&gt;컴퓨터에서는 숫자를 다루기 위해 여러가지 기준이 정해져 있다. 그 중에서도 소수를 나타내기 위한 기준은 IEEE 754 로 알려져 있다. 대부분의 언어에서 사용하는 &lt;em&gt;float&lt;/em&gt; 은 IEEE 754 single-precision 을 사용하여 계산된다.(&lt;em&gt;double&lt;/em&gt; 또한 마찬가지.) 데이터를 어떻게 저장하는지, 그 데이터의 표현방식은 어떻게 되는지에 대하여 간략하게 알아보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/General_floating_point_ko.png&quot; alt=&quot;위키백과 : IEEE 754&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://ko.wikipedia.org/wiki/IEEE_754&quot;&gt;위키백과 : IEEE 754
&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;데이터의 저장 방식은 다음과 같다. 일반적으로 들어가는 부호를 위한 1bit, 그리고 우리가 아직 살펴보지 않은 지수(&lt;em&gt;exponent&lt;/em&gt;), 가수(&lt;em&gt;fraction&lt;/em&gt;) 부분으로 나뉘어져 있다. single-precision 을 예시로 보며 설명해보겠다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/single-precision_example.png&quot; alt=&quot;Wikipedia : Single-precision floating-point format&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://en.wikipedia.org/wiki/Single-precision_floating-point_format&quot;&gt;Wikipedia : Single-precision floating-point format&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;각각의 부분이 어떠한 숫자를 저장하는지만 알면 된다. &lt;em&gt;fraction&lt;/em&gt; 은 말 그대로 소숫점 아래의 숫자만 나타내는 부분이다. 가장 왼쪽의 비트(22번쨰 비트)는 2^-1 을 저장하고 오른쪽으로 2^-2, 2^-3 이런식의 숫자에 대한 정보를 기록한다. 이렇게 실질적인 소수부와 나머지인 &lt;em&gt;exponent&lt;/em&gt; 부분이 남아 있다. &lt;em&gt;exponent&lt;/em&gt; 는 &lt;em&gt;fraction&lt;/em&gt; 숫자를 얼마나 곱하는지 나타내는 숫자다. 아래의 그림을 보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/single-precision_formatted.svg&quot; alt=&quot;Wikipedia : Single-precision floating-point format&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://en.wikipedia.org/wiki/Single-precision_floating-point_format&quot;&gt;Wikipedia : Single-precision floating-point format&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;여기서 이해가 안되는 부분이 많을 것이다. 첫번째로 맨 오른쪽의 식은 2의 지수가 음수가 되는 부분의 데이터를 나타내는 식인데, 전부다 더한 이후에 1을 더한다. 이는 숫자의 표현을 위해 넣은 부분이다. 그 다음은 중간 2의 지수가 들어가는 식에서 2^(e-127) 인데, 이는 지수를 양수, 음수로 표현하기 위한 수단이다. 양수가 된다면 가수부가 나타내는 숫자보다 큰 숫자를 나타낼 것이며, 음수가 된다면 가수부가 나타내던 숫자보다 더 작은 수를 표현할 것이다. 즉 표현하는 숫자의 범위는 굉장히 큰 것을 알 수 있다. 또한 정밀도는 보장하지 못한다는 것을 알 수 있다.&lt;/p&gt;

&lt;h2&gt;참조&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://ko.wikipedia.org/wiki/IEEE_754&quot;&gt;위키피디아(한글) : IEEE 754&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Single-precision_floating-point_format&quot;&gt;Wikipedia : Single-precision floating-point format&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Half-precision_floating-point_format&quot;&gt;Wikipedia : Half-precision floating-point format&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="math" />
      
        <category term="floating_point" />
      

      

      
        <summary type="html">코딩을 하던 도중, 비트로 나타내어진 2 byte floating number(half-precision) 데이터를 일반적인 4byte floating number(single-precision) 으로 나타내야 할 일이 있었다. 그래서 귀찮아서 알아보지 않았던 컴퓨터의 소수를 표현하는 방법에 대해서 알아보았다. 이 글에서는 간략하게 어떤식으로 표현되는지에 대해서만 적어보기로 하겠다. 134.75 이와 같은 소수가 있다. 이는 10진법으로 나타낸 소숫점으로, 2진법으로 나타내면 다음과 같다. 134.75(demical) = 10000110.11(binary) 이를 소수부와 정수부를 나누면 다음과 같다. 134(demical) = 10000110(binary) 0.75(demical) = 0.11(binary) 정수부는 오른쪽부터 2^0 인 1부터 나란히 2^¹, 2^², 2^³, 2^⁴, … 2^ⁿ 로 구성되고(n은 자리의 끝), 소수부는 점 이하인 숫자부터 2^-1, 2^-2, … 2^-n 으로 구성된다. 0.75 는 2^-1 * 1 + 2^-2 * 2 가 되니 위의 경우처럼 굉장히 쉽게 표현이 가능하다. 하지만 다음과 같은 숫자는 어떻게 표현할까? 0.9999... = 0.1111... 이런식으로 표기는 가능할 것이다. 하지만 컴퓨터는 유한한 데이터만을 다루기 때문에 한계가 있다. IEEE 754 컴퓨터에서는 숫자를 다루기 위해 여러가지 기준이 정해져 있다. 그 중에서도 소수를 나타내기 위한 기준은 IEEE 754 로 알려져 있다. 대부분의 언어에서 사용하는 float 은 IEEE 754 single-precision 을 사용하여 계산된다.(double 또한 마찬가지.) 데이터를 어떻게 저장하는지, 그 데이터의 표현방식은 어떻게 되는지에 대하여 간략하게 알아보자. 출처 : 위키백과 : IEEE 754 데이터의 저장 방식은 다음과 같다. 일반적으로 들어가는 부호를 위한 1bit, 그리고 우리가 아직 살펴보지 않은 지수(exponent), 가수(fraction) 부분으로 나뉘어져 있다. single-precision 을 예시로 보며 설명해보겠다. 출처 : Wikipedia : Single-precision floating-point format 각각의 부분이 어떠한 숫자를 저장하는지만 알면 된다. fraction 은 말 그대로 소숫점 아래의 숫자만 나타내는 부분이다. 가장 왼쪽의 비트(22번쨰 비트)는 2^-1 을 저장하고 오른쪽으로 2^-2, 2^-3 이런식의 숫자에 대한 정보를 기록한다. 이렇게 실질적인 소수부와 나머지인 exponent 부분이 남아 있다. exponent 는 fraction 숫자를 얼마나 곱하는지 나타내는 숫자다. 아래의 그림을 보자. 출처 : Wikipedia : Single-precision floating-point format 여기서 이해가 안되는 부분이 많을 것이다. 첫번째로 맨 오른쪽의 식은 2의 지수가 음수가 되는 부분의 데이터를 나타내는 식인데, 전부다 더한 이후에 1을 더한다. 이는 숫자의 표현을 위해 넣은 부분이다. 그 다음은 중간 2의 지수가 들어가는 식에서 2^(e-127) 인데, 이는 지수를 양수, 음수로 표현하기 위한 수단이다. 양수가 된다면 가수부가 나타내는 숫자보다 큰 숫자를 나타낼 것이며, 음수가 된다면 가수부가 나타내던 숫자보다 더 작은 수를 표현할 것이다. 즉 표현하는 숫자의 범위는 굉장히 큰 것을 알 수 있다. 또한 정밀도는 보장하지 못한다는 것을 알 수 있다. 참조 위키피디아(한글) : IEEE 754 Wikipedia : Single-precision floating-point format Wikipedia : Half-precision floating-point format</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Anisotropic Filtering</title>
      
      <link href="https://hrmrzizon.github.io/2018/05/13/anisotropic-filtering/" rel="alternate" type="text/html" title="Anisotropic Filtering" />
      <published>2018-05-13T00:00:00+00:00</published>
      <updated>2018-05-13T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2018/05/13/anisotropic-filtering</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2018/05/13/anisotropic-filtering/">&lt;p&gt;보통 사용되는 &lt;em&gt;Texture Fltering&lt;/em&gt; 들은 &lt;em&gt;Axis Align&lt;/em&gt; 된 방향을 기준으로 추가적인 샘플링을 하는 방법들이 대부분이다.(bilinear, bicubic, etc..) 하지만 특이한 것이 하나 있다. 바로 &lt;em&gt;Anisotropic Filtering&lt;/em&gt; 이다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Anisotropic Filtering&lt;/em&gt; 은 원거리에 있는 물체들을 선명하게 보이게 하기위해서 쓰여지는 &lt;em&gt;Fiterling&lt;/em&gt; 으로, 말보다는 아래 그림을 보는게 훨씬 직관적으로 이해할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/aniso_pixel_to_texel.png&quot; alt=&quot;Real-time Rendering 3rd&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : Real-time Rendering 3rd&amp;lt;/a&amp;gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위의 그림과 같이 &lt;em&gt;Texture-Space&lt;/em&gt; 에서 픽셀안에 있는 텍스쳐를 여러번 샘플링하여 평균을 구하는 방식인듯하다. 그런데 아주 중요한 것이 하나 남아있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/img034.gif&quot; alt=&quot;Unsolved Problems and Opportunities for High-quality, High-perfornmance 3D Graphics on a PC Platform : Anisotropic Filtering&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://www.graphicshardware.org/previous/www_1998/presentations/kirk/sld030.htm&quot;&gt;Unsolved Problems and Opportunities for High-quality, High-perfornmance 3D Graphics on a PC Platform : Anisotropic Filtering&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위의 그림을 보면 알겠지만 &lt;em&gt;bilinear filtering&lt;/em&gt; 과 함께 쓸 경우 엄청난 샘플링 부하가 생길 것이라는 것을 예상할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Anisotropic Filtering&lt;/em&gt; 을 처음 접했을 떄, 가장 이해가 가지 않았던 것은 결국 내부에서 샘플링을 해야할 텐데 어떤 방식으로 방향을 구할지 가장 이해가 안됬었다. 지금 다시 생각해보면, &lt;em&gt;ddx&lt;/em&gt; 키워드를 &lt;em&gt;uv&lt;/em&gt; 좌표에 쓰듯이 &lt;em&gt;Texutre-Space&lt;/em&gt; 의 차이 벡터를 쉽게 구할 수 있을 듯 하다.&lt;/p&gt;

&lt;h2&gt;참조&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Real-Time Rendering 3rd&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.graphicshardware.org/previous/www_1998/presentations/kirk/sld030.htm&quot;&gt;Unsolved Problems and Opportunities for High-quality, High-perfornmance 3D Graphics on a PC Platform : Anisotropic Filtering&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="anisotropic" />
      
        <category term="filtering" />
      

      

      
        <summary type="html">보통 사용되는 Texture Fltering 들은 Axis Align 된 방향을 기준으로 추가적인 샘플링을 하는 방법들이 대부분이다.(bilinear, bicubic, etc..) 하지만 특이한 것이 하나 있다. 바로 Anisotropic Filtering 이다. Anisotropic Filtering 은 원거리에 있는 물체들을 선명하게 보이게 하기위해서 쓰여지는 Fiterling 으로, 말보다는 아래 그림을 보는게 훨씬 직관적으로 이해할 수 있다. 출처 : Real-time Rendering 3rd&amp;lt;/a&amp;gt; 위의 그림과 같이 Texture-Space 에서 픽셀안에 있는 텍스쳐를 여러번 샘플링하여 평균을 구하는 방식인듯하다. 그런데 아주 중요한 것이 하나 남아있다. 출처 : Unsolved Problems and Opportunities for High-quality, High-perfornmance 3D Graphics on a PC Platform : Anisotropic Filtering 위의 그림을 보면 알겠지만 bilinear filtering 과 함께 쓸 경우 엄청난 샘플링 부하가 생길 것이라는 것을 예상할 수 있다. Anisotropic Filtering 을 처음 접했을 떄, 가장 이해가 가지 않았던 것은 결국 내부에서 샘플링을 해야할 텐데 어떤 방식으로 방향을 구할지 가장 이해가 안됬었다. 지금 다시 생각해보면, ddx 키워드를 uv 좌표에 쓰듯이 Texutre-Space 의 차이 벡터를 쉽게 구할 수 있을 듯 하다. 참조 Real-Time Rendering 3rd Unsolved Problems and Opportunities for High-quality, High-perfornmance 3D Graphics on a PC Platform : Anisotropic Filtering</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Recommandation Of Gpu Skinning In Github</title>
      
      <link href="https://hrmrzizon.github.io/2018/03/13/recommandation-of-gpu-skinning-in-github/" rel="alternate" type="text/html" title="Recommandation Of Gpu Skinning In Github" />
      <published>2018-03-13T00:00:00+00:00</published>
      <updated>2018-03-13T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2018/03/13/recommandation-of-gpu-skinning-in-github</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2018/03/13/recommandation-of-gpu-skinning-in-github/">&lt;p&gt;최근 급하게 어떤 프로젝트에 투입되서 작업을 하고 있다. 다른 회사와 같이 일을 하고 있는데, 다른 회사에서 해놓은 것들이 너무 느려서 최적화를 해야했다. 결국 일반적인 잔머리로는 도저히 해결할 수 없는 상황에 봉착했다. 1년 전의 필자였다면 포기하고 안된다고 했었겠지만 다행히 약간의 노하우를 통해 해결할 수 있었다.&lt;/p&gt;

&lt;h2&gt;상황과 문제?&lt;/h2&gt;

&lt;p&gt;요즘 한국에서는 VR 게임/컨텐츠들이 굉장히 많이 개발되고 있다. 필자도 그 와중에 떨어진 프로젝트를 하나 받아진행하게 되었다. 개발은 주변 3D 환경을 다른 회사에서 해주고, 게임이 돌아가는 코어 시스템을 필자의 회사에서 작업하기로 되어 있었다. 게임에서의 코어 시스템은 개발을 하였으나 &lt;em&gt;Unity&lt;/em&gt; 에 의존적인 코딩은 거의 진행되지 않은 상태에서 한달전 프로젝트에 투입되었다.&lt;/p&gt;

&lt;p&gt;그렇게 하나하나 작업을 하면서 코어 시스템과 3D 환경을 결합하는 도중, 터무니 없는 경우를 만났다. 바로 &lt;strong&gt;SkinnedMeshRenderer&lt;/strong&gt; 와 &lt;em&gt;Animator&lt;/em&gt; 가 약 800 개 정도되는 상황에 부딫쳤다. 여기서는 두가지의 큰 부하가 있었다. 절대적인 &lt;em&gt;Vertex Skinning&lt;/em&gt; 부하와 &lt;em&gt;Animator&lt;/em&gt; 가 800개가 한꺼번에 계산되는 부하였다. &lt;em&gt;Vertex&lt;/em&gt; 숫자는 &lt;em&gt;LOD&lt;/em&gt; 베이커를 구해서 어떻게든 해결이 되었으나, 800개의 &lt;em&gt;Animator&lt;/em&gt; 부하는 우회방법이 없었다. 즉 이는 &lt;em&gt;Vertex Shader&lt;/em&gt; 나 &lt;em&gt;Compute Shader&lt;/em&gt; 안에서 &lt;em&gt;AnimationClip&lt;/em&gt; 의 정보들을 처리하는 정공법이 필요했다.&lt;/p&gt;

&lt;p&gt;하지만 기한도 얼마 남지않아 급한 와중에 저것들을 직접 코딩할 여유는 없었다. 게다가 처음 건드려보는 부분이라서 헤멜 코스트까지 합하면 굉장히 암울했다. 혹시 오픈소스가 있나 싶어서 생각을 해보았는데 예전에 &lt;em&gt;SkinRenderer&lt;/em&gt; 를 직접 구현하면서 찾아본 오픈소스 레포지토리가 하나 있었다.&lt;/p&gt;

&lt;h2&gt;Github : &lt;a href=&quot;https://github.com/chengkehan/GPUSkinning&quot;&gt;GPUSkinning&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;한 중국인이 개발한 스키닝 툴이다. 이는 두가지의 큰 기능을 담고있다. 하나는 &lt;em&gt;Vertex Shader&lt;/em&gt; 에서 &lt;em&gt;Skinning&lt;/em&gt; 처리를 해주는 기능과, 하나는 &lt;em&gt;AnimationClip&lt;/em&gt; 들을 직접 샘플링해 바이너리 파일로 저장해 GPU 메모리에 텍스쳐의 형태로 올려두어 사용하는 기능이 있다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Compute Shader&lt;/em&gt; 나 &lt;em&gt;Vetex Shader&lt;/em&gt; 를 사용해 &lt;em&gt;Skinning&lt;/em&gt; 을 구현하는 것은 크게 어려운 것은 아니다. &lt;em&gt;Deformation&lt;/em&gt;  하지만 제일 시간이 오래걸리는 부분은 &lt;em&gt;AnimationClip&lt;/em&gt; 을 가공하는 부분이 제일 오래 걸리는 부분 중 하나다. 해당 레포지토리에는 그 오래걸리는 부분을 만들어 놓았다. 사실 이 부분으로만으로 꽤 큰 가치가 있다. 부가적으로 원하는 &lt;em&gt;bone&lt;/em&gt; 의 위치와 회전값을 &lt;strong&gt;Transform&lt;/strong&gt; 으로 만들어 &lt;em&gt;Hierarchy&lt;/em&gt; 상에서 컨트롤이 가능하다.&lt;/p&gt;

&lt;p&gt;조금 불편한 점도 몇가지 있다. 한번에 여러개의 샘플링이 안되고, 에디터에서 필수적으로 지원해야할 멀티 에디팅이 안된다. 후자는 간단히 코드를 수정하면 되지만 전자는 샘플링 과정에서 플레이를 해야되기 때문에 직접 수정하기엔 조금 부담스럽다. &lt;em&gt;Vertex Shader&lt;/em&gt; 에 스키닝이 물려있기 때문에 &lt;em&gt;Skinning&lt;/em&gt; 을 수정하려면 &lt;em&gt;Shader&lt;/em&gt; 부분도 바꿔야하고, &lt;em&gt;LOD&lt;/em&gt; 기능도 어설프게 들어있어 조금 애매하다.&lt;/p&gt;

&lt;p&gt;더 기능을 생각하자면 IK 나 특정 본을 타겟으로 회전을 시키는 기능이 없다. 이는 &lt;em&gt;mecanim&lt;/em&gt; 에서 &lt;em&gt;Humanoid&lt;/em&gt; 를 타겟으로 지원하는 기능으로 이 기능까지 만들어 놓았으면 엄청 유용 했을것 같다. 다만 구현이 &lt;em&gt;Vertex Shader&lt;/em&gt; 에서 되어서 다른 기능을 끼워넣기는 조금 부담스러운 것으로 생각된다.&lt;/p&gt;

&lt;p&gt;굳이 단점을 생각하지 않아도 샘플링 코드와 구현만으로도 충분히 가치있는 레포지토리로 생각된다. &lt;em&gt;ComputeShader&lt;/em&gt; 를 사용해서 구현했으면 더 좋았을 것 같다는 생각이 문득든다.&lt;/p&gt;

&lt;p&gt;써보면서 이게 본격적으로 쓰려고 만들어진 코드는 아닌 것 같다는 생각이 들었다. 기본적인 캐싱도 안되있어서 약간의 삽질을 했었다.&lt;/p&gt;

&lt;p&gt;가장 큰 문제는 &lt;em&gt;AnimationClip&lt;/em&gt; 이 많으면 많을수록 에디터, 런타임 로드시에 엄청나게 로딩이 걸린다. 아마 ScriptableObject 에 Serialization 으로 저장한 정보들이 많아서 그런듯 하다. 이는 따로 텍스쳐든 뭐든 Unity 에서 직접 관리하는 리소스로 바꾸어야 겠다.&lt;/p&gt;

&lt;h2&gt;참조&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/chengkehan/GPUSkinning&quot;&gt;Github : GPUSkining&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="unity" />
      
        <category term="gpu-skinning" />
      

      

      
        <summary type="html">최근 급하게 어떤 프로젝트에 투입되서 작업을 하고 있다. 다른 회사와 같이 일을 하고 있는데, 다른 회사에서 해놓은 것들이 너무 느려서 최적화를 해야했다. 결국 일반적인 잔머리로는 도저히 해결할 수 없는 상황에 봉착했다. 1년 전의 필자였다면 포기하고 안된다고 했었겠지만 다행히 약간의 노하우를 통해 해결할 수 있었다. 상황과 문제? 요즘 한국에서는 VR 게임/컨텐츠들이 굉장히 많이 개발되고 있다. 필자도 그 와중에 떨어진 프로젝트를 하나 받아진행하게 되었다. 개발은 주변 3D 환경을 다른 회사에서 해주고, 게임이 돌아가는 코어 시스템을 필자의 회사에서 작업하기로 되어 있었다. 게임에서의 코어 시스템은 개발을 하였으나 Unity 에 의존적인 코딩은 거의 진행되지 않은 상태에서 한달전 프로젝트에 투입되었다. 그렇게 하나하나 작업을 하면서 코어 시스템과 3D 환경을 결합하는 도중, 터무니 없는 경우를 만났다. 바로 SkinnedMeshRenderer 와 Animator 가 약 800 개 정도되는 상황에 부딫쳤다. 여기서는 두가지의 큰 부하가 있었다. 절대적인 Vertex Skinning 부하와 Animator 가 800개가 한꺼번에 계산되는 부하였다. Vertex 숫자는 LOD 베이커를 구해서 어떻게든 해결이 되었으나, 800개의 Animator 부하는 우회방법이 없었다. 즉 이는 Vertex Shader 나 Compute Shader 안에서 AnimationClip 의 정보들을 처리하는 정공법이 필요했다. 하지만 기한도 얼마 남지않아 급한 와중에 저것들을 직접 코딩할 여유는 없었다. 게다가 처음 건드려보는 부분이라서 헤멜 코스트까지 합하면 굉장히 암울했다. 혹시 오픈소스가 있나 싶어서 생각을 해보았는데 예전에 SkinRenderer 를 직접 구현하면서 찾아본 오픈소스 레포지토리가 하나 있었다. Github : GPUSkinning 한 중국인이 개발한 스키닝 툴이다. 이는 두가지의 큰 기능을 담고있다. 하나는 Vertex Shader 에서 Skinning 처리를 해주는 기능과, 하나는 AnimationClip 들을 직접 샘플링해 바이너리 파일로 저장해 GPU 메모리에 텍스쳐의 형태로 올려두어 사용하는 기능이 있다. Compute Shader 나 Vetex Shader 를 사용해 Skinning 을 구현하는 것은 크게 어려운 것은 아니다. Deformation 하지만 제일 시간이 오래걸리는 부분은 AnimationClip 을 가공하는 부분이 제일 오래 걸리는 부분 중 하나다. 해당 레포지토리에는 그 오래걸리는 부분을 만들어 놓았다. 사실 이 부분으로만으로 꽤 큰 가치가 있다. 부가적으로 원하는 bone 의 위치와 회전값을 Transform 으로 만들어 Hierarchy 상에서 컨트롤이 가능하다. 조금 불편한 점도 몇가지 있다. 한번에 여러개의 샘플링이 안되고, 에디터에서 필수적으로 지원해야할 멀티 에디팅이 안된다. 후자는 간단히 코드를 수정하면 되지만 전자는 샘플링 과정에서 플레이를 해야되기 때문에 직접 수정하기엔 조금 부담스럽다. Vertex Shader 에 스키닝이 물려있기 때문에 Skinning 을 수정하려면 Shader 부분도 바꿔야하고, LOD 기능도 어설프게 들어있어 조금 애매하다. 더 기능을 생각하자면 IK 나 특정 본을 타겟으로 회전을 시키는 기능이 없다. 이는 mecanim 에서 Humanoid 를 타겟으로 지원하는 기능으로 이 기능까지 만들어 놓았으면 엄청 유용 했을것 같다. 다만 구현이 Vertex Shader 에서 되어서 다른 기능을 끼워넣기는 조금 부담스러운 것으로 생각된다. 굳이 단점을 생각하지 않아도 샘플링 코드와 구현만으로도 충분히 가치있는 레포지토리로 생각된다. ComputeShader 를 사용해서 구현했으면 더 좋았을 것 같다는 생각이 문득든다. 써보면서 이게 본격적으로 쓰려고 만들어진 코드는 아닌 것 같다는 생각이 들었다. 기본적인 캐싱도 안되있어서 약간의 삽질을 했었다. 가장 큰 문제는 AnimationClip 이 많으면 많을수록 에디터, 런타임 로드시에 엄청나게 로딩이 걸린다. 아마 ScriptableObject 에 Serialization 으로 저장한 정보들이 많아서 그런듯 하다. 이는 따로 텍스쳐든 뭐든 Unity 에서 직접 관리하는 리소스로 바꾸어야 겠다. 참조 Github : GPUSkining</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Dualized Asset Management Of Unity</title>
      
      <link href="https://hrmrzizon.github.io/2018/03/06/dualized-asset-management-of-unity/" rel="alternate" type="text/html" title="Dualized Asset Management Of Unity" />
      <published>2018-03-06T00:00:00+00:00</published>
      <updated>2018-03-06T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2018/03/06/dualized-asset-management-of-unity</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2018/03/06/dualized-asset-management-of-unity/">&lt;p&gt;&lt;em&gt;Unity&lt;/em&gt; 에서는 모든 사용자의 작업물을 &lt;em&gt;Assets&lt;/em&gt; 폴더에 저장한다. 그리고 &lt;em&gt;Assets&lt;/em&gt; 폴더안의 파일의 변경이 발생할 시 안의 파일들을 재가공하여 다시 로드한다. 보통 파일의 변경은 &lt;em&gt;assetDatabaseX&lt;/em&gt; 바이너리 파일로 들어가게 되며, 스크립트, 바이너리의 변경은 다시 컴파일을 함으로써 현재 변경사항을 프로젝트에 적용시킨다.&lt;/p&gt;

&lt;p&gt;이러한 시스템을 위해 &lt;em&gt;Unity&lt;/em&gt; 에서는 모든 파일, 디렉토리에 &lt;em&gt;meta&lt;/em&gt; 파일을 생성한다. 파일별 &lt;em&gt;meta&lt;/em&gt; 파일에는 해당 파일의 순수한 정보가 아닌 메타 정보가 들어간다. 중요한 정보는 두개로 나뉜다.&lt;/p&gt;

&lt;p&gt;하나는 &lt;em&gt;Unity&lt;/em&gt; 프로젝트상에서 파일을 처음 감지했을 때, 파일의 &lt;em&gt;GUID&lt;/em&gt; 를 생성한다. &lt;em&gt;GUID&lt;/em&gt; 란 고유의 16진수 32글자로 이루어지는 총 512비트로 이루어지는 &lt;em&gt;ID&lt;/em&gt; 로써 자동으로 생성되는 알고리즘을 가지고 있으며 겹칠 염려는 거의 없는 &lt;em&gt;ID&lt;/em&gt; 알고리즘이다. 그래서 생성된 &lt;em&gt;GUID&lt;/em&gt; 는 다른 곳에서 해당 파일을 참조할떄 쓰인다. 즉 파일이 삭제되서 같은 것으로 다시 생성한다고 해도 &lt;em&gt;GUID&lt;/em&gt; 가 랜덤으로 결정되기 때문에 다시 연결을 해주어야 한다. 이는 &lt;em&gt;Unity&lt;/em&gt; 내부에서 파일 링크를 &lt;em&gt;GUID&lt;/em&gt; 로 한다는 추측을 할 수 있게 해준다. 또한 &lt;em&gt;Edit -&amp;gt; Project Setting -&amp;gt; Editor&lt;/em&gt; 에서 &lt;em&gt;Asset Serialization&lt;/em&gt; 모드가 &lt;em&gt;Force Text&lt;/em&gt; 로 되어있을 시에는 &lt;em&gt;meta&lt;/em&gt; 파일들을 직접 텍스트 에디터로 확인이 가능하다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fileFormatVersion: 2
guid: 5d44a238286f6904198ab78e914c229d
MonoImporter:
  serializedVersion: 2
  defaultReferences: []
  executionOrder: 0
  icon: {instanceID: 0}
  userData:
  assetBundleName:
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;어떤 스크립트에 딸린 &lt;em&gt;meta&lt;/em&gt; 파일의 내용이다. 두번째 줄에 생성된 &lt;em&gt;guid&lt;/em&gt; 가 존재한다. 이는 &lt;em&gt;Library/metadata&lt;/em&gt; 디렉토리에 쓰여진 이름들과 매칭된다.&lt;/p&gt;

&lt;p&gt;두번째는 바로 해당 파일의 &lt;em&gt;Importer&lt;/em&gt; 정보가 들어있다. 위의 &lt;em&gt;meta&lt;/em&gt; 파일은 스크립트이기 때문에 3번째 줄에 &lt;em&gt;MonoImporter&lt;/em&gt; 라고 쓰여져 있으며, 파일의 성질에 따라서 &lt;em&gt;built-in importer&lt;/em&gt; 가 달라진다. 바이너리 파일들은 &lt;em&gt;NativeImporter&lt;/em&gt;, 텍스쳐 파일들은 &lt;em&gt;TextureImporter&lt;/em&gt;, 3D 모델 파일들은 &lt;em&gt;ModelImporter&lt;/em&gt; 로 자동으로 매칭된다.&lt;/p&gt;

&lt;p&gt;이러한 &lt;em&gt;Importer&lt;/em&gt; 정보들은 보통 해당 &lt;em&gt;Asset&lt;/em&gt; 의 옵션을 세팅할 떄 쓰인다. 또한 &lt;em&gt;2017&lt;/em&gt; 버젼에서는 파일의 확장자를 사용자가 직접 지정해 &lt;em&gt;Importer&lt;/em&gt; 를 사용할 수도 있게 해두었다.(&lt;a href=&quot;/2018/01/11/unity-scripted-importer/&quot;&gt;링크&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;즉 &lt;em&gt;Unity&lt;/em&gt; 에서는 새로운 파일을 감지했을 때, &lt;em&gt;GUID&lt;/em&gt; 를 생성하고 파일의 확장자에 따라 &lt;em&gt;Importer&lt;/em&gt; 정보를 갱신한 후, 정보를 &lt;em&gt;Library/metadata&lt;/em&gt; 에 갱신하는 것으로 볼 수 있다. &lt;em&gt;Library/metadata&lt;/em&gt; 에서는 &lt;em&gt;GUID&lt;/em&gt; 로 된 파일과 (해당 &lt;em&gt;GUID&lt;/em&gt;).info 로 파일이 구성되어 있다. 각각의 파일은 파일의 유형별로 다른 것으로 보인다.&lt;/p&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="unity" />
      

      

      
        <summary type="html">Unity 에서는 모든 사용자의 작업물을 Assets 폴더에 저장한다. 그리고 Assets 폴더안의 파일의 변경이 발생할 시 안의 파일들을 재가공하여 다시 로드한다. 보통 파일의 변경은 assetDatabaseX 바이너리 파일로 들어가게 되며, 스크립트, 바이너리의 변경은 다시 컴파일을 함으로써 현재 변경사항을 프로젝트에 적용시킨다. 이러한 시스템을 위해 Unity 에서는 모든 파일, 디렉토리에 meta 파일을 생성한다. 파일별 meta 파일에는 해당 파일의 순수한 정보가 아닌 메타 정보가 들어간다. 중요한 정보는 두개로 나뉜다. 하나는 Unity 프로젝트상에서 파일을 처음 감지했을 때, 파일의 GUID 를 생성한다. GUID 란 고유의 16진수 32글자로 이루어지는 총 512비트로 이루어지는 ID 로써 자동으로 생성되는 알고리즘을 가지고 있으며 겹칠 염려는 거의 없는 ID 알고리즘이다. 그래서 생성된 GUID 는 다른 곳에서 해당 파일을 참조할떄 쓰인다. 즉 파일이 삭제되서 같은 것으로 다시 생성한다고 해도 GUID 가 랜덤으로 결정되기 때문에 다시 연결을 해주어야 한다. 이는 Unity 내부에서 파일 링크를 GUID 로 한다는 추측을 할 수 있게 해준다. 또한 Edit -&amp;gt; Project Setting -&amp;gt; Editor 에서 Asset Serialization 모드가 Force Text 로 되어있을 시에는 meta 파일들을 직접 텍스트 에디터로 확인이 가능하다. fileFormatVersion: 2 guid: 5d44a238286f6904198ab78e914c229d MonoImporter: serializedVersion: 2 defaultReferences: [] executionOrder: 0 icon: {instanceID: 0} userData: assetBundleName: 어떤 스크립트에 딸린 meta 파일의 내용이다. 두번째 줄에 생성된 guid 가 존재한다. 이는 Library/metadata 디렉토리에 쓰여진 이름들과 매칭된다. 두번째는 바로 해당 파일의 Importer 정보가 들어있다. 위의 meta 파일은 스크립트이기 때문에 3번째 줄에 MonoImporter 라고 쓰여져 있으며, 파일의 성질에 따라서 built-in importer 가 달라진다. 바이너리 파일들은 NativeImporter, 텍스쳐 파일들은 TextureImporter, 3D 모델 파일들은 ModelImporter 로 자동으로 매칭된다. 이러한 Importer 정보들은 보통 해당 Asset 의 옵션을 세팅할 떄 쓰인다. 또한 2017 버젼에서는 파일의 확장자를 사용자가 직접 지정해 Importer 를 사용할 수도 있게 해두었다.(링크) 즉 Unity 에서는 새로운 파일을 감지했을 때, GUID 를 생성하고 파일의 확장자에 따라 Importer 정보를 갱신한 후, 정보를 Library/metadata 에 갱신하는 것으로 볼 수 있다. Library/metadata 에서는 GUID 로 된 파일과 (해당 GUID).info 로 파일이 구성되어 있다. 각각의 파일은 파일의 유형별로 다른 것으로 보인다.</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">What Is Ddx And Ddy</title>
      
      <link href="https://hrmrzizon.github.io/2018/03/04/what-is-ddx-and-ddy/" rel="alternate" type="text/html" title="What Is Ddx And Ddy" />
      <published>2018-03-04T00:00:00+00:00</published>
      <updated>2018-03-04T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2018/03/04/what-is-ddx-and-ddy</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2018/03/04/what-is-ddx-and-ddy/">&lt;p&gt;&lt;em&gt;HLSL&lt;/em&gt; 에는 &lt;em&gt;ddx&lt;/em&gt; 와 &lt;em&gt;ddy&lt;/em&gt; &lt;em&gt;intrisic&lt;/em&gt; 이 &lt;em&gt;Shader Model 2.0&lt;/em&gt; 부터 존재했다. 필자는 이를 이해하기 위해 자료를 찾아보았지만 쉽게 이해되는 것들은 거의 없었다. 이해한 것을 정리하기 위해 이글을 쓴다.&lt;/p&gt;

&lt;p&gt;예전부터 &lt;em&gt;Pixel Shader&lt;/em&gt; 를 처리할 때 픽셀 단위로 하나하나 처리하는게 아닌 적어도 2x2 개의 픽셀들을 한꺼번에 처리했다고 한다. 그래서 이러한 아키텍쳐를 이용한 키워드가 &lt;em&gt;ddx&lt;/em&gt; 와 &lt;em&gt;ddy&lt;/em&gt; 다. 기본적으로 쉐이더는 병렬로 처리되기 때문에, 4개의 &lt;em&gt;Pixel Shader&lt;/em&gt; 가 한꺼번에 실행되는 것으로 생각할 수 있다. 아래 코드를 보면서 생각해보자.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-hlsl&quot;&gt;    half3 dpdx = ddx(position);
    half3 dpdy = ddy(position);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;4개의 픽셀 쉐이더가 첫번째 라인을 실행할 때 ddx 는 들어온 파라미터의 x축, 가로의 픽셀들의 파라미터의 차이를 구해 반환한다. 이는 &lt;em&gt;δ/δx&lt;/em&gt; 의 의미와 같다. 즉 x 를 기준으로 편미분을 한것이라고 한다. 마찬가지로 ddy 는 y축을 기준으로 차이를 계산해 반환하는 키워드로 생각하면 된다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Shader Model 5.0&lt;/em&gt; 부터는 &lt;em&gt;ddx_coarse/ddy_coarse&lt;/em&gt; 와 &lt;em&gt;ddx_fine/ddy_fine&lt;/em&gt; 으로 키워드가 나뉜다. 기존의 &lt;em&gt;ddx/ddy&lt;/em&gt; 는 &lt;em&gt;ddx_coarse/ddy_coarse&lt;/em&gt; 와 같다고 한다. &lt;em&gt;fine&lt;/em&gt; 과 &lt;em&gt;coarse&lt;/em&gt; 의 차이는 간단하다. 4개의 픽셀을 기준으로 각각의 차이를 전부 구하는게 &lt;em&gt;fine&lt;/em&gt;, 한쪽의 차이만 구하는게 &lt;em&gt;coarse&lt;/em&gt; 라고 한다. 자세한 것은 아래 참조에서 보는 것을 추천한다.&lt;/p&gt;

&lt;h2&gt;참조&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/bb509588.aspx&quot;&gt;MSDN HLSL Intrisic : ddx&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://gamedev.stackexchange.com/questions/62648/what-does-ddx-hlsl-actually-do&quot;&gt;gamedev.stackexchange.net : What does ddx (hlsl) actually do?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://fgiesen.wordpress.com/2011/07/10/a-trip-through-the-graphics-pipeline-2011-part-8/#comment-1990&quot;&gt;The ryg blog : A trip through the Graphics Pipeline 2011, part 8&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/hh446950.aspx&quot;&gt;MSDN Shader Model Assembly 5.0 : deriv_rtx_fine&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/hh446948.aspx&quot;&gt;MSDN Shader Model Assembly 5.0 : deriv_rtx_coarse &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="shader" />
      
        <category term="hlsl" />
      

      

      
        <summary type="html">HLSL 에는 ddx 와 ddy intrisic 이 Shader Model 2.0 부터 존재했다. 필자는 이를 이해하기 위해 자료를 찾아보았지만 쉽게 이해되는 것들은 거의 없었다. 이해한 것을 정리하기 위해 이글을 쓴다. 예전부터 Pixel Shader 를 처리할 때 픽셀 단위로 하나하나 처리하는게 아닌 적어도 2x2 개의 픽셀들을 한꺼번에 처리했다고 한다. 그래서 이러한 아키텍쳐를 이용한 키워드가 ddx 와 ddy 다. 기본적으로 쉐이더는 병렬로 처리되기 때문에, 4개의 Pixel Shader 가 한꺼번에 실행되는 것으로 생각할 수 있다. 아래 코드를 보면서 생각해보자. half3 dpdx = ddx(position); half3 dpdy = ddy(position); 4개의 픽셀 쉐이더가 첫번째 라인을 실행할 때 ddx 는 들어온 파라미터의 x축, 가로의 픽셀들의 파라미터의 차이를 구해 반환한다. 이는 δ/δx 의 의미와 같다. 즉 x 를 기준으로 편미분을 한것이라고 한다. 마찬가지로 ddy 는 y축을 기준으로 차이를 계산해 반환하는 키워드로 생각하면 된다. Shader Model 5.0 부터는 ddx_coarse/ddy_coarse 와 ddx_fine/ddy_fine 으로 키워드가 나뉜다. 기존의 ddx/ddy 는 ddx_coarse/ddy_coarse 와 같다고 한다. fine 과 coarse 의 차이는 간단하다. 4개의 픽셀을 기준으로 각각의 차이를 전부 구하는게 fine, 한쪽의 차이만 구하는게 coarse 라고 한다. 자세한 것은 아래 참조에서 보는 것을 추천한다. 참조 MSDN HLSL Intrisic : ddx gamedev.stackexchange.net : What does ddx (hlsl) actually do? The ryg blog : A trip through the Graphics Pipeline 2011, part 8 MSDN Shader Model Assembly 5.0 : deriv_rtx_fine MSDN Shader Model Assembly 5.0 : deriv_rtx_coarse</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Gpu Branching And Divergence</title>
      
      <link href="https://hrmrzizon.github.io/2018/02/19/gpu-branching-and-divergence/" rel="alternate" type="text/html" title="Gpu Branching And Divergence" />
      <published>2018-02-19T00:00:00+00:00</published>
      <updated>2018-02-19T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2018/02/19/gpu-branching-and-divergence</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2018/02/19/gpu-branching-and-divergence/">&lt;p&gt;요즘은 꽤나 많은 것들을 GPU 로 처리할 수 있다. GPGPU 기술이 나온지 10년이 넘어가는 이 시점에서 꽤나 많은 것들이 GPU 로 처리되고 있다. 그 중에서도 GPGPU 를 다뤄볼 사람이라면 필수적인 상식하나가 있다. 아래 그림을 보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/prior_simt.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://composter.com.ua/documents/Volta-Architecture-Whitepaper.pdf&quot;&gt;NVidia : GV100 Whitepaper&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위 그림은 분기가 나뉘어져 있는 코드를 여러개의 스레드가 실행하는 것을 보여준다. 즉 GPU 에서의 코드 실행 모습이다. 왼쪽은 스레드의 번호로 나뉘는 간단한 분기 코드다. 이는 CPU 에서는 크게 문제가 없다. 하지만 분기가 있던 말던 모든 명령어들을(분기안의 코드들) 전부 실행시키는 것이다. 오른쪽의 그림에서 설명하는 것은 이를 실행하는 쓰레드의 모습을 나타낸다. 딱 봐도 이 그림은 처리가 비효율적일 것처럼 보인다.&lt;/p&gt;

&lt;p&gt;CPU 는 한 쓰레드에서 하나의 &lt;em&gt;Program Conter&lt;/em&gt; 를 가지기 때문에 분기가 나오면 조건에 맞게 단순히 포인터를 증가시키기만 한다. 하지만 &lt;em&gt;SIMT&lt;/em&gt;(&lt;em&gt;Single Instruction Multiple Threads&lt;/em&gt;) 의 구조를 가진 GPU 에서의 분기는 조금 다르다. 여태까지는 여러개의 쓰레드를 가진 그룹 하나당 &lt;em&gt;Program Counter&lt;/em&gt; 를 가지는게 일반적이였다. 그래서 위와같이 동시에 활성화된 쓰레드들 끼리만 실행하게 되는 것이다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Volta&lt;/em&gt; 아키텍쳐에서는 이를 개선시켜 한 &lt;em&gt;Thread&lt;/em&gt; 당 하나의 &lt;em&gt;Program Counter&lt;/em&gt; 와 &lt;em&gt;Call-Stack&lt;/em&gt; 을 두므로써 각 &lt;em&gt;Thread&lt;/em&gt; 를 독립적으로 실행시키게 해준다고 한다. &lt;em&gt;SIMT&lt;/em&gt; 의 &lt;em&gt;Concurrency&lt;/em&gt; 를 고려하여 전부 한꺼번에 실행시키지는 못하지만 각각의 &lt;em&gt;Thread&lt;/em&gt; 를 같은 명령별로 그룹지어 실행시키거나, 한번에 실행시키는게 아니라 각 그룹의 실행을 클럭이나 시분할로 쪼개어 실행하는 것들을 지원한다고 한다. 아래 그림을 보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/interleaved_execution.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://composter.com.ua/documents/Volta-Architecture-Whitepaper.pdf&quot;&gt;NVidia : GV100 Whitepaper&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;하지만 이는 그림에서 나와 있다시피 그다지 효율적인 실행은 아니다. 결국 아직은 분기의 사용은 최소화해야될 것으로 보인다. 다만 이 기능들은 &lt;em&gt;Graphic Processing Unit&lt;/em&gt; 들이 &lt;em&gt;Accelerator&lt;/em&gt; 로 바뀌는 하나의 과정으로 볼 수 있을듯 하다.&lt;/p&gt;

&lt;h2&gt;참조&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://composter.com.ua/documents/Volta-Architecture-Whitepaper.pdf&quot;&gt;NVidia : GV100 Whitepaper&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="gpu" />
      
        <category term="gpgpu" />
      

      

      
        <summary type="html">요즘은 꽤나 많은 것들을 GPU 로 처리할 수 있다. GPGPU 기술이 나온지 10년이 넘어가는 이 시점에서 꽤나 많은 것들이 GPU 로 처리되고 있다. 그 중에서도 GPGPU 를 다뤄볼 사람이라면 필수적인 상식하나가 있다. 아래 그림을 보자. 출처 : NVidia : GV100 Whitepaper 위 그림은 분기가 나뉘어져 있는 코드를 여러개의 스레드가 실행하는 것을 보여준다. 즉 GPU 에서의 코드 실행 모습이다. 왼쪽은 스레드의 번호로 나뉘는 간단한 분기 코드다. 이는 CPU 에서는 크게 문제가 없다. 하지만 분기가 있던 말던 모든 명령어들을(분기안의 코드들) 전부 실행시키는 것이다. 오른쪽의 그림에서 설명하는 것은 이를 실행하는 쓰레드의 모습을 나타낸다. 딱 봐도 이 그림은 처리가 비효율적일 것처럼 보인다. CPU 는 한 쓰레드에서 하나의 Program Conter 를 가지기 때문에 분기가 나오면 조건에 맞게 단순히 포인터를 증가시키기만 한다. 하지만 SIMT(Single Instruction Multiple Threads) 의 구조를 가진 GPU 에서의 분기는 조금 다르다. 여태까지는 여러개의 쓰레드를 가진 그룹 하나당 Program Counter 를 가지는게 일반적이였다. 그래서 위와같이 동시에 활성화된 쓰레드들 끼리만 실행하게 되는 것이다. Volta 아키텍쳐에서는 이를 개선시켜 한 Thread 당 하나의 Program Counter 와 Call-Stack 을 두므로써 각 Thread 를 독립적으로 실행시키게 해준다고 한다. SIMT 의 Concurrency 를 고려하여 전부 한꺼번에 실행시키지는 못하지만 각각의 Thread 를 같은 명령별로 그룹지어 실행시키거나, 한번에 실행시키는게 아니라 각 그룹의 실행을 클럭이나 시분할로 쪼개어 실행하는 것들을 지원한다고 한다. 아래 그림을 보자. 출처 : NVidia : GV100 Whitepaper 하지만 이는 그림에서 나와 있다시피 그다지 효율적인 실행은 아니다. 결국 아직은 분기의 사용은 최소화해야될 것으로 보인다. 다만 이 기능들은 Graphic Processing Unit 들이 Accelerator 로 바뀌는 하나의 과정으로 볼 수 있을듯 하다. 참조 NVidia : GV100 Whitepaper</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Frustum Traced Shadow With Irrelgular Z Buffer 2</title>
      
      <link href="https://hrmrzizon.github.io/2018/01/14/frustum-traced-shadow-with-irrelgular-z-buffer-2/" rel="alternate" type="text/html" title="Frustum Traced Shadow With Irrelgular Z Buffer 2" />
      <published>2018-01-14T00:00:00+00:00</published>
      <updated>2018-01-14T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2018/01/14/frustum-traced-shadow-with-irrelgular-z-buffer-2</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2018/01/14/frustum-traced-shadow-with-irrelgular-z-buffer-2/">&lt;p&gt;&lt;a href=&quot;/2018/01/13/frustum-traced-shadow-with-irrelgular-z-buffer-1/&quot;&gt;frustum Traced Shadow with Irregular Z-Buffer 1&lt;/a&gt; 에서 포괄적인 전체 시스템과 복잡도에 대하여 알아보았다. 이번 글에서는 시스템 구현에 관한 디테일한 사항들을 알아볼 것이다.&lt;/p&gt;

&lt;p&gt;첫번째로는 &lt;em&gt;Irregular Z-Buffer&lt;/em&gt; 와 &lt;em&gt;Sampling Rate&lt;/em&gt; 간의 최적화다. 논문의 저자는 기본적으로 &lt;em&gt;32spp&lt;/em&gt; (&lt;em&gt;sampling per pixel&lt;/em&gt;) 를 제안했다. 정확히 짚자면, &lt;em&gt;Light-Space&lt;/em&gt; 에서 &lt;em&gt;Occluder Geometry&lt;/em&gt; 를 &lt;em&gt;Conservative Rasterization&lt;/em&gt; 을 하면 &lt;em&gt;Visibility Test&lt;/em&gt; 를 계산하는 것이, 한번 &lt;em&gt;Visibility Test&lt;/em&gt; 를 할때 32번을 하는게 가장 신경쓰이는 부분이다. 이의 결과를 저장하기 위해 두가지 방법이 있다고 한다. 하나는 &lt;em&gt;μQuad&lt;/em&gt; 를 &lt;em&gt;Light-Space&lt;/em&gt; 에서 &lt;em&gt;IZB&lt;/em&gt; 를 만들 떄 &lt;em&gt;Rasterize&lt;/em&gt; 하는 것, 다른 방법은 32 번의 &lt;em&gt;Visibility Test&lt;/em&gt; 샘플링 결과를 &lt;em&gt;IZB&lt;/em&gt; 에 저장하는 것이다. 전자는 비용이 크기 때문에 안쓰고, 후자를 선택했다고 한다. 이를 &lt;em&gt;Sample-based insertion&lt;/em&gt; 이라고 명명했다. 그래서 이 방식으로 &lt;em&gt;Prototype&lt;/em&gt; 을 만들어 보니, &lt;em&gt;IZB&lt;/em&gt; 의 중복을 위한 최적화를 했음에도 불구하고 한 픽셀당 8개 이상의 &lt;em&gt;IZB Node&lt;/em&gt; 가 생성되었다고 한다.&lt;/p&gt;

&lt;p&gt;그래서 고안해낸 간단한 근사(&lt;em&gt;approximate&lt;/em&gt;)하는 방법을 언급한다. &lt;em&gt;μQuad&lt;/em&gt; 의 &lt;em&gt;Normal&lt;/em&gt; 벡터와 &lt;em&gt;View Ray&lt;/em&gt;(&lt;em&gt;Eye Direction&lt;/em&gt;) 벡터의 내적 값이 0에 가까워질수록(90도에 가까워질수록) &lt;em&gt;μQuad&lt;/em&gt; 를 늘리는 것이다. 아래 그림의 왼쪽 그림을 보면 쉽게 이해할 수 있다.
그래서 고안해낸 간단한 근사(&lt;em&gt;approximate&lt;/em&gt;)하는 방법을 언급한다. &lt;em&gt;μQuad&lt;/em&gt; 의 &lt;em&gt;Normal&lt;/em&gt; 벡터와 &lt;em&gt;View Ray&lt;/em&gt;(&lt;em&gt;Eye Direction&lt;/em&gt;) 벡터의 내적 값이 0에 가까워질수록(90도에 가까워질수록) &lt;em&gt;μQuad&lt;/em&gt; 를 늘리는 것이다. 아래 그림의 왼쪽 그림을 보면 쉽게 이해할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/fts_microquad_elongate.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://cwyman.org/papers/tvcg16_ftizbExtended.pdf&quot;&gt;Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;보다 정확하게 계산 방법을 설명하자면, &lt;em&gt;μQuad&lt;/em&gt; 의 넓이는 표면의 방향과 상관없이 상수로 정해주고 &lt;em&gt;View Ray&lt;/em&gt; 의 앞뒤 방향으로 &lt;em&gt;μQuad&lt;/em&gt; 의 길이가 늘어나고, 그 방향을 따라서 1줄로 샘플링을 한다. 1차원이라고도 할 수 있겠다. 1 ~ 8 개의 샘플링을 해준다고한다. 위 그림의 오른쪽 그림을 보면 쉽게 이해할 수 있다.&lt;/p&gt;

&lt;p&gt;다만 이는 단지 &lt;em&gt;Irregular Z-Buffer&lt;/em&gt; 를 &lt;em&gt;Approximate&lt;/em&gt;(근사) 하는 것이기 때문에 오차가 생길 수 있다. &lt;em&gt;μQuad&lt;/em&gt; 가 커질수록 &lt;em&gt;IZB Node&lt;/em&gt; 를 넣는 것을 놓치고, &lt;em&gt;Light Leak&lt;/em&gt; 을 발생시킬 수 있다. 보통 가리는 물체가 작거나, 멀리있는 경우에 해당된다. &lt;em&gt;Light Leak&lt;/em&gt; 을 없에는 방법은 몇가지가 존재하는데, 가장 쉬운 방법은 &lt;em&gt;IZB&lt;/em&gt; 의 기본적인 모토인 1:1 샘플링을 맞춰주는 것이다. 하지만 이는 정확히 해주기에 어려운 경우가 있다고 한다. 그래서 다른 방법을 제시한다. &lt;em&gt;Conservative Rasterization&lt;/em&gt; 은 보통 결과에서 0.5 픽셀을 늘려준다. 하지만 1 픽셀 팽창(&lt;em&gt;dilation&lt;/em&gt;)을 해주는 &lt;em&gt;Conservative Rasterization&lt;/em&gt; 을 사용하면 &lt;em&gt;Light Leak&lt;/em&gt; 을 막을 수 있다. 원래 &lt;em&gt;μQuad&lt;/em&gt; 2차원으로 샘플링을 했었으나 기준을 1차원으로 줄이면서 각각의 폴리곤의 넓이을 늘리는 방식으로 보완한 것이라고 생각하면 되겠다. 아래 그림에 적용을 한 사례가 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/fts_approx-insert_vs_over-conserv-raster.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://cwyman.org/papers/tvcg16_ftizbExtended.pdf&quot;&gt;Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이전 글에서 언급한 복잡도는 O(&lt;em&gt;La&lt;/em&gt; * &lt;em&gt;F&lt;/em&gt;) 이다. 우선 평균적인 리스트의 길이는 줄어든다고 한다. 샘플링을 하는 횟수가 최소 1/4 정도 줄었기 때문이다.(8 / 32) 그리고 더 넓은 &lt;em&gt;Conservative Rasterization&lt;/em&gt; 의 결과로 &lt;em&gt;Fragment&lt;/em&gt; 의 갯수는 최대 60% 증가했다고 한다. 이는 성능상 엄청난 이득을 가져온다.&lt;/p&gt;

&lt;p&gt;하지만 이 방법은 &lt;em&gt;Approximate&lt;/em&gt; 하는 방법이란 것을 알아야 한다. 아주 극성맞은 경우와 안좋은 파라미터 설정에는 &lt;em&gt;Light Leak&lt;/em&gt; 이 발생할 수 있다고 한다. 아래 그림에서 그 경우를 볼 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/fts_lightleaks.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://cwyman.org/papers/tvcg16_ftizbExtended.pdf&quot;&gt;Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;왼쪽의 그림은 정상적으로 그림자가 보일 때, 두번째는 정말 안좋은 경우들이 겹친 &lt;em&gt;Light Leak&lt;/em&gt; 이 발생하는 경우, 세번째 경우는 세팅값을 맞춰주어 &lt;em&gt;Light Leak&lt;/em&gt; 을 없엔 장면이다. 하지만 결과를 잘 모르는 경우에는 이 결과들이 맞는지 아닌지 쉽게 구별할 수 있는 정도는 아니다. 즉 아주 정확한 결과를 원하는게 아니라면 그냥 써도 된다는 말이다.&lt;/p&gt;

&lt;p&gt;두번째는 데이터 구조와 메모리 레이아웃 최적화다. 가장 맨처음 이를 구현할 때는 링크드 리스트의 2D 그리드의 형태로 만들었다고 한다. 각각의 리스트의 노드는 다음노드를 가리키는 포인터와 &lt;em&gt;G-Buffer&lt;/em&gt; 를 참조하기 위한 명시적인&lt;sup id=&quot;fnref:C2&quot;&gt;&lt;a href=&quot;#fn:C2&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; 인덱스로 구성되었다고 한다. 하지만 이 구조는 &lt;em&gt;GPU&lt;/em&gt; 에서의 두가지 쓰레드 동기화를 필요로 했다. 하나는 &lt;em&gt;Global Node Pool&lt;/em&gt; 에서 비어있는 노드를 찾기위한 &lt;em&gt;Global&lt;/em&gt; 동기화&lt;sup id=&quot;fnref:C1&quot;&gt;&lt;a href=&quot;#fn:C1&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;, 나머지는 헤드 포인터(&lt;em&gt;Light-Space Data&lt;/em&gt;)를 업데이트하기 위한 &lt;em&gt;Per-Texel&lt;/em&gt; 동기화&lt;sup id=&quot;fnref:C1:1&quot;&gt;&lt;a href=&quot;#fn:C1&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;였다. 동기화를 많이 걸면 걸수록 성능상으로는 그다지 좋지않다. 그렇기 때문에 데이터 구조를 바꾸었다고 한다.&lt;/p&gt;

&lt;p&gt;여러 시도 끝에 가장 성공적인 결과는 리스트의 각 노드의 크기를 줄이는 것이였다. 이를 위한 준비는 노드를 저장하기 위한 &lt;em&gt;Screen-Space Grid&lt;/em&gt; 버퍼를 미리 할당한다. 그리고 각 노드들은 자신을 기준으로한 다음 노드의 오프셋을 저장한다. 이는 &lt;em&gt;Linked-List&lt;/em&gt; 의 기준으로 보자면 &lt;em&gt;Next Pointer&lt;/em&gt; 가 된다. 이를 논문에서는 간접적인(&lt;em&gt;Implcit&lt;/em&gt;) &lt;em&gt;G-Buffer&lt;/em&gt; 인덱스라고 부른다. 이렇게 계속 픽셀의 노드 정보를 참조하면서 픽셀의 위치를 &lt;em&gt;Linked-List&lt;/em&gt; 의 형태로 나타낼 수 있는 것이다. 아래 중간의 그림의 노란색 화살표는 이를 간단하게 나타냈다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/fts_SimpleLayout.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://cwyman.org/papers/tvcg16_ftizbExtended.pdf&quot;&gt;Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이는 &lt;em&gt;IZB Node&lt;/em&gt; 의 크기도 반으로 줄이고, 위에서 언급한 두가지의 동기화 중 &lt;em&gt;Global&lt;/em&gt; 동기화를 안할 수 있게 되었다. 32spp 그림자를 보여주기 위해서는 적어도 픽셀별로 8개의 노드가 필요했다. 이렇게 반으로 노드의 크기를 줄임으로써 큰 퍼포먼스 향상을 얻게 되었다.&lt;/p&gt;

&lt;p&gt;세번째는 헤드 포인터를 가지고 있는 &lt;em&gt;Light-Space Buffer&lt;/em&gt; 의 해상도다. 일반적인 &lt;em&gt;Shadow Mapping&lt;/em&gt; 기법의 &lt;em&gt;Shadow Map&lt;/em&gt; 의 해상도는 보여지는 정도를 결정하지만, 여기서의 해상도는 퍼포먼스를 결정한다.(&lt;em&gt;La&lt;/em&gt;) 1920 x 1080 을 기준으로 추천하는 해상도는 1400 ~ 2500 사이라고 한다.&lt;/p&gt;

&lt;p&gt;네번째로는 기존의 &lt;em&gt;Shadow Mapping&lt;/em&gt; 의 잘 알려진 기법인 &lt;em&gt;Cascaded Shadow Mapping&lt;/em&gt;&lt;sup id=&quot;fnref:P1&quot;&gt;&lt;a href=&quot;#fn:P1&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; 을 이 기법에 적용시키는 것이다. 이 기법의 원리는 &lt;em&gt;View frustum&lt;/em&gt; 을 원하는 갯수대로 쪼갠 후, 쪼개진 &lt;em&gt;frustum&lt;/em&gt; 안의 오브젝트들의 &lt;em&gt;Shadow&lt;/em&gt; 를 계산한다. 논문에서 쪼개는 방법은 &lt;em&gt;Sample Distribution Shadow Map&lt;/em&gt; 과 &lt;em&gt;Logarithm Partitioning&lt;/em&gt; 을 언급했다. 여기서는 쪼개진 &lt;em&gt;frustum&lt;/em&gt; 마다 전부 &lt;em&gt;IZB&lt;/em&gt; 를 생성한다. 이때 각각의 쪼개진 &lt;em&gt;frustum&lt;/em&gt; 의 끝부분이 잘 맞도록 신경써야줘야 한다고 언급했다. 논문의 저자는 구현할때 &lt;em&gt;2D Texture Array&lt;/em&gt; 를 사용하여 &lt;em&gt;IZB&lt;/em&gt; 를 저장하고, 병렬로 각각의 &lt;em&gt;Detph Texture&lt;/em&gt; 마다 &lt;em&gt;Light-Space Culling Prepass&lt;/em&gt; 를 넣어줬다고 한다. 일반적으로 각각의 &lt;em&gt;Cascade&lt;/em&gt; 를 계산할때는 한개당 하나의 &lt;em&gt;Pass&lt;/em&gt; 를 사용하여 계산하는데, 여기서는 1 Pass 로 적절히 프리미티브를 나누어 성능 향상을 고려했다고 한다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Cascade&lt;/em&gt; 의 적용은 &lt;em&gt;Occluder Geometry&lt;/em&gt; 의 &lt;em&gt;Rasterize&lt;/em&gt; 퍼포먼스를 안고 가면서 &lt;em&gt;Thread Divergence&lt;/em&gt; 의 시간을 줄여준다. 이는 사용시 적절한 타협점을 찾아야 한다는 뜻으로, 보통은 두개의 &lt;em&gt;Cascade&lt;/em&gt; 를 사용하고, 복잡한 게임에서는 3개나 4개의 &lt;em&gt;Cascade&lt;/em&gt; 를 사용하여 상황에 따라 뜀뛰는 시간을 최소화 시킨다. 아래 그림은 &lt;em&gt;Cascade&lt;/em&gt; 의 효과를 증명해준다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/fts_cascaded-izb.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://cwyman.org/papers/tvcg16_ftizbExtended.pdf&quot;&gt;Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;다섯번째로는 &lt;em&gt;N dot L Culling&lt;/em&gt; 이다. 일반적으로 &lt;em&gt;N dot L&lt;/em&gt; 의 값이 음수가 되는 경우에는 0으로 클램핑하여 사용한다. 이 말은 값이 음수나 0 인 경우에는 무조건 &lt;em&gt;Shadow&lt;/em&gt; 가 비춘다는 말이다. 이때는 &lt;em&gt;La&lt;/em&gt; : 평균적인 &lt;em&gt;IZB&lt;/em&gt; 리스트의 길이를 줄여 성능 향상을 해줄 수 있다고 한다. 보통은 10 ~ 15% 의 성능향상을 해준다고 한다.&lt;/p&gt;

&lt;p&gt;여섯번째로는 &lt;em&gt;Early-Z&lt;/em&gt; 의 개념을 응용한 &lt;em&gt;Early-Out&lt;/em&gt; 이다. &lt;em&gt;Visibility Test&lt;/em&gt; 에서 한 픽셀을 완전하게 그림자를 드리우는 경우, 다음 후속으로 같은 픽셀에 &lt;em&gt;Node&lt;/em&gt; 가 추가될 필요가 없다. 그러므로 완전히 그림자 처리가 되는 부분은 &lt;em&gt;Node&lt;/em&gt; 를 지워준다. 이때 &lt;em&gt;atomic&lt;/em&gt; 연산을 사용하지 않는데, 최악에 경우에는 &lt;em&gt;Visibility Test&lt;/em&gt; 를 다시할 수도 있다. &lt;em&gt;Early-Out&lt;/em&gt; 은 추가적인 시간과 메모리를 요구함에도 불구하고 10 ~ 15% 의 성능향상을 보인다.&lt;/p&gt;

&lt;p&gt;일곱번째로는 &lt;em&gt;Unchanged Mask&lt;/em&gt; 를 이용한 메모리 동기화다. &lt;em&gt;Visibility Test&lt;/em&gt; 는 메모리 대역폭, 처리량, 동기화로 인하여 병목이 일어난다. 픽셀 각각의 &lt;em&gt;Visibility Mask&lt;/em&gt; 를 사용해 동기화를 한다. 정확히 말하면 각각의 폴리곤들이 픽셀의 가시성을 테스트 할때 &lt;em&gt;Race Condition&lt;/em&gt; 을 피하기 위하여 동기화를 하여 &lt;em&gt;Visibility&lt;/em&gt; 를 기록한다. 그러므로 &lt;em&gt;Visibility Mask&lt;/em&gt; 는 반드시 폴리곤이 기존의 &lt;em&gt;Visibility&lt;/em&gt; 를 바꿀 때만 업데이트된다. 바뀌는지 비교를 하기위해 이전에 사용한 마스크를 써야하지만, 이는 최고 14% 의 성능 향상을 보여준다고 한다.&lt;/p&gt;

&lt;p&gt;마지막으로는 코드를 통한 &lt;em&gt;Latency Hiding&lt;/em&gt; 이다. 단계가 복잡하여 &lt;em&gt;Memory Latency&lt;/em&gt; 가 꽤나 긴편인데, GPU 에서는 이 &lt;em&gt;Latency&lt;/em&gt; 를 감출 방법이 없다. 다행히 사전에 루프를 돌면서 &lt;em&gt;G-Buffer&lt;/em&gt; 좌표를 계산하여 &lt;em&gt;Latency Hiding&lt;/em&gt; 이 가능하다고 한다. 이는 5 ~ 15% 성능 향상을 보였다고 한다.&lt;/p&gt;

&lt;p&gt;시스템 구현에 대한 디테일한 사항은 여기까지가 끝이다.&lt;/p&gt;

&lt;p&gt;마지막으로 &lt;em&gt;Transparency Geometry&lt;/em&gt; 를 처리하는 방법에 대해서 써보겠다. 이 기법의 &lt;em&gt;per-pixel&lt;/em&gt; 테스트는 &lt;em&gt;Visibility Mask Buffer&lt;/em&gt; 에 결과가 저장된다. &lt;em&gt;Visibility Mask Buffer&lt;/em&gt; 의 효율적인 사용을 위해 항상 각 픽셀의 여러개의 32bit 데이터를 저장해준다고 한다. 이정도의 크기라면 단지 &lt;em&gt;Visibility&lt;/em&gt; 만을 사용하는게 아니라 &lt;em&gt;Opacity&lt;/em&gt; 또한 저장이 가능하다. 통상적인 가시성을 위한 투명 오브젝트의 처리 방법은 &lt;em&gt;Alpha to Coverage&lt;/em&gt;&lt;sup id=&quot;fnref:C3&quot;&gt;&lt;a href=&quot;#fn:C3&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; 를 쓴다고 한다. 그리고 여기에서도 비슷한 방법을 사용할 수 있다고한다.&lt;/p&gt;

&lt;p&gt;처음에는 &lt;em&gt;Coverage&lt;/em&gt; 를 계산하기 위해 &lt;em&gt;Visibility Test&lt;/em&gt; 를 해준다. 그리고 해당 알파가 저장된 텍스쳐를 참조하여 투명도를 가져오고, 해당 투명도를 사용하여 &lt;em&gt;Alpha to Coverage&lt;/em&gt; 를 실행하여 투명도 마스크를 얻는다. 이를 비트 AND 연산으로 합쳐서 &lt;em&gt;Coverage&lt;/em&gt; 를 &lt;em&gt;Visibility Buffer&lt;/em&gt; 에 저장한다.&lt;/p&gt;

&lt;p&gt;이 기법에서 알파 데이터를 처리하는 방법은 두가지로 나뉜다. 적은 비용으로 &lt;em&gt;Aliasing&lt;/em&gt; 을 생기게 하는 방법과 높은 비용으로 완벽하게 구현하는 방법으로 나뉜다. 적은 비용의 방식은 &lt;em&gt;Alpha&lt;/em&gt; 텍스쳐의 값을 &lt;em&gt;IZB Node&lt;/em&gt; 를 순회하기 전에 가져와서 계산하는 방식이다. &lt;em&gt;Light-Space Texel&lt;/em&gt; 을 기준으로 계산하므로 &lt;em&gt;Aliasing&lt;/em&gt; 이 생길 것으롸 예상된다. 하지만 이 논문의 저자는 구현물을 이 방식으로 구현했다고 한다. 나머지 한개의 방식은 &lt;em&gt;IZB Node&lt;/em&gt; 를 하나하나 순회하면서 &lt;em&gt;Alpha&lt;/em&gt; 텍스쳐의 값을 가져와 계산하는 것이다. 이는 일반적으로 생각되는 텍스쳐 샘플링의 부하와 &lt;em&gt;Varing&lt;/em&gt; 부하를 생기게 한다. 이는 꽤나 큰 비용이라고 한다.&lt;/p&gt;

&lt;p&gt;여기까지가 끝이다. 논문에 그 다음 내용들은 전부 퍼포먼스들의 분석밖에 없다. 다음으로 쓸 내용은 &lt;em&gt;HFTS&lt;/em&gt; 에 대한 내용이다.&lt;/p&gt;

&lt;h2&gt;참조&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://cwyman.org/papers/tvcg16_ftizbExtended.pdf&quot;&gt;Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://software.intel.com/en-us/articles/sample-distribution-shadow-maps&quot;&gt;Intel Developer Zone : Sample Distribution Shadow Maps&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:C2&quot;&gt;
      &lt;p&gt;명시적이란 뜻은 바로 넣어서 계산할 수 있는 절대적인 위치의 텍셀 인덱스를 뜻한다. &lt;a href=&quot;#fnref:C2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:C1&quot;&gt;
      &lt;p&gt;여기서의 동기화는 &lt;em&gt;atomic&lt;/em&gt; 의 개념을 말한다. 성능상의 단점은 다른 쓰레드에서 선점하는 경우에는 기다리는 것이다. &lt;a href=&quot;#fnref:C1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:C1:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:P1&quot;&gt;
      &lt;p&gt;이 블로그에서 &lt;em&gt;Cascaded Shadow Mapping&lt;/em&gt; 에 대한 내용을 다루었었다. &lt;a href=&quot;/2017/12/17/cascaded-shadow-mapping/&quot;&gt;여기&lt;/a&gt;에서 볼 수 있다. &lt;a href=&quot;#fnref:P1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:C3&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://medium.com/@bgolus/anti-aliased-alpha-test-the-esoteric-alpha-to-coverage-8b177335ae4f&quot;&gt;https://medium.com/@bgolus/anti-aliased-alpha-test-the-esoteric-alpha-to-coverage-8b177335ae4f&lt;/a&gt; &lt;a href=&quot;#fnref:C3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="shader" />
      
        <category term="shadow" />
      
        <category term="rendering" />
      
        <category term="fts" />
      

      

      
        <summary type="html">frustum Traced Shadow with Irregular Z-Buffer 1 에서 포괄적인 전체 시스템과 복잡도에 대하여 알아보았다. 이번 글에서는 시스템 구현에 관한 디테일한 사항들을 알아볼 것이다. 첫번째로는 Irregular Z-Buffer 와 Sampling Rate 간의 최적화다. 논문의 저자는 기본적으로 32spp (sampling per pixel) 를 제안했다. 정확히 짚자면, Light-Space 에서 Occluder Geometry 를 Conservative Rasterization 을 하면 Visibility Test 를 계산하는 것이, 한번 Visibility Test 를 할때 32번을 하는게 가장 신경쓰이는 부분이다. 이의 결과를 저장하기 위해 두가지 방법이 있다고 한다. 하나는 μQuad 를 Light-Space 에서 IZB 를 만들 떄 Rasterize 하는 것, 다른 방법은 32 번의 Visibility Test 샘플링 결과를 IZB 에 저장하는 것이다. 전자는 비용이 크기 때문에 안쓰고, 후자를 선택했다고 한다. 이를 Sample-based insertion 이라고 명명했다. 그래서 이 방식으로 Prototype 을 만들어 보니, IZB 의 중복을 위한 최적화를 했음에도 불구하고 한 픽셀당 8개 이상의 IZB Node 가 생성되었다고 한다. 그래서 고안해낸 간단한 근사(approximate)하는 방법을 언급한다. μQuad 의 Normal 벡터와 View Ray(Eye Direction) 벡터의 내적 값이 0에 가까워질수록(90도에 가까워질수록) μQuad 를 늘리는 것이다. 아래 그림의 왼쪽 그림을 보면 쉽게 이해할 수 있다. 그래서 고안해낸 간단한 근사(approximate)하는 방법을 언급한다. μQuad 의 Normal 벡터와 View Ray(Eye Direction) 벡터의 내적 값이 0에 가까워질수록(90도에 가까워질수록) μQuad 를 늘리는 것이다. 아래 그림의 왼쪽 그림을 보면 쉽게 이해할 수 있다. 출처 : Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows 보다 정확하게 계산 방법을 설명하자면, μQuad 의 넓이는 표면의 방향과 상관없이 상수로 정해주고 View Ray 의 앞뒤 방향으로 μQuad 의 길이가 늘어나고, 그 방향을 따라서 1줄로 샘플링을 한다. 1차원이라고도 할 수 있겠다. 1 ~ 8 개의 샘플링을 해준다고한다. 위 그림의 오른쪽 그림을 보면 쉽게 이해할 수 있다. 다만 이는 단지 Irregular Z-Buffer 를 Approximate(근사) 하는 것이기 때문에 오차가 생길 수 있다. μQuad 가 커질수록 IZB Node 를 넣는 것을 놓치고, Light Leak 을 발생시킬 수 있다. 보통 가리는 물체가 작거나, 멀리있는 경우에 해당된다. Light Leak 을 없에는 방법은 몇가지가 존재하는데, 가장 쉬운 방법은 IZB 의 기본적인 모토인 1:1 샘플링을 맞춰주는 것이다. 하지만 이는 정확히 해주기에 어려운 경우가 있다고 한다. 그래서 다른 방법을 제시한다. Conservative Rasterization 은 보통 결과에서 0.5 픽셀을 늘려준다. 하지만 1 픽셀 팽창(dilation)을 해주는 Conservative Rasterization 을 사용하면 Light Leak 을 막을 수 있다. 원래 μQuad 2차원으로 샘플링을 했었으나 기준을 1차원으로 줄이면서 각각의 폴리곤의 넓이을 늘리는 방식으로 보완한 것이라고 생각하면 되겠다. 아래 그림에 적용을 한 사례가 있다. 출처 : Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows 이전 글에서 언급한 복잡도는 O(La * F) 이다. 우선 평균적인 리스트의 길이는 줄어든다고 한다. 샘플링을 하는 횟수가 최소 1/4 정도 줄었기 때문이다.(8 / 32) 그리고 더 넓은 Conservative Rasterization 의 결과로 Fragment 의 갯수는 최대 60% 증가했다고 한다. 이는 성능상 엄청난 이득을 가져온다. 하지만 이 방법은 Approximate 하는 방법이란 것을 알아야 한다. 아주 극성맞은 경우와 안좋은 파라미터 설정에는 Light Leak 이 발생할 수 있다고 한다. 아래 그림에서 그 경우를 볼 수 있다. 출처 : Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows 왼쪽의 그림은 정상적으로 그림자가 보일 때, 두번째는 정말 안좋은 경우들이 겹친 Light Leak 이 발생하는 경우, 세번째 경우는 세팅값을 맞춰주어 Light Leak 을 없엔 장면이다. 하지만 결과를 잘 모르는 경우에는 이 결과들이 맞는지 아닌지 쉽게 구별할 수 있는 정도는 아니다. 즉 아주 정확한 결과를 원하는게 아니라면 그냥 써도 된다는 말이다. 두번째는 데이터 구조와 메모리 레이아웃 최적화다. 가장 맨처음 이를 구현할 때는 링크드 리스트의 2D 그리드의 형태로 만들었다고 한다. 각각의 리스트의 노드는 다음노드를 가리키는 포인터와 G-Buffer 를 참조하기 위한 명시적인1 인덱스로 구성되었다고 한다. 하지만 이 구조는 GPU 에서의 두가지 쓰레드 동기화를 필요로 했다. 하나는 Global Node Pool 에서 비어있는 노드를 찾기위한 Global 동기화2, 나머지는 헤드 포인터(Light-Space Data)를 업데이트하기 위한 Per-Texel 동기화2였다. 동기화를 많이 걸면 걸수록 성능상으로는 그다지 좋지않다. 그렇기 때문에 데이터 구조를 바꾸었다고 한다. 여러 시도 끝에 가장 성공적인 결과는 리스트의 각 노드의 크기를 줄이는 것이였다. 이를 위한 준비는 노드를 저장하기 위한 Screen-Space Grid 버퍼를 미리 할당한다. 그리고 각 노드들은 자신을 기준으로한 다음 노드의 오프셋을 저장한다. 이는 Linked-List 의 기준으로 보자면 Next Pointer 가 된다. 이를 논문에서는 간접적인(Implcit) G-Buffer 인덱스라고 부른다. 이렇게 계속 픽셀의 노드 정보를 참조하면서 픽셀의 위치를 Linked-List 의 형태로 나타낼 수 있는 것이다. 아래 중간의 그림의 노란색 화살표는 이를 간단하게 나타냈다. 출처 : Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows 이는 IZB Node 의 크기도 반으로 줄이고, 위에서 언급한 두가지의 동기화 중 Global 동기화를 안할 수 있게 되었다. 32spp 그림자를 보여주기 위해서는 적어도 픽셀별로 8개의 노드가 필요했다. 이렇게 반으로 노드의 크기를 줄임으로써 큰 퍼포먼스 향상을 얻게 되었다. 세번째는 헤드 포인터를 가지고 있는 Light-Space Buffer 의 해상도다. 일반적인 Shadow Mapping 기법의 Shadow Map 의 해상도는 보여지는 정도를 결정하지만, 여기서의 해상도는 퍼포먼스를 결정한다.(La) 1920 x 1080 을 기준으로 추천하는 해상도는 1400 ~ 2500 사이라고 한다. 네번째로는 기존의 Shadow Mapping 의 잘 알려진 기법인 Cascaded Shadow Mapping3 을 이 기법에 적용시키는 것이다. 이 기법의 원리는 View frustum 을 원하는 갯수대로 쪼갠 후, 쪼개진 frustum 안의 오브젝트들의 Shadow 를 계산한다. 논문에서 쪼개는 방법은 Sample Distribution Shadow Map 과 Logarithm Partitioning 을 언급했다. 여기서는 쪼개진 frustum 마다 전부 IZB 를 생성한다. 이때 각각의 쪼개진 frustum 의 끝부분이 잘 맞도록 신경써야줘야 한다고 언급했다. 논문의 저자는 구현할때 2D Texture Array 를 사용하여 IZB 를 저장하고, 병렬로 각각의 Detph Texture 마다 Light-Space Culling Prepass 를 넣어줬다고 한다. 일반적으로 각각의 Cascade 를 계산할때는 한개당 하나의 Pass 를 사용하여 계산하는데, 여기서는 1 Pass 로 적절히 프리미티브를 나누어 성능 향상을 고려했다고 한다. Cascade 의 적용은 Occluder Geometry 의 Rasterize 퍼포먼스를 안고 가면서 Thread Divergence 의 시간을 줄여준다. 이는 사용시 적절한 타협점을 찾아야 한다는 뜻으로, 보통은 두개의 Cascade 를 사용하고, 복잡한 게임에서는 3개나 4개의 Cascade 를 사용하여 상황에 따라 뜀뛰는 시간을 최소화 시킨다. 아래 그림은 Cascade 의 효과를 증명해준다. 출처 : Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows 다섯번째로는 N dot L Culling 이다. 일반적으로 N dot L 의 값이 음수가 되는 경우에는 0으로 클램핑하여 사용한다. 이 말은 값이 음수나 0 인 경우에는 무조건 Shadow 가 비춘다는 말이다. 이때는 La : 평균적인 IZB 리스트의 길이를 줄여 성능 향상을 해줄 수 있다고 한다. 보통은 10 ~ 15% 의 성능향상을 해준다고 한다. 여섯번째로는 Early-Z 의 개념을 응용한 Early-Out 이다. Visibility Test 에서 한 픽셀을 완전하게 그림자를 드리우는 경우, 다음 후속으로 같은 픽셀에 Node 가 추가될 필요가 없다. 그러므로 완전히 그림자 처리가 되는 부분은 Node 를 지워준다. 이때 atomic 연산을 사용하지 않는데, 최악에 경우에는 Visibility Test 를 다시할 수도 있다. Early-Out 은 추가적인 시간과 메모리를 요구함에도 불구하고 10 ~ 15% 의 성능향상을 보인다. 일곱번째로는 Unchanged Mask 를 이용한 메모리 동기화다. Visibility Test 는 메모리 대역폭, 처리량, 동기화로 인하여 병목이 일어난다. 픽셀 각각의 Visibility Mask 를 사용해 동기화를 한다. 정확히 말하면 각각의 폴리곤들이 픽셀의 가시성을 테스트 할때 Race Condition 을 피하기 위하여 동기화를 하여 Visibility 를 기록한다. 그러므로 Visibility Mask 는 반드시 폴리곤이 기존의 Visibility 를 바꿀 때만 업데이트된다. 바뀌는지 비교를 하기위해 이전에 사용한 마스크를 써야하지만, 이는 최고 14% 의 성능 향상을 보여준다고 한다. 마지막으로는 코드를 통한 Latency Hiding 이다. 단계가 복잡하여 Memory Latency 가 꽤나 긴편인데, GPU 에서는 이 Latency 를 감출 방법이 없다. 다행히 사전에 루프를 돌면서 G-Buffer 좌표를 계산하여 Latency Hiding 이 가능하다고 한다. 이는 5 ~ 15% 성능 향상을 보였다고 한다. 시스템 구현에 대한 디테일한 사항은 여기까지가 끝이다. 마지막으로 Transparency Geometry 를 처리하는 방법에 대해서 써보겠다. 이 기법의 per-pixel 테스트는 Visibility Mask Buffer 에 결과가 저장된다. Visibility Mask Buffer 의 효율적인 사용을 위해 항상 각 픽셀의 여러개의 32bit 데이터를 저장해준다고 한다. 이정도의 크기라면 단지 Visibility 만을 사용하는게 아니라 Opacity 또한 저장이 가능하다. 통상적인 가시성을 위한 투명 오브젝트의 처리 방법은 Alpha to Coverage4 를 쓴다고 한다. 그리고 여기에서도 비슷한 방법을 사용할 수 있다고한다. 처음에는 Coverage 를 계산하기 위해 Visibility Test 를 해준다. 그리고 해당 알파가 저장된 텍스쳐를 참조하여 투명도를 가져오고, 해당 투명도를 사용하여 Alpha to Coverage 를 실행하여 투명도 마스크를 얻는다. 이를 비트 AND 연산으로 합쳐서 Coverage 를 Visibility Buffer 에 저장한다. 이 기법에서 알파 데이터를 처리하는 방법은 두가지로 나뉜다. 적은 비용으로 Aliasing 을 생기게 하는 방법과 높은 비용으로 완벽하게 구현하는 방법으로 나뉜다. 적은 비용의 방식은 Alpha 텍스쳐의 값을 IZB Node 를 순회하기 전에 가져와서 계산하는 방식이다. Light-Space Texel 을 기준으로 계산하므로 Aliasing 이 생길 것으롸 예상된다. 하지만 이 논문의 저자는 구현물을 이 방식으로 구현했다고 한다. 나머지 한개의 방식은 IZB Node 를 하나하나 순회하면서 Alpha 텍스쳐의 값을 가져와 계산하는 것이다. 이는 일반적으로 생각되는 텍스쳐 샘플링의 부하와 Varing 부하를 생기게 한다. 이는 꽤나 큰 비용이라고 한다. 여기까지가 끝이다. 논문에 그 다음 내용들은 전부 퍼포먼스들의 분석밖에 없다. 다음으로 쓸 내용은 HFTS 에 대한 내용이다. 참조 Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows Intel Developer Zone : Sample Distribution Shadow Maps 명시적이란 뜻은 바로 넣어서 계산할 수 있는 절대적인 위치의 텍셀 인덱스를 뜻한다. &amp;#8617; 여기서의 동기화는 atomic 의 개념을 말한다. 성능상의 단점은 다른 쓰레드에서 선점하는 경우에는 기다리는 것이다. &amp;#8617; &amp;#8617;2 이 블로그에서 Cascaded Shadow Mapping 에 대한 내용을 다루었었다. 여기에서 볼 수 있다. &amp;#8617; https://medium.com/@bgolus/anti-aliased-alpha-test-the-esoteric-alpha-to-coverage-8b177335ae4f &amp;#8617;</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Frustum Traced Shadow With Irrelgular Z Buffer 1</title>
      
      <link href="https://hrmrzizon.github.io/2018/01/13/frustum-traced-shadow-with-irrelgular-z-buffer-1/" rel="alternate" type="text/html" title="Frustum Traced Shadow With Irrelgular Z Buffer 1" />
      <published>2018-01-13T00:00:00+00:00</published>
      <updated>2018-01-13T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2018/01/13/frustum-traced-shadow-with-irrelgular-z-buffer-1</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2018/01/13/frustum-traced-shadow-with-irrelgular-z-buffer-1/">&lt;p&gt;&lt;a href=&quot;/2018/01/13/frustum-traced-shadow-with-irrelgular-z-buffer-0/&quot;&gt;frustum Traced Shadow with Irregular Z-Buffer 0&lt;/a&gt; 에서 기법의 아이디어를 둘러봄으러써 대강 이 알고리즘이 무엇인지 살펴보았다. 이번 글에서는 논문에 수록된 포괄적인 전체 시스템과 복잡도에 대하여 알아볼 것이다.&lt;/p&gt;

&lt;h3&gt;전체 시스템&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;/2018/01/13/frustum-traced-shadow-with-irrelgular-z-buffer-0/&quot;&gt;이전 글&lt;/a&gt;에서 두가지 단계에 대해서 자세한 설명을 했었다. &lt;em&gt;Irregular Z-Buffer&lt;/em&gt; 를 생성하고 &lt;em&gt;Visibility Test&lt;/em&gt; 를 하는 것이였다. 실제 구현된 단계는 총 6개의 단계로 이루어진다고 한다.&lt;/p&gt;

&lt;p&gt;첫번째로는 &lt;em&gt;Eye-Space Z-Prepass&lt;/em&gt; 를 해준다. 요즘의 엔진들이나 큰 규모가 아닌 게임이더라도 &lt;em&gt;Z-Prepass&lt;/em&gt;&lt;sup id=&quot;fnref:L1&quot;&gt;&lt;a href=&quot;#fn:L1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; 는 거의 대부분 해준다. &lt;em&gt;Geometry Pass&lt;/em&gt; 가 두번 걸리기는 하지만 &lt;em&gt;Fill Rate&lt;/em&gt; 의 부하가 &lt;em&gt;Geometry Pass&lt;/em&gt; 의 부하보다 많이 커서 그런 듯하다. 중요한건 단순히 언급한 &lt;em&gt;Eye-Space Z-Prepass&lt;/em&gt; 를 뜻하는게 아니다. 이전에 언급한 &lt;em&gt;μQuad&lt;/em&gt; 의 빠른 계산을 위해 &lt;em&gt;G-Buffer&lt;/em&gt; 에 3개의 실수 값들을 넣는다. 이 3개의 실수는 실제 그려지는 카메라의 위치와 &lt;em&gt;Tangent Plane&lt;/em&gt; 의 4개의 코너중에 3개의 거리를 나타낸다. 이는 &lt;em&gt;μQuad&lt;/em&gt; 를 다시 계산하기에 충분하다고 한다.&lt;/p&gt;

&lt;p&gt;이 방법은 &lt;em&gt;Visibility Test&lt;/em&gt; 의 속도를 빠르게 하는데 도움이 되지만, 당연히 &lt;em&gt;G-Buffer&lt;/em&gt; 의 공간이 부족한 경우에는 쓰지 못한다. AAA 급의 게임들은 &lt;em&gt;G-Buffer&lt;/em&gt; 를 bit 단위로 최대한 아껴쓰기 경우가 많기 때문에 이와 같은 상황이 일어날 수도 있다. 이런 경우에는 명시적으로 &lt;em&gt;Visibility Test&lt;/em&gt; 를 할때 &lt;em&gt;μQuad&lt;/em&gt; 를 계산한다고 한다. 아래 그림은 2009년에 발매된 &lt;em&gt;KillZone 2&lt;/em&gt; 의 &lt;em&gt;G-Buffer&lt;/em&gt; 사용을 나타내는 PT의 한 부분이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/killzone2_g-buffer.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://www.slideshare.net/guerrillagames/deferred-rendering-in-killzone-2-9691589&quot;&gt;Deferred Rendering in Killzone 2&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;두번째로는 씬의 경계를 설정해주는 것이다. &lt;em&gt;Shadow Mapping&lt;/em&gt; 에서 &lt;em&gt;Light-Space Projection&lt;/em&gt; 행렬은 씬에 딱 맞게 해주어야 한다.&lt;sup id=&quot;fnref:C1&quot;&gt;&lt;a href=&quot;#fn:C1&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; 딱 맞지 않는 경우에는 &lt;em&gt;IZB&lt;/em&gt; 에 쓸모없는 텍셀을 생기게 하기 때문이다. 그래서 &lt;em&gt;Light-Space Projection&lt;/em&gt; 행렬을 계산하기 위해 &lt;em&gt;Compute Shader&lt;/em&gt; 와 &lt;em&gt;Z-Buffer&lt;/em&gt; 를 사용하여 &lt;em&gt;Bounding Box&lt;/em&gt; 를 계산한다. 이 계산은 화면의 해상도에 따라서 달라진다. 하지만 논문의 저자는 이 비용이 병목의 큰 원인이 아니기 때문에 특별한 해결 방법을 제시하지는 않는다.&lt;/p&gt;

&lt;p&gt;세번째는 &lt;em&gt;Irregular Z-Buffer&lt;/em&gt; 를 만드는 것이다. 이전 글에서 &lt;em&gt;IZB&lt;/em&gt; 에 대한 대략적인 아이디어는 언급했었다. 더 디테일하게 이를 말해보면, 우선 &lt;em&gt;Eye-Space Z Buffer&lt;/em&gt; 를 참조해 &lt;em&gt;Light-Space&lt;/em&gt; 로 변환한 후에 &lt;em&gt;Light-Space A-Buffer&lt;/em&gt; 의 텍셀의 &lt;em&gt;Linked-List&lt;/em&gt; 에 넣는다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;IZB&lt;/em&gt; 의 발상은 &lt;em&gt;Eye-Space&lt;/em&gt; 의 픽셀과 &lt;em&gt;Light-Space&lt;/em&gt; 의 텍셀이 1:1 로 매칭되지 않고 하나의 텍셀이 참조당하는 횟수가 1을 넘을때 &lt;em&gt;allasing&lt;/em&gt; 이 발생하는 것에서 시작되었다. 그래서 여기서 구현된 &lt;em&gt;IZB&lt;/em&gt; 는 텍셀에 &lt;em&gt;Linked-List&lt;/em&gt; 의 개념을 도입하여 보다 정확히 계산할 수 있게 하였다.&lt;/p&gt;

&lt;p&gt;아래 그림은 &lt;em&gt;IZB&lt;/em&gt; 의 데이터를 나타낸다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/fts_depth_length_cull.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://cwyman.org/papers/tvcg16_ftizbExtended.pdf&quot;&gt;Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;왼쪽의 그림은 일반적인 &lt;em&gt;Shadow Map&lt;/em&gt; 을 나타내고, 중간의 그림은 &lt;em&gt;Linked-List&lt;/em&gt; 의 크기를 나타낸다. 흰색은 &lt;em&gt;Linked-List&lt;/em&gt; 의 크기가 0인 텍셀을 나타내고, 검은색에 가까워질수록 &lt;em&gt;Linked-List&lt;/em&gt; 의 크기가 점점 커지는 것을 나타낸다. 오른쪽의 그림은 필요없는 부분을 0으로 나타내고, 나머지 부분은 0 이상의 숫자를 나타내는 방식이다. 이는 아래에서 언급할 &lt;em&gt;Light-Space Culling Prepass&lt;/em&gt; 에서 쓰인다.&lt;/p&gt;

&lt;p&gt;픽셀별로 여러개의 가시성(가려진 정도)를 나타내는 픽셀은 여러개의 &lt;em&gt;IZB Node&lt;/em&gt; 를 필요로 한다. 일반적인 &lt;em&gt;Shadow Map&lt;/em&gt; 과는 다르게 &lt;em&gt;μQuad&lt;/em&gt; 는 다른 &lt;em&gt;Light-Space Texel&lt;/em&gt; 에 &lt;em&gt;Projection&lt;/em&gt; 이 가능하다.&lt;/p&gt;

&lt;p&gt;대충 생각해보면, &lt;em&gt;N&lt;/em&gt; 개의 픽셀별 쉐도우가 필요하면, &lt;em&gt;N&lt;/em&gt; 개의 &lt;em&gt;IZB Node&lt;/em&gt; 가 필요하다. 하지만 쓸데없는 데이터를(&lt;em&gt;Geometry&lt;/em&gt; 가 없는 경우) 넣지 않을 수 있으므로 &lt;em&gt;M&lt;/em&gt; 개의 &lt;em&gt;Light-Space Texel&lt;/em&gt; 에 &lt;em&gt;μQuad&lt;/em&gt; 를 투영한다면, 우리는 min(&lt;em&gt;N&lt;/em&gt;, &lt;em&gt;M&lt;/em&gt;)개의 &lt;em&gt;IZB Node&lt;/em&gt; 가 필요하다고 알 수 있다.&lt;/p&gt;

&lt;p&gt;아래 그림은 &lt;em&gt;IZB&lt;/em&gt; 를 사용하는 디테일한 구조를 나타낸다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/fts_SimpleLayout.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://cwyman.org/papers/tvcg16_ftizbExtended.pdf&quot;&gt;Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;IZB&lt;/em&gt; 의 데이터 구조는 &lt;em&gt;Light-Space A-Buffer&lt;/em&gt; : &lt;em&gt;Eye-Space A-Buffer&lt;/em&gt; 의 &lt;em&gt;Node&lt;/em&gt; 를 찾아가기 위한 데이터, &lt;em&gt;Eye-Space A-Buffer&lt;/em&gt; : 리스트의 크기를 나타내는 버퍼와 같다. 출력되는 결과 : &lt;em&gt;Visibility Buffer&lt;/em&gt; 또한 따로 존재한다. 이게 최종적으로 그려질때 사용된다.&lt;/p&gt;

&lt;p&gt;다음은 &lt;em&gt;Light-Space Culling Prepass&lt;/em&gt; 다. &lt;em&gt;Visibility Test&lt;/em&gt; 를 위해 &lt;em&gt;Geometry&lt;/em&gt; 의 &lt;em&gt;Light-Space Conservative Rasterization&lt;/em&gt; 을 하게 되는데, GPU 에서 &lt;em&gt;Early-Z&lt;/em&gt; 기능을 제공한다면 쓸데없는 부분(아무것도 없는 텍셀, 가장 끝)을 컬링할 수 있다.&lt;/p&gt;

&lt;p&gt;하지만 &lt;em&gt;Early-Z&lt;/em&gt; 하드웨어를 사용할때는 &lt;em&gt;Light-Space Z-Buffer&lt;/em&gt; 를 필요로 한다. 이는 조금 당황스러운 상황을 만든다. 그래서 이 단계에서는 반드시 스텐실 버퍼를 만들어야 한다. 위에서 언급한 쓸데없는 부분(아무것도 없는 텍셀, 가장 끝)의 &lt;em&gt;Depth&lt;/em&gt; 를 0으로 세팅한다. 나머지는 0 이상의 숫자로 세팅한다.&lt;/p&gt;

&lt;p&gt;논문의 저자에 따르면, 엄청 큰 씬을 제외하고는 30% ~ 50% 의 성능 향상을 보였다고 한다. 이는 &lt;em&gt;Conservative Rasterization&lt;/em&gt; 으로 생성된 &lt;em&gt;Fragment&lt;/em&gt; 의 절대적인 숫자를 줄이고, 아무것도 없는 리스트를 스킵하면서 길이의 다양성을 없엔 효과다.&lt;/p&gt;

&lt;p&gt;컬링을 된 이후에는 픽셀별로 &lt;em&gt;Visibility Test&lt;/em&gt; 를 해준다. 이때 각각의 폴리곤들은 임의의 텍셀 집합을 가리기 된다. 그리고 각각의 텍셀이 가진 리스트를 순회하면서 &lt;em&gt;Visibility Test&lt;/em&gt; 를 한다. 이때 &lt;em&gt;atomic OR&lt;/em&gt; 을 사용하여 &lt;em&gt;Visibility Buffer&lt;/em&gt; 에 기록한다. 여기서 가장 병목이 되는 구간은 각각의 폴리곤이 임의의 서로다른 길이를 가진 텍셀의 리스트를 커버링하여 각각의 쓰레드별로 실행되는 시간이 제각각이 된다. 문제는 시간이 제각각인 경우, 가장 느린 시간을 소모한 쓰레드를 기준으로 &lt;em&gt;divergence&lt;/em&gt; 하여 각각의 텍셀의 리스트의 길이 중 가장 긴 길이의 시간으로 맞춰진다. 이는 최악의 상황을 유발할 수 있다.&lt;/p&gt;

&lt;p&gt;마지막으로 &lt;em&gt;Visibility Buffer&lt;/em&gt; 를 사용하여 실제 오브젝트들을 렌더링하면 된다.&lt;/p&gt;

&lt;h3&gt;복잡도 계산&lt;/h3&gt;

&lt;p&gt;위에서도 언급햇다시피 이 기법은 &lt;em&gt;N&lt;/em&gt; 번의 &lt;em&gt;Visibility Test&lt;/em&gt; 한다면, 시간 복잡도는 O(&lt;em&gt;N&lt;/em&gt;) 과 같다. 여기서 &lt;em&gt;N&lt;/em&gt; 을 분해하면 다음과 같다. O(&lt;em&gt;La&lt;/em&gt; * &lt;em&gt;F&lt;/em&gt;), &lt;em&gt;F&lt;/em&gt; 는 &lt;em&gt;Light-Space&lt;/em&gt; 의 &lt;em&gt;Fragment&lt;/em&gt; 갯수이고, &lt;em&gt;La&lt;/em&gt; 는 &lt;em&gt;Linked-List&lt;/em&gt; 의 평균 길이다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;La&lt;/em&gt; 또한 다른 요소로 나타낼 수 있다. &lt;em&gt;Visibility Test&lt;/em&gt; 는 &lt;em&gt;Eye-Space&lt;/em&gt; 에서 한다. 그래서 &lt;em&gt;IZB Node&lt;/em&gt; 의 갯수는 &lt;em&gt;Eye-Space&lt;/em&gt; 의 해상도에 비례한다. 그리고 &lt;em&gt;Eye-Space&lt;/em&gt; 의 데이터는 전부 &lt;em&gt;Light-Space&lt;/em&gt; 에 기록되므로 &lt;em&gt;La&lt;/em&gt; ≈ (&lt;em&gt;Eye-Space Resolution&lt;/em&gt;) / (&lt;em&gt;Light-Space Resolution&lt;/em&gt;) 으로 계산될 수 있다.&lt;/p&gt;

&lt;p&gt;하지만 위에서 언급했던 것과 같이 &lt;em&gt;SIMT&lt;/em&gt; 기반의 GPU 에서는 O(&lt;em&gt;La&lt;/em&gt; * &lt;em&gt;F&lt;/em&gt;) 가 아닌 O(&lt;em&gt;Lm&lt;/em&gt; * &lt;em&gt;F&lt;/em&gt;) 이 될 수 밖에 없다. &lt;em&gt;Lm&lt;/em&gt; 은 &lt;em&gt;Linked-List&lt;/em&gt; 의 최대 길이다. 결국 퍼포먼스를 내기 위해선 &lt;em&gt;Lm&lt;/em&gt; 의 길이를 줄이는 것이 가장 중요하다는 것이 된다.&lt;/p&gt;

&lt;p&gt;이번 글에서는 전체 시스템과 복잡도에 대해서 알아보았다. 다음은 더 디테일한 구현 부분의 내용을과 &lt;em&gt;Alpha&lt;/em&gt; 처리에 대한 부분을 알아볼것이다.&lt;/p&gt;

&lt;h2&gt;참조&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://cwyman.org/papers/tvcg16_ftizbExtended.pdf&quot;&gt;Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:L1&quot;&gt;
      &lt;p&gt;https://ypchoi.gitbooks.io/rendering-techniques/content/z_prepass.html &lt;a href=&quot;#fnref:L1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:C1&quot;&gt;
      &lt;p&gt;Cascaded Shadow Map 의 Crop Matrix 를 떠올리면 된다. &lt;a href=&quot;#fnref:C1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="shader" />
      
        <category term="shadow" />
      
        <category term="rendering" />
      
        <category term="fts" />
      

      

      
        <summary type="html">frustum Traced Shadow with Irregular Z-Buffer 0 에서 기법의 아이디어를 둘러봄으러써 대강 이 알고리즘이 무엇인지 살펴보았다. 이번 글에서는 논문에 수록된 포괄적인 전체 시스템과 복잡도에 대하여 알아볼 것이다. 전체 시스템 이전 글에서 두가지 단계에 대해서 자세한 설명을 했었다. Irregular Z-Buffer 를 생성하고 Visibility Test 를 하는 것이였다. 실제 구현된 단계는 총 6개의 단계로 이루어진다고 한다. 첫번째로는 Eye-Space Z-Prepass 를 해준다. 요즘의 엔진들이나 큰 규모가 아닌 게임이더라도 Z-Prepass1 는 거의 대부분 해준다. Geometry Pass 가 두번 걸리기는 하지만 Fill Rate 의 부하가 Geometry Pass 의 부하보다 많이 커서 그런 듯하다. 중요한건 단순히 언급한 Eye-Space Z-Prepass 를 뜻하는게 아니다. 이전에 언급한 μQuad 의 빠른 계산을 위해 G-Buffer 에 3개의 실수 값들을 넣는다. 이 3개의 실수는 실제 그려지는 카메라의 위치와 Tangent Plane 의 4개의 코너중에 3개의 거리를 나타낸다. 이는 μQuad 를 다시 계산하기에 충분하다고 한다. 이 방법은 Visibility Test 의 속도를 빠르게 하는데 도움이 되지만, 당연히 G-Buffer 의 공간이 부족한 경우에는 쓰지 못한다. AAA 급의 게임들은 G-Buffer 를 bit 단위로 최대한 아껴쓰기 경우가 많기 때문에 이와 같은 상황이 일어날 수도 있다. 이런 경우에는 명시적으로 Visibility Test 를 할때 μQuad 를 계산한다고 한다. 아래 그림은 2009년에 발매된 KillZone 2 의 G-Buffer 사용을 나타내는 PT의 한 부분이다. 출처 : Deferred Rendering in Killzone 2 두번째로는 씬의 경계를 설정해주는 것이다. Shadow Mapping 에서 Light-Space Projection 행렬은 씬에 딱 맞게 해주어야 한다.2 딱 맞지 않는 경우에는 IZB 에 쓸모없는 텍셀을 생기게 하기 때문이다. 그래서 Light-Space Projection 행렬을 계산하기 위해 Compute Shader 와 Z-Buffer 를 사용하여 Bounding Box 를 계산한다. 이 계산은 화면의 해상도에 따라서 달라진다. 하지만 논문의 저자는 이 비용이 병목의 큰 원인이 아니기 때문에 특별한 해결 방법을 제시하지는 않는다. 세번째는 Irregular Z-Buffer 를 만드는 것이다. 이전 글에서 IZB 에 대한 대략적인 아이디어는 언급했었다. 더 디테일하게 이를 말해보면, 우선 Eye-Space Z Buffer 를 참조해 Light-Space 로 변환한 후에 Light-Space A-Buffer 의 텍셀의 Linked-List 에 넣는다. IZB 의 발상은 Eye-Space 의 픽셀과 Light-Space 의 텍셀이 1:1 로 매칭되지 않고 하나의 텍셀이 참조당하는 횟수가 1을 넘을때 allasing 이 발생하는 것에서 시작되었다. 그래서 여기서 구현된 IZB 는 텍셀에 Linked-List 의 개념을 도입하여 보다 정확히 계산할 수 있게 하였다. 아래 그림은 IZB 의 데이터를 나타낸다. 출처 : Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows 왼쪽의 그림은 일반적인 Shadow Map 을 나타내고, 중간의 그림은 Linked-List 의 크기를 나타낸다. 흰색은 Linked-List 의 크기가 0인 텍셀을 나타내고, 검은색에 가까워질수록 Linked-List 의 크기가 점점 커지는 것을 나타낸다. 오른쪽의 그림은 필요없는 부분을 0으로 나타내고, 나머지 부분은 0 이상의 숫자를 나타내는 방식이다. 이는 아래에서 언급할 Light-Space Culling Prepass 에서 쓰인다. 픽셀별로 여러개의 가시성(가려진 정도)를 나타내는 픽셀은 여러개의 IZB Node 를 필요로 한다. 일반적인 Shadow Map 과는 다르게 μQuad 는 다른 Light-Space Texel 에 Projection 이 가능하다. 대충 생각해보면, N 개의 픽셀별 쉐도우가 필요하면, N 개의 IZB Node 가 필요하다. 하지만 쓸데없는 데이터를(Geometry 가 없는 경우) 넣지 않을 수 있으므로 M 개의 Light-Space Texel 에 μQuad 를 투영한다면, 우리는 min(N, M)개의 IZB Node 가 필요하다고 알 수 있다. 아래 그림은 IZB 를 사용하는 디테일한 구조를 나타낸다. 출처 : Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows IZB 의 데이터 구조는 Light-Space A-Buffer : Eye-Space A-Buffer 의 Node 를 찾아가기 위한 데이터, Eye-Space A-Buffer : 리스트의 크기를 나타내는 버퍼와 같다. 출력되는 결과 : Visibility Buffer 또한 따로 존재한다. 이게 최종적으로 그려질때 사용된다. 다음은 Light-Space Culling Prepass 다. Visibility Test 를 위해 Geometry 의 Light-Space Conservative Rasterization 을 하게 되는데, GPU 에서 Early-Z 기능을 제공한다면 쓸데없는 부분(아무것도 없는 텍셀, 가장 끝)을 컬링할 수 있다. 하지만 Early-Z 하드웨어를 사용할때는 Light-Space Z-Buffer 를 필요로 한다. 이는 조금 당황스러운 상황을 만든다. 그래서 이 단계에서는 반드시 스텐실 버퍼를 만들어야 한다. 위에서 언급한 쓸데없는 부분(아무것도 없는 텍셀, 가장 끝)의 Depth 를 0으로 세팅한다. 나머지는 0 이상의 숫자로 세팅한다. 논문의 저자에 따르면, 엄청 큰 씬을 제외하고는 30% ~ 50% 의 성능 향상을 보였다고 한다. 이는 Conservative Rasterization 으로 생성된 Fragment 의 절대적인 숫자를 줄이고, 아무것도 없는 리스트를 스킵하면서 길이의 다양성을 없엔 효과다. 컬링을 된 이후에는 픽셀별로 Visibility Test 를 해준다. 이때 각각의 폴리곤들은 임의의 텍셀 집합을 가리기 된다. 그리고 각각의 텍셀이 가진 리스트를 순회하면서 Visibility Test 를 한다. 이때 atomic OR 을 사용하여 Visibility Buffer 에 기록한다. 여기서 가장 병목이 되는 구간은 각각의 폴리곤이 임의의 서로다른 길이를 가진 텍셀의 리스트를 커버링하여 각각의 쓰레드별로 실행되는 시간이 제각각이 된다. 문제는 시간이 제각각인 경우, 가장 느린 시간을 소모한 쓰레드를 기준으로 divergence 하여 각각의 텍셀의 리스트의 길이 중 가장 긴 길이의 시간으로 맞춰진다. 이는 최악의 상황을 유발할 수 있다. 마지막으로 Visibility Buffer 를 사용하여 실제 오브젝트들을 렌더링하면 된다. 복잡도 계산 위에서도 언급햇다시피 이 기법은 N 번의 Visibility Test 한다면, 시간 복잡도는 O(N) 과 같다. 여기서 N 을 분해하면 다음과 같다. O(La * F), F 는 Light-Space 의 Fragment 갯수이고, La 는 Linked-List 의 평균 길이다. La 또한 다른 요소로 나타낼 수 있다. Visibility Test 는 Eye-Space 에서 한다. 그래서 IZB Node 의 갯수는 Eye-Space 의 해상도에 비례한다. 그리고 Eye-Space 의 데이터는 전부 Light-Space 에 기록되므로 La ≈ (Eye-Space Resolution) / (Light-Space Resolution) 으로 계산될 수 있다. 하지만 위에서 언급했던 것과 같이 SIMT 기반의 GPU 에서는 O(La * F) 가 아닌 O(Lm * F) 이 될 수 밖에 없다. Lm 은 Linked-List 의 최대 길이다. 결국 퍼포먼스를 내기 위해선 Lm 의 길이를 줄이는 것이 가장 중요하다는 것이 된다. 이번 글에서는 전체 시스템과 복잡도에 대해서 알아보았다. 다음은 더 디테일한 구현 부분의 내용을과 Alpha 처리에 대한 부분을 알아볼것이다. 참조 Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows https://ypchoi.gitbooks.io/rendering-techniques/content/z_prepass.html &amp;#8617; Cascaded Shadow Map 의 Crop Matrix 를 떠올리면 된다. &amp;#8617;</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Frustum Traced Shadow With Irrelgular Z Buffer 0</title>
      
      <link href="https://hrmrzizon.github.io/2018/01/13/frustum-traced-shadow-with-irrelgular-z-buffer-0/" rel="alternate" type="text/html" title="Frustum Traced Shadow With Irrelgular Z Buffer 0" />
      <published>2018-01-13T00:00:00+00:00</published>
      <updated>2018-01-13T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2018/01/13/frustum-traced-shadow-with-irrelgular-z-buffer-0</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2018/01/13/frustum-traced-shadow-with-irrelgular-z-buffer-0/">&lt;p&gt;&lt;a href=&quot;/2017/12/27/percentage-closer-soft-shadows/&quot;&gt;Percentage-Closer Filtering Shadows&lt;/a&gt; 에서 &lt;em&gt;PCF&lt;/em&gt; 를 응용한 &lt;em&gt;PCSS&lt;/em&gt; 라는 &lt;em&gt;Soft Shadow&lt;/em&gt; 를 나타내는 기법에 대해서 알아보았다. 이번 글에서는 여태까지 알아본 것들에 비해 굉장히 최근에 나온 기법인 &lt;em&gt;frustum-Traced Shadow&lt;/em&gt; 에 대해서 알아볼것이다.&lt;/p&gt;

&lt;p&gt;해당 기법은 2015년에 &lt;em&gt;Siggraph&lt;/em&gt;, &lt;em&gt;Interactive 3D&lt;/em&gt; 같은 컨퍼런스에서 발표되었으며, 현재 &lt;em&gt;Tom Clansy’s the Division&lt;/em&gt; 에 &lt;em&gt;PCSS&lt;/em&gt; 와 혼합된 형태(&lt;em&gt;Hybrid frustum Traced Shadow&lt;/em&gt;)로 적용되어 있다. &lt;em&gt;Frame Rate&lt;/em&gt; 에 조금 영향을 미쳐 대부분의 게이머들은 아직은 &lt;em&gt;HFTS&lt;/em&gt; 를 사용하지 않는듯 하다.(&lt;a href=&quot;https://www.reddit.com/r/nvidia/comments/49idz3/nvidia_hfts_the_division/&quot;&gt;Reddit : Nvidia HFTS (The Division)&lt;/a&gt;) 하지만 컴퓨팅 파워가 늘어나는 것을 가정한다면 앞으로 하이엔드 게임의 주 옵션이 될수도 있겠다.&lt;/p&gt;

&lt;p&gt;이 기법의 저자는 &lt;em&gt;Shadow Map&lt;/em&gt; 처럼 따로 붙은 기법없이 &lt;em&gt;Aliasing&lt;/em&gt; 이 없어야 했으며, 현세대의 GPU 와 해상도를 &lt;em&gt;Interactive&lt;/em&gt; 하게 지원하는 것이 완벽한 &lt;em&gt;Hard Shadow&lt;/em&gt; 를 목표로 &lt;em&gt;FTS&lt;/em&gt; 를 고안했다. 가장 많이 쓰이는 &lt;em&gt;Shadow Map&lt;/em&gt; 기법은 공간적(&lt;em&gt;Light-Space&lt;/em&gt; 와 &lt;em&gt;Clipping-Space&lt;/em&gt; 의 &lt;em&gt;Discretize&lt;/em&gt; 된 결과의 차이), 일시적인(필터링이 필요한 &lt;em&gt;Aliasing&lt;/em&gt;)인 문제들이 산재한다. 이는 이 기법을 고안한 시발점이였다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;FTS&lt;/em&gt; 의 이론적인 뿌리를 정하기 위해 저자는 여태까지 존재하는 여러 기법을 언급한다. 빛을 하나의 직선단위로 시뮬레이팅 하는 &lt;em&gt;Ray-Tracing&lt;/em&gt;, 볼륨을 통한 각각의 폴리곤들을 테스트 하는 &lt;em&gt;Shadow Volume&lt;/em&gt;, &lt;em&gt;Irregular Z-Buffers&lt;/em&gt; 를 언급했다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Shadow Volume&lt;/em&gt; 은 3차원상으로 &lt;em&gt;Shadow&lt;/em&gt; 가 생기는 부분을 정해 그 부분을 테스트해서 &lt;em&gt;Shadow&lt;/em&gt; 를 정해주는 기법이다. 이는 &lt;em&gt;Shadow Map&lt;/em&gt; 보다 픽셀 단위로 처리할 수 있지만, 여러 단점이 있다고 한다. 한번에 해결되는 깔끔한 방법이 없으며, 보이지 않는 부분도 처리하기 때문에 &lt;em&gt;Fill-Rate&lt;/em&gt; 를 많이 소모한다. 게다가 처리 자체가 간단하지 않기 때문에 개발자들도 많이 쓰는 기법이 아니라고 한다. 필자도 &lt;em&gt;Shadow Map&lt;/em&gt; 에 대한 자료는 굉장히 많이 봤지만 &lt;em&gt;Shadow Volume&lt;/em&gt; 은 거의 본적이 없다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/NVidia_ShadowVolume.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://developer.download.nvidia.com/books/HTML/gpugems/gpugems_ch09.html&quot;&gt;GPU Gems : Efficient Shadow Volume Rendering&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Ray-Tracing&lt;/em&gt; 은 빛을 직선 단위로 시뮬레이팅을 하는 기법으로, 계산 비용 자체가 비싸기 때문에 하드웨어와 구조에 굉장히 의존적이라고 한다. 게임에서도 쓰일 수 있는 기법이 있었지만 다른 후보에 밀려났다. 바로 &lt;em&gt;Irregular Z-Buffer&lt;/em&gt; 다. 현대 GPU 의 &lt;em&gt;Geometry&lt;/em&gt; -&amp;gt; &lt;em&gt;Rasterize&lt;/em&gt; 구조에 맞춰 가장 걸맞는 방법이라고 한다. 자세한 설명은 아래에서 보자.&lt;/p&gt;

&lt;h3&gt;Key Idea&lt;/h3&gt;

&lt;p&gt;이 기법의 중요한 아이디어는 앞에서 소개한 &lt;em&gt;Irregular Z-Buffer&lt;/em&gt; 와 &lt;em&gt;frustum-Triangle Test&lt;/em&gt; 이 두가지다. &lt;em&gt;Irregular Z-Buffer&lt;/em&gt; 는 앞서 &lt;em&gt;Shadow Map&lt;/em&gt; 의 단점중에 공간적 괴리를 해결하는 데이터 구조이고, &lt;em&gt;frustum-Triangle Test&lt;/em&gt; 는 논문에서 한 말을 이용하면 &lt;em&gt;Sub-Pixel Accurate Pixel&lt;/em&gt; 을 구성하기 위한 시뮬레이션 테스트다. 이 두가지를 간단하게 살펴보자.&lt;/p&gt;

&lt;p&gt;첫번째로는 바로 위에서 언급했던 &lt;em&gt;Irregular Z-Buffer&lt;/em&gt; 다. 여기서의 &lt;em&gt;IZB&lt;/em&gt; 는 우리가 알던 일반적인 &lt;em&gt;Buffer&lt;/em&gt; 의 쓰임새와는 조금 다르게 쓰인다. 이 기법에서의 &lt;em&gt;IZB&lt;/em&gt; 는 일반적인 &lt;em&gt;Shadow Map&lt;/em&gt; 에서의 &lt;em&gt;Eye-Space&lt;/em&gt; 와 &lt;em&gt;Light-Space&lt;/em&gt; 의 괴리를 없에기 위해 &lt;em&gt;Light-Space&lt;/em&gt; 를 기준으로 &lt;em&gt;Depth&lt;/em&gt; 를 쭉 저장하는게 아닌, &lt;em&gt;Eye-Space&lt;/em&gt; 의 각각 픽셀별로 표현하는 물체에 영향을 미치는 광원을 방향으로 &lt;em&gt;Ray&lt;/em&gt; 를 쏜다. 그리고 &lt;em&gt;Light-Space&lt;/em&gt; 를 기준으로 만든 &lt;em&gt;Grid&lt;/em&gt; 버퍼에 &lt;em&gt;Ray&lt;/em&gt; 가 부딫치고, 부딫친 부분에서 가장 가까운 텍셀에 데이터를 저장한다. 위에서 설명한 &lt;em&gt;IZB&lt;/em&gt; 를 구성하는 방법에 대한 그림이 아래에 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/fts_IZB.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://cwyman.org/papers/tvcg16_ftizbExtended.pdf&quot;&gt;Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;간단하게 이런식으로 &lt;em&gt;IZB&lt;/em&gt; 가 구성되는 것을 알 수 있다. 이제 &lt;em&gt;Geometry&lt;/em&gt; 와 비교하는 &lt;em&gt;Visibility Test&lt;/em&gt; 가 필요하다. 일반적인 &lt;em&gt;Shadow Mapping&lt;/em&gt; 의 &lt;em&gt;Visibility Test&lt;/em&gt; 와는 조금 다르다. 기존의 &lt;em&gt;Shadow Mapping&lt;/em&gt; 은 정점을 &lt;em&gt;Light-Space&lt;/em&gt; 로 바꾸어 &lt;em&gt;Z&lt;/em&gt; 값을 비교하여 &lt;em&gt;Visibility Test&lt;/em&gt; 를 한다. 하지만 이 기법에서의 &lt;em&gt;Visibility Test&lt;/em&gt; 는 다르다. 위에서 언급한 것과 같이 &lt;em&gt;IZB&lt;/em&gt; 를 만든다. 그 다음 &lt;em&gt;Occlluder Geometry&lt;/em&gt; 들을 &lt;em&gt;Light-Space&lt;/em&gt; 를 기준으로 &lt;em&gt;Conservative Rasterization&lt;/em&gt; 을 해준다.&lt;sup id=&quot;fnref:C1&quot;&gt;&lt;a href=&quot;#fn:C1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; 그렇게 나온 결과를 통해 &lt;em&gt;IZB&lt;/em&gt; 와 함께 &lt;em&gt;Visibility Test&lt;/em&gt; 를 한다. &lt;em&gt;Conservative Rasterization&lt;/em&gt; 의 결과는 거의 &lt;em&gt;Flag&lt;/em&gt; 로 사용될것으로 예측되고, &lt;em&gt;Eye-Space&lt;/em&gt; 픽셀의 그림자 계산은 복잡한 계산을 통해 구한다. 아래는 논문에 있던 &lt;em&gt;IZB&lt;/em&gt; 를 기준으로 쓰여진 수도 코드다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;// Step 1: Identify pixel locations we need to shadow
G(x, y) ← RenderGBufferFromEye()

// Step 2: Add pixels to our light-space IZB data structure
for pixel p ∈ G(x, y) do
    lsTexelp ← ShadowMapXform[ GetEyeSpacePos( p ) ]
    izbNodep ← CreateIZBNode[ p ]
    AddNodeToLightSpaceList[ lsTexelp, izbNodep ]
end for

// Step 3: Test each triangle with pixels in lists it covers
for tri t ∈ SceneTriangles do
    for frag f ∈ ConservateLightSpaceRaster( t ) do
        lsTexelf ← FragmentLocationInRasterGrid[ f ]
        for node n ∈ IZBNodeList( lsTexelf ) do
            p ← GetEyeSpacePixel( n )
            visMask[p] = visMask[p] | TestVisibility[ p, t ]
        end for
    end for
end for
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;center&gt;출처 : &lt;a href=&quot;http://cwyman.org/papers/tvcg16_ftizbExtended.pdf&quot;&gt;Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;다음은 &lt;em&gt;Visibility Test&lt;/em&gt; 다. 논문에서는 &lt;em&gt;frustum-Triangle Test&lt;/em&gt; 라고 부르는데, 이는 조금 복잡한 과정으로 구성된다. 아래 그림을 보면서 알아보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/fts_VisibilityTest.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://cwyman.org/papers/tvcg16_ftizbExtended.pdf&quot;&gt;Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;가장 처음에는 &lt;em&gt;μQuad&lt;/em&gt; 라는 것을 생성한다. &lt;em&gt;μQuad&lt;/em&gt; 는 &lt;em&gt;Tangent-Space&lt;/em&gt; 를 기준으로 설정하며 각 픽셀별로 생성한다. 위 그림에서는 중간 그림에 구 위에있는 평면을 뜻한다. 그 다음 가리는 &lt;em&gt;Geometry&lt;/em&gt; 의 폴리곤들의 각각의 &lt;em&gt;Edge&lt;/em&gt; 를 사용하여 &lt;em&gt;Shadow Plane&lt;/em&gt; 을 생성한다. 마지막으로 만들어진 &lt;em&gt;Shadow Plane&lt;/em&gt; 을 &lt;em&gt;μQuad&lt;/em&gt; 에 &lt;em&gt;Projection&lt;/em&gt; 한다. 이때 가지고 있던 &lt;em&gt;LUT&lt;/em&gt; 를 통해 가려짐을 계산한다. 그리고 다른 &lt;em&gt;Edge&lt;/em&gt; 들도 계속해서 누적시킨다.&lt;/p&gt;

&lt;p&gt;간단하게 &lt;em&gt;frustum-Triangle Test&lt;/em&gt; 의 단계에 대해 설명해보았다. 이제 각각의 과정 : &lt;em&gt;μQuad Construction&lt;/em&gt;, &lt;em&gt;Shadow Plane Construction&lt;/em&gt;, &lt;em&gt;Visibility Computation&lt;/em&gt; 에 대해 조금 더 자세히 써보겠다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;μQuad&lt;/em&gt; 의 생성은 &lt;em&gt;Geometry&lt;/em&gt; 의 &lt;em&gt;Tangent-Space&lt;/em&gt; 를 기준으로 계산되는 것 빼고는 특이한 점이 없다. 하지만 생성되는 시기에 대해선 조금 특별한게 있다. 가시성을 계산할 때 생성할 수도 있지만 &lt;em&gt;G-Buffer&lt;/em&gt; 를 생성할 때 미리 계산하는 것이 더 효율적이라고 한다.&lt;/p&gt;

&lt;p&gt;가시성을 계산할 때 &lt;em&gt;ray-triangle intersection&lt;/em&gt; 을 계산하기 보다는 앖에서 언급한 각각의 폴리곤의 &lt;em&gt;Shadow Volume&lt;/em&gt; 을 각 점마다 계산한다고 한다. &lt;em&gt;Shadow Volume&lt;/em&gt; 은 그림자를 생성하는 &lt;em&gt;Occluder Triangle&lt;/em&gt; 을 기준으로 각각의 &lt;em&gt;Edge&lt;/em&gt; 에서 뻗어나오는 직사각형 면으로 구성된다. 아래 그림을 보면 쉽게 이해할 수 있다. 그리고 &lt;em&gt;μQuad&lt;/em&gt; 에서 샘플링한 각각의 점들을 기준으로 가시성을 계산한다면 한다면, 4번의 내적으로 가시성을 테스트할 수 있다는 것을 의미한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/HFTS_frustumTracing.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://developer.download.nvidia.com/gameworks/events/  GDC2016/jstory_hfts.pdf&quot;&gt;NVidia : Advanced Geometrically Correct Shadows for Modern Game Engines&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;또한 그림자를 멀티샘플링 하기 위해서는 여기서 언급한 &lt;em&gt;Shadow Volume&lt;/em&gt; 의 &lt;em&gt;Shadow Plane&lt;/em&gt; 들을 이용한다고 한다.&lt;/p&gt;

&lt;p&gt;마지막으로 &lt;em&gt;Visibility Computation&lt;/em&gt; 이 남아있다. 이 부분의 대략적인 것은 위에서 언급했다. 자세한 계산방식을 말해보겠다. 위에서 언급한 &lt;em&gt;μQuad&lt;/em&gt; 와 &lt;em&gt;Shadow Plane&lt;/em&gt; 사용해서 해당 폴리곤들의 데이터 누적을 위해서는 &lt;em&gt;μQuad&lt;/em&gt; 에서 이산화된 &lt;em&gt;binary&lt;/em&gt; 샘플링이 필요하다. 논문의 저자는 32 번의 &lt;em&gt;Halton sampling&lt;/em&gt;&lt;sup id=&quot;fnref:C2&quot;&gt;&lt;a href=&quot;#fn:C2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; 을 사용했다고 한다.&lt;/p&gt;

&lt;p&gt;가시성을 계산하기 위해서는 &lt;em&gt;Shadow Plane&lt;/em&gt; 을 &lt;em&gt;μQuad&lt;/em&gt; 에 &lt;em&gt;Projection&lt;/em&gt; 해줘야 한다. 그러면 &lt;em&gt;μQuad&lt;/em&gt; 는 최대 3개의 &lt;em&gt;line&lt;/em&gt; 을 얻게 된다. 논문에서는 이 &lt;em&gt;line&lt;/em&gt; 을 &lt;em&gt;μQuad&lt;/em&gt; 를 기준으로 극좌표계&lt;sup id=&quot;fnref:C3&quot;&gt;&lt;a href=&quot;#fn:C3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; 데이터로 저장한다고 한다. 반지름과 각도가 5bit 크기로 저장된다. 해당 10bit 데이터를 사용하여 미리 계산된 테이블에서 32개의 이진 가시성 샘플을 가져온다. 결과와  bit 단위의 and 연산을 통해 &lt;em&gt;μQuad&lt;/em&gt; 의 가시성을 계산할 수 있다고 한다.&lt;/p&gt;

&lt;p&gt;해당 기법을 고안한 사람은 두가지의 아이디어 : &lt;em&gt;Irregular Z-Buffer&lt;/em&gt; 와 &lt;em&gt;frustum-Triangle Test&lt;/em&gt; 를 통해  &lt;em&gt;Sub-pixel Hard Shadow&lt;/em&gt; 의 이론을 만들었다. 하지만 이 아이디어들과 구현을 위한 노력의 차이는 꽤 큰듯하다. 논문을 보면 아이디어에 대한 텍스트보다 최적화를 위한 텍스트가 2배가 될정도로 많다. 다음 글에서는 논문에서 나온 전체 과정과 디테일한 구현 사항에 대해 적어보겠다.&lt;/p&gt;

&lt;h2&gt;참조&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://cwyman.org/papers/tvcg16_ftizbExtended.pdf&quot;&gt;Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Irregular_Z-buffer&quot;&gt;Wikipedia : Irregular Z-Buffer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://cwyman.org/videos/sig1657-chris-wyman-magic-behind-gameworks-hybrid-frustum-traced-shadows-hfts.mp4&quot;&gt;cywman.org : HFTS Presentation Video&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://developer.nvidia.com/content/dont-be-conservative-conservative-rasterization&quot;&gt;NVidia : Don’t be conservative with Conservative Rasterization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://developer.download.nvidia.com/gameworks/events/GDC2016/jstory_hfts.pdf&quot;&gt;NVidia : Advanced Geometrically Correct Shadows for Modern Game Engines&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:C1&quot;&gt;
      &lt;p&gt;일반적으로 오브젝트를 그리는 것과 다른 &lt;em&gt;Conservative Rasterization&lt;/em&gt; 을 해주는 이유는 일반적인 &lt;em&gt;Rasterization&lt;/em&gt; 은 픽셀의 반이상을 차지해야 해당 픽셀을 처리해준다. 하지만 정확한 &lt;em&gt;Visibility&lt;/em&gt; 를 계산하기 위해서는 폴리곤이 해당되는 모든 픽셀들을 처리해주어야 한다. &lt;em&gt;Conservative Rasterization&lt;/em&gt; 은 앞에서 말한바와 같이 모든 부분을 픽셀로 처리한다. &lt;em&gt;Conservative Rasterization&lt;/em&gt; 에 대한 자세한 정보는 &lt;a href=&quot;https://developer.nvidia.com/content/dont-be-conservative-conservative-rasterization&quot;&gt;NVidia : Don’t be conservative with Conservative Rasterization&lt;/a&gt; 에서 확인할 수 있다. &lt;a href=&quot;#fnref:C1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:C2&quot;&gt;
      &lt;p&gt;몬테카를로 시뮬레이션과 같은 방식의 점을 생성하는 방식이다. 쉽게 말하면 랜덤하게 생성하는 것이라고 생각하면 된다. 자세한 정보는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Halton_sequence&quot;&gt;Wikipedia : Halton sequence&lt;/a&gt; 에서 확인할 수 있다. &lt;a href=&quot;#fnref:C2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:C3&quot;&gt;
      &lt;p&gt;여기서는 각도와 반지름(거리)를 사용하여 나나탠다. 극좌표계에 대한 자세한 정보는 &lt;a href=&quot;https://ko.wikipedia.org/wiki/%EA%B7%B9%EC%A2%8C%ED%91%9C%EA%B3%84&quot;&gt;위키피디아 : 극좌표계&lt;/a&gt; 에서 확인할 수 있다. &lt;a href=&quot;#fnref:C3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="shader" />
      
        <category term="shadow" />
      
        <category term="rendering" />
      
        <category term="fts" />
      

      

      
        <summary type="html">Percentage-Closer Filtering Shadows 에서 PCF 를 응용한 PCSS 라는 Soft Shadow 를 나타내는 기법에 대해서 알아보았다. 이번 글에서는 여태까지 알아본 것들에 비해 굉장히 최근에 나온 기법인 frustum-Traced Shadow 에 대해서 알아볼것이다. 해당 기법은 2015년에 Siggraph, Interactive 3D 같은 컨퍼런스에서 발표되었으며, 현재 Tom Clansy’s the Division 에 PCSS 와 혼합된 형태(Hybrid frustum Traced Shadow)로 적용되어 있다. Frame Rate 에 조금 영향을 미쳐 대부분의 게이머들은 아직은 HFTS 를 사용하지 않는듯 하다.(Reddit : Nvidia HFTS (The Division)) 하지만 컴퓨팅 파워가 늘어나는 것을 가정한다면 앞으로 하이엔드 게임의 주 옵션이 될수도 있겠다. 이 기법의 저자는 Shadow Map 처럼 따로 붙은 기법없이 Aliasing 이 없어야 했으며, 현세대의 GPU 와 해상도를 Interactive 하게 지원하는 것이 완벽한 Hard Shadow 를 목표로 FTS 를 고안했다. 가장 많이 쓰이는 Shadow Map 기법은 공간적(Light-Space 와 Clipping-Space 의 Discretize 된 결과의 차이), 일시적인(필터링이 필요한 Aliasing)인 문제들이 산재한다. 이는 이 기법을 고안한 시발점이였다. FTS 의 이론적인 뿌리를 정하기 위해 저자는 여태까지 존재하는 여러 기법을 언급한다. 빛을 하나의 직선단위로 시뮬레이팅 하는 Ray-Tracing, 볼륨을 통한 각각의 폴리곤들을 테스트 하는 Shadow Volume, Irregular Z-Buffers 를 언급했다. Shadow Volume 은 3차원상으로 Shadow 가 생기는 부분을 정해 그 부분을 테스트해서 Shadow 를 정해주는 기법이다. 이는 Shadow Map 보다 픽셀 단위로 처리할 수 있지만, 여러 단점이 있다고 한다. 한번에 해결되는 깔끔한 방법이 없으며, 보이지 않는 부분도 처리하기 때문에 Fill-Rate 를 많이 소모한다. 게다가 처리 자체가 간단하지 않기 때문에 개발자들도 많이 쓰는 기법이 아니라고 한다. 필자도 Shadow Map 에 대한 자료는 굉장히 많이 봤지만 Shadow Volume 은 거의 본적이 없다. 출처 : GPU Gems : Efficient Shadow Volume Rendering Ray-Tracing 은 빛을 직선 단위로 시뮬레이팅을 하는 기법으로, 계산 비용 자체가 비싸기 때문에 하드웨어와 구조에 굉장히 의존적이라고 한다. 게임에서도 쓰일 수 있는 기법이 있었지만 다른 후보에 밀려났다. 바로 Irregular Z-Buffer 다. 현대 GPU 의 Geometry -&amp;gt; Rasterize 구조에 맞춰 가장 걸맞는 방법이라고 한다. 자세한 설명은 아래에서 보자. Key Idea 이 기법의 중요한 아이디어는 앞에서 소개한 Irregular Z-Buffer 와 frustum-Triangle Test 이 두가지다. Irregular Z-Buffer 는 앞서 Shadow Map 의 단점중에 공간적 괴리를 해결하는 데이터 구조이고, frustum-Triangle Test 는 논문에서 한 말을 이용하면 Sub-Pixel Accurate Pixel 을 구성하기 위한 시뮬레이션 테스트다. 이 두가지를 간단하게 살펴보자. 첫번째로는 바로 위에서 언급했던 Irregular Z-Buffer 다. 여기서의 IZB 는 우리가 알던 일반적인 Buffer 의 쓰임새와는 조금 다르게 쓰인다. 이 기법에서의 IZB 는 일반적인 Shadow Map 에서의 Eye-Space 와 Light-Space 의 괴리를 없에기 위해 Light-Space 를 기준으로 Depth 를 쭉 저장하는게 아닌, Eye-Space 의 각각 픽셀별로 표현하는 물체에 영향을 미치는 광원을 방향으로 Ray 를 쏜다. 그리고 Light-Space 를 기준으로 만든 Grid 버퍼에 Ray 가 부딫치고, 부딫친 부분에서 가장 가까운 텍셀에 데이터를 저장한다. 위에서 설명한 IZB 를 구성하는 방법에 대한 그림이 아래에 있다. 출처 : Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows 간단하게 이런식으로 IZB 가 구성되는 것을 알 수 있다. 이제 Geometry 와 비교하는 Visibility Test 가 필요하다. 일반적인 Shadow Mapping 의 Visibility Test 와는 조금 다르다. 기존의 Shadow Mapping 은 정점을 Light-Space 로 바꾸어 Z 값을 비교하여 Visibility Test 를 한다. 하지만 이 기법에서의 Visibility Test 는 다르다. 위에서 언급한 것과 같이 IZB 를 만든다. 그 다음 Occlluder Geometry 들을 Light-Space 를 기준으로 Conservative Rasterization 을 해준다.1 그렇게 나온 결과를 통해 IZB 와 함께 Visibility Test 를 한다. Conservative Rasterization 의 결과는 거의 Flag 로 사용될것으로 예측되고, Eye-Space 픽셀의 그림자 계산은 복잡한 계산을 통해 구한다. 아래는 논문에 있던 IZB 를 기준으로 쓰여진 수도 코드다. // Step 1: Identify pixel locations we need to shadow G(x, y) ← RenderGBufferFromEye() // Step 2: Add pixels to our light-space IZB data structure for pixel p ∈ G(x, y) do lsTexelp ← ShadowMapXform[ GetEyeSpacePos( p ) ] izbNodep ← CreateIZBNode[ p ] AddNodeToLightSpaceList[ lsTexelp, izbNodep ] end for // Step 3: Test each triangle with pixels in lists it covers for tri t ∈ SceneTriangles do for frag f ∈ ConservateLightSpaceRaster( t ) do lsTexelf ← FragmentLocationInRasterGrid[ f ] for node n ∈ IZBNodeList( lsTexelf ) do p ← GetEyeSpacePixel( n ) visMask[p] = visMask[p] | TestVisibility[ p, t ] end for end for end for 출처 : Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows 다음은 Visibility Test 다. 논문에서는 frustum-Triangle Test 라고 부르는데, 이는 조금 복잡한 과정으로 구성된다. 아래 그림을 보면서 알아보자. 출처 : Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows 가장 처음에는 μQuad 라는 것을 생성한다. μQuad 는 Tangent-Space 를 기준으로 설정하며 각 픽셀별로 생성한다. 위 그림에서는 중간 그림에 구 위에있는 평면을 뜻한다. 그 다음 가리는 Geometry 의 폴리곤들의 각각의 Edge 를 사용하여 Shadow Plane 을 생성한다. 마지막으로 만들어진 Shadow Plane 을 μQuad 에 Projection 한다. 이때 가지고 있던 LUT 를 통해 가려짐을 계산한다. 그리고 다른 Edge 들도 계속해서 누적시킨다. 간단하게 frustum-Triangle Test 의 단계에 대해 설명해보았다. 이제 각각의 과정 : μQuad Construction, Shadow Plane Construction, Visibility Computation 에 대해 조금 더 자세히 써보겠다. μQuad 의 생성은 Geometry 의 Tangent-Space 를 기준으로 계산되는 것 빼고는 특이한 점이 없다. 하지만 생성되는 시기에 대해선 조금 특별한게 있다. 가시성을 계산할 때 생성할 수도 있지만 G-Buffer 를 생성할 때 미리 계산하는 것이 더 효율적이라고 한다. 가시성을 계산할 때 ray-triangle intersection 을 계산하기 보다는 앖에서 언급한 각각의 폴리곤의 Shadow Volume 을 각 점마다 계산한다고 한다. Shadow Volume 은 그림자를 생성하는 Occluder Triangle 을 기준으로 각각의 Edge 에서 뻗어나오는 직사각형 면으로 구성된다. 아래 그림을 보면 쉽게 이해할 수 있다. 그리고 μQuad 에서 샘플링한 각각의 점들을 기준으로 가시성을 계산한다면 한다면, 4번의 내적으로 가시성을 테스트할 수 있다는 것을 의미한다. 출처 : NVidia : Advanced Geometrically Correct Shadows for Modern Game Engines 또한 그림자를 멀티샘플링 하기 위해서는 여기서 언급한 Shadow Volume 의 Shadow Plane 들을 이용한다고 한다. 마지막으로 Visibility Computation 이 남아있다. 이 부분의 대략적인 것은 위에서 언급했다. 자세한 계산방식을 말해보겠다. 위에서 언급한 μQuad 와 Shadow Plane 사용해서 해당 폴리곤들의 데이터 누적을 위해서는 μQuad 에서 이산화된 binary 샘플링이 필요하다. 논문의 저자는 32 번의 Halton sampling2 을 사용했다고 한다. 가시성을 계산하기 위해서는 Shadow Plane 을 μQuad 에 Projection 해줘야 한다. 그러면 μQuad 는 최대 3개의 line 을 얻게 된다. 논문에서는 이 line 을 μQuad 를 기준으로 극좌표계3 데이터로 저장한다고 한다. 반지름과 각도가 5bit 크기로 저장된다. 해당 10bit 데이터를 사용하여 미리 계산된 테이블에서 32개의 이진 가시성 샘플을 가져온다. 결과와 bit 단위의 and 연산을 통해 μQuad 의 가시성을 계산할 수 있다고 한다. 해당 기법을 고안한 사람은 두가지의 아이디어 : Irregular Z-Buffer 와 frustum-Triangle Test 를 통해 Sub-pixel Hard Shadow 의 이론을 만들었다. 하지만 이 아이디어들과 구현을 위한 노력의 차이는 꽤 큰듯하다. 논문을 보면 아이디어에 대한 텍스트보다 최적화를 위한 텍스트가 2배가 될정도로 많다. 다음 글에서는 논문에서 나온 전체 과정과 디테일한 구현 사항에 대해 적어보겠다. 참조 Frustum-Traced Irregular Z-Buffers: Fast, Sub-pixel Accurate Hard Shadows Wikipedia : Irregular Z-Buffer cywman.org : HFTS Presentation Video NVidia : Don’t be conservative with Conservative Rasterization NVidia : Advanced Geometrically Correct Shadows for Modern Game Engines 일반적으로 오브젝트를 그리는 것과 다른 Conservative Rasterization 을 해주는 이유는 일반적인 Rasterization 은 픽셀의 반이상을 차지해야 해당 픽셀을 처리해준다. 하지만 정확한 Visibility 를 계산하기 위해서는 폴리곤이 해당되는 모든 픽셀들을 처리해주어야 한다. Conservative Rasterization 은 앞에서 말한바와 같이 모든 부분을 픽셀로 처리한다. Conservative Rasterization 에 대한 자세한 정보는 NVidia : Don’t be conservative with Conservative Rasterization 에서 확인할 수 있다. &amp;#8617; 몬테카를로 시뮬레이션과 같은 방식의 점을 생성하는 방식이다. 쉽게 말하면 랜덤하게 생성하는 것이라고 생각하면 된다. 자세한 정보는 Wikipedia : Halton sequence 에서 확인할 수 있다. &amp;#8617; 여기서는 각도와 반지름(거리)를 사용하여 나나탠다. 극좌표계에 대한 자세한 정보는 위키피디아 : 극좌표계 에서 확인할 수 있다. &amp;#8617;</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Unity Scripted Importer</title>
      
      <link href="https://hrmrzizon.github.io/2018/01/11/unity-scripted-importer/" rel="alternate" type="text/html" title="Unity Scripted Importer" />
      <published>2018-01-11T00:00:00+00:00</published>
      <updated>2018-01-11T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2018/01/11/unity-scripted-importer</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2018/01/11/unity-scripted-importer/">&lt;p&gt;Unity 의 에디터 시스템은 꽤나 유연하다. 이번 글에서는 &lt;em&gt;AssetImporter&lt;/em&gt; 에 대한 기능들에 대하여 알아볼 것이다.&lt;/p&gt;

&lt;p&gt;Unity 의 각각의 Asset 들은 확장자의 이름에 따라서 &lt;em&gt;AssetImporter&lt;/em&gt; 가 하나씩 만들어지고, 해당 &lt;em&gt;AssetImporter&lt;/em&gt; 에 따라서 &lt;em&gt;Post-Process&lt;/em&gt; 가 진행된다. 필자가 많이 봤던 &lt;em&gt;AssetImporter&lt;/em&gt; 는 &lt;a href=&quot;https://docs.unity3d.com/ScriptReference/TextureImporter.html&quot;&gt;&lt;em&gt;TextureImporter&lt;/em&gt;&lt;/a&gt; 와 &lt;a href=&quot;https://docs.unity3d.com/ScriptReference/ModelImporter.html&quot;&gt;&lt;em&gt;ModelImporter&lt;/em&gt;&lt;/a&gt; 가 있었다. 확인할 당시에는 당연히 &lt;em&gt;AssetImporter&lt;/em&gt; 를 커스터마이징 할 수 있겠다는 생각이 들어 찾아봤지만 전혀 없었다.(Unity 5 버젼을 사용할 때다.) 이렇게 시스템을 만들어 놓고 왜 없냐는 의문이 들었지만 이는 정말 할수있는게 없었기에 넘어갔었다.&lt;/p&gt;

&lt;p&gt;그런데 이번에 Unity 2018.1 베타가 릴리즈 되면서 SRP 를 살펴보던 도중 &lt;em&gt;Expremental&lt;/em&gt; 기능들 중에 &lt;a href=&quot;https://docs.unity3d.com/2018.1/Documentation/Manual/ScriptedImporters.html&quot;&gt;&lt;em&gt;ScriptedImporter&lt;/em&gt;&lt;/a&gt; 라는 기능이 있는 것을 발견했다.&lt;/p&gt;

&lt;p&gt;이 기능은 말그대로 예전의 내가 원하던 기능이였다. &lt;em&gt;AssetImporter&lt;/em&gt; 클래스와 다른점은 딱 한가지다. 가상 &lt;em&gt;OnImportAsset&lt;/em&gt; 메소드가 존재하는 것이다. 즉 &lt;em&gt;ScriptedImporter&lt;/em&gt; 를 상속하여 &lt;em&gt;OnImportAsset&lt;/em&gt; 를 구현하면 간단하게 에셋을 &lt;em&gt;Customize&lt;/em&gt; 할 수 있는 것이다. 또한 에디터 기능을 지원하는 &lt;a href=&quot;https://docs.unity3d.com/2018.1/Documentation/ScriptReference/Experimental.AssetImporters.ScriptedImporterEditor.html&quot;&gt;ScriptedImporterEditor &lt;/a&gt; 라는 클래스를 사용하면 에디터를 손쉽게 바꿀 수 있다. 자세한 사항은 글에서 언급된 링크를 통해 보면 된다.&lt;/p&gt;

&lt;h2&gt;참조&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.unity3d.com/2018.1/Documentation/Manual/ScriptedImporters.html&quot;&gt;Unity Documentation : ScriptedImporter &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.unity3d.com/2018.1/Documentation/ScriptReference/Experimental.AssetImporters.ScriptedImporter.html&quot;&gt;Unity Reference : ScriptedImporter&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.unity3d.com/2018.1/Documentation/ScriptReference/Experimental.AssetImporters.ScriptedImporterEditor.html&quot;&gt;Unity Reference : ScriptedImporterEditor &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="unity" />
      

      

      
        <summary type="html">Unity 의 에디터 시스템은 꽤나 유연하다. 이번 글에서는 AssetImporter 에 대한 기능들에 대하여 알아볼 것이다. Unity 의 각각의 Asset 들은 확장자의 이름에 따라서 AssetImporter 가 하나씩 만들어지고, 해당 AssetImporter 에 따라서 Post-Process 가 진행된다. 필자가 많이 봤던 AssetImporter 는 TextureImporter 와 ModelImporter 가 있었다. 확인할 당시에는 당연히 AssetImporter 를 커스터마이징 할 수 있겠다는 생각이 들어 찾아봤지만 전혀 없었다.(Unity 5 버젼을 사용할 때다.) 이렇게 시스템을 만들어 놓고 왜 없냐는 의문이 들었지만 이는 정말 할수있는게 없었기에 넘어갔었다. 그런데 이번에 Unity 2018.1 베타가 릴리즈 되면서 SRP 를 살펴보던 도중 Expremental 기능들 중에 ScriptedImporter 라는 기능이 있는 것을 발견했다. 이 기능은 말그대로 예전의 내가 원하던 기능이였다. AssetImporter 클래스와 다른점은 딱 한가지다. 가상 OnImportAsset 메소드가 존재하는 것이다. 즉 ScriptedImporter 를 상속하여 OnImportAsset 를 구현하면 간단하게 에셋을 Customize 할 수 있는 것이다. 또한 에디터 기능을 지원하는 ScriptedImporterEditor 라는 클래스를 사용하면 에디터를 손쉽게 바꿀 수 있다. 자세한 사항은 글에서 언급된 링크를 통해 보면 된다. 참조 Unity Documentation : ScriptedImporter Unity Reference : ScriptedImporter Unity Reference : ScriptedImporterEditor</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Percentage Closer Soft Shadows</title>
      
      <link href="https://hrmrzizon.github.io/2017/12/27/percentage-closer-soft-shadows/" rel="alternate" type="text/html" title="Percentage Closer Soft Shadows" />
      <published>2017-12-27T00:00:00+00:00</published>
      <updated>2017-12-27T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2017/12/27/percentage-closer-soft-shadows</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2017/12/27/percentage-closer-soft-shadows/">&lt;p&gt;&lt;a href=&quot;/2017/12/19/shadow-map-filtering/&quot;&gt;Shadow Map Filtering&lt;/a&gt; 에서 &lt;em&gt;PCF&lt;/em&gt; 와 &lt;em&gt;VSM&lt;/em&gt; 에 대하여 간단히 알아보았다. 이번 글에서 설명할 것은 &lt;em&gt;PCF&lt;/em&gt; 를 활용한 &lt;em&gt;PCSS&lt;/em&gt; 다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;PCSS&lt;/em&gt; 는 &lt;em&gt;Soft Shadow&lt;/em&gt; 를 구현하는 기법 중 하나로써 2005년에 발표되어 여태까지도 꽤나 알려진 기법이다. 우선 &lt;em&gt;Soft Shadow&lt;/em&gt; 가 무엇인지 알아보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/soft_vs_hardshadow.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://www.assignmentpoint.com/science/computer/real-time-soft-shadow-rendering.html&quot;&gt;assignmentpoint.com : Real Time Soft Shadow Rendering&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위와 같이 빛을 가린 물체와 거리가 멀어지면 멀어질수록 밝아지는 그림자를 &lt;em&gt;Soft Shadow&lt;/em&gt; 라고 한다. 완전한 &lt;em&gt;Hard Shadow&lt;/em&gt; 는 어색하기 때문에 보통 &lt;em&gt;PCF&lt;/em&gt; 를 사용하여 끝부분을 부드럽게 처리했으나, 태양광 처럼 길게 그림자를 만드는 경우가 있으면 끝 부분이 가면 갈수록 부드러워져야 한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/tree_shadow.jpg&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://www.youtube.com/watch?v=Ax8G8P3tA28&quot;&gt;Youtube
&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;태양 빛에의해 만들어진 나무의 그림자다. 짧은 길이의 그림자는 적당히 &lt;em&gt;PCF&lt;/em&gt; 로 대략 표현이 가능하나 이런 길은 그림자를 고정된 사이즈의 &lt;em&gt;PCF&lt;/em&gt; 로 표현하기엔 무리가 있다. 그래서 나온것이 &lt;em&gt;PCSS&lt;/em&gt; 다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;PCSS&lt;/em&gt; 를 보기전에, 우리가 알아야할 용어들이 있다. 바로 &lt;em&gt;Umbra&lt;/em&gt; 와 &lt;em&gt;Penumbra&lt;/em&gt; 다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/umbra_penumbra_antumbra.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://en.wikipedia.org/wiki/Umbra,_penumbra_and_antumbra&quot;&gt;Wikipedia : Umbra, penumbra and antumbra &lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Soft Shadow&lt;/em&gt; 가 표현하는 부드러운 부분의 그림자는 위 그림에서도 보이듯이 &lt;em&gt;Penumbra&lt;/em&gt; 라고 한다. &lt;em&gt;PCSS&lt;/em&gt; 에서는 부드러운 부분의 그림자를 &lt;em&gt;Penumbra&lt;/em&gt; 라고 한다. &lt;em&gt;PCSS&lt;/em&gt; 에서는 &lt;em&gt;Penumbra&lt;/em&gt; 의 크기를 사용하여 &lt;em&gt;PCF&lt;/em&gt; 의 샘플링 범위를 정해준다. 우선 &lt;em&gt;PCSS&lt;/em&gt; 의 &lt;em&gt;Penumbra&lt;/em&gt; 를 계산하는 방법을 보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/PCSS_PenumbraSizeEstimation.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://http.download.nvidia.com/developer/presentations/2005/SIGGRAPH/Percentage_Closer_Soft_Shadows.pdf&quot;&gt;Siggraph 2005 : Percentage-Closer Soft Shadows&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;맨위의 노란색으로 표시된 부분은 광원을 뜻하며, 일정한 범위로 빛을 비추는 &lt;em&gt;Area Light&lt;/em&gt; 로 가정한 후 계산한다. W - light 는 &lt;em&gt;Area Light&lt;/em&gt; 의 범위를 뜻한다. 중간에 있는 &lt;em&gt;Blocker&lt;/em&gt; 는 빛을 가리는 물체를 뜻하며, d blocker 는 가리는 물체와 빛과의 거리, d receiver 는 그림자가 비추는 물체와 광원 사이의 거리를 뜻한다. 그림자를 받는 부분과 빛을 가리는 물체와 광원을 서로 평행하다고 가정해서 계산한다. 2차원의 그림을 3차원으로 바꿔보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/PCSS_Scheme.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://developer.download.nvidia.com/shaderlibrary/docs/shadow_PCSS.pdf&quot;&gt;NVidia : Percentage-Closer Soft Shadows&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;보통 &lt;em&gt;Pixel Shader&lt;/em&gt; 에서 그림자의 비춘 정도를 계산하기 때문에 &lt;em&gt;Receiver&lt;/em&gt; 의 작은 부분을 기준으로 그림이 그려져 있다. 작은 부분을 기준으로 &lt;em&gt;Area Light&lt;/em&gt; 와의 &lt;em&gt;frustum&lt;/em&gt; 과 &lt;em&gt;Blocker&lt;/em&gt; 가 얼마나 충돌되는지 체크한다. 우리는 &lt;em&gt;Shadow Map&lt;/em&gt; 을 사용하기 때문에 아래 그림이 조금 더 실제 계산과 비슷하다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/PCSS_Scheme2.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://developer.download.nvidia.com/shaderlibrary/docs/shadow_PCSS.pdf&quot;&gt;NVidia : Percentage-Closer Soft Shadows&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;그래서 &lt;em&gt;Shadow Map&lt;/em&gt; 의 빨간색으로 하이라이트 된 부분을 샘플링해 얼마나 가리고 있는지를 확인한다. 그러면 빛을 받는 정도를 알 수 있게 되는것이다. 해당 부분을 적당히 샘플링한 다음 평균을 구해서 &lt;em&gt;PCF&lt;/em&gt; 로 샘플링하는 범위를 계산한다. 계산하여 &lt;em&gt;PCF&lt;/em&gt; 에서 범위를 사용해 계산한다. 코드를 보면서 이해해보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;PCSS_Shadow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;uv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;float2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dz_duv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zEye&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;// ------------------------&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;// STEP 1: blocker search&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;// ------------------------&lt;/span&gt;
	&lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accumBlockerDepth&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numBlockers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;float2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;searchRegionRadiusUV&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SearchRegionRadiusUV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zEye&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;FindBlocker&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accumBlockerDepth&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numBlockers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;g_shadowMap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;uv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dz_duv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;searchRegionRadiusUV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

	&lt;span class=&quot;c1&quot;&gt;// Early out if not in the penumbra&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numBlockers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numBlockers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BLOCKER_SEARCH_COUNT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

	&lt;span class=&quot;c1&quot;&gt;// ------------------------&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;// STEP 2: penumbra size&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;// ------------------------&lt;/span&gt;
	&lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;avgBlockerDepth&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accumBlockerDepth&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numBlockers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
	&lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;avgBlockerDepthWorld&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ZClipToZEye&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;avgBlockerDepth&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;float2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;penumbraRadiusUV&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PenumbraRadiusUV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zEye&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;avgBlockerDepthWorld&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;float2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filterRadiusUV&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ProjectToLightUV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;penumbraRadiusUV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;zEye&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

	&lt;span class=&quot;c1&quot;&gt;// ------------------------&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;// STEP 3: filtering&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;// ------------------------&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PCF_Filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dz_duv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;filterRadiusUV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://github.com/NVIDIAGameWorks/D3DSamples&quot;&gt;Github NVIDIAGameWorks : D3DSamples&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;해당 픽셀이 어두워지는 정도를 반환하는 &lt;em&gt;PCSS&lt;/em&gt; 계산 함수다. 코드의 주석에서는 계산을 세단계로 나눈다. 첫번째로는 &lt;em&gt;Shadow Map&lt;/em&gt; 을 샘플링해서 얼마나 빛이 얼마나 가려지는지 계산한다. 이를 &lt;em&gt;STEP 1: blocker search&lt;/em&gt; 라고 표기해놓았고, 두번째는 &lt;em&gt;PCF&lt;/em&gt; 에서 샘플링할 범위를 결정하는 넓이를 계산한다. 이를 &lt;em&gt;STEP 2: penumbra size&lt;/em&gt; 라고 한다. 세번째로는 &lt;em&gt;PCF&lt;/em&gt; 를 계산해서 가려지는 정도를 반환한다. 자세한 코드는 출처에서 &lt;em&gt;SoftShadows&lt;/em&gt; 항목을 들어가면 볼 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;PCSS&lt;/em&gt; 의 장점은 아무래도 확실한 &lt;em&gt;Soft Shadow&lt;/em&gt; 를 구현했다는 점이다. 비록 대략적으로 가정한 부분이 많지만 장면별로 잘 맞춰주기만 한다면 괜찮은 결과가 나올것 같다. 하지만 샘플링 횟수가 꽤나 된다. &lt;em&gt;PCF&lt;/em&gt; 만 하더라도 가볍지는 않은 편인데, &lt;em&gt;Blocker&lt;/em&gt; 를 계산하느라 더 많이 샘플링을 한다. 하지만 잘 만들어진 게임과 요즘의 GPU 에서는 아주 큰 오버헤드는 없는걸로 보인다. (&lt;a href=&quot;https://www.reddit.com/r/nvidia/comments/49idz3/nvidia_hfts_the_division/&quot;&gt;Redit : Nvidia HFTS (The Division)&lt;/a&gt;)&lt;/p&gt;

&lt;h2&gt;참조&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://http.download.nvidia.com/developer/presentations/2005/SIGGRAPH/Percentage_Closer_Soft_Shadows.pdf&quot;&gt;Siggraph 2005 : Percentage-Closer Soft Shadows&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://developer.download.nvidia.com/shaderlibrary/docs/shadow_PCSS.pdf&quot;&gt;NVidia : Percentage-Closer Soft Shadows&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/NVIDIAGameWorks/D3DSamples&quot;&gt;Github NVidiaGameWorks : D3DSample&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="shader" />
      
        <category term="shadow" />
      
        <category term="rendering" />
      
        <category term="pcss" />
      

      

      
        <summary type="html">Shadow Map Filtering 에서 PCF 와 VSM 에 대하여 간단히 알아보았다. 이번 글에서 설명할 것은 PCF 를 활용한 PCSS 다. PCSS 는 Soft Shadow 를 구현하는 기법 중 하나로써 2005년에 발표되어 여태까지도 꽤나 알려진 기법이다. 우선 Soft Shadow 가 무엇인지 알아보자. 출처 : assignmentpoint.com : Real Time Soft Shadow Rendering 위와 같이 빛을 가린 물체와 거리가 멀어지면 멀어질수록 밝아지는 그림자를 Soft Shadow 라고 한다. 완전한 Hard Shadow 는 어색하기 때문에 보통 PCF 를 사용하여 끝부분을 부드럽게 처리했으나, 태양광 처럼 길게 그림자를 만드는 경우가 있으면 끝 부분이 가면 갈수록 부드러워져야 한다. 출처 : Youtube 태양 빛에의해 만들어진 나무의 그림자다. 짧은 길이의 그림자는 적당히 PCF 로 대략 표현이 가능하나 이런 길은 그림자를 고정된 사이즈의 PCF 로 표현하기엔 무리가 있다. 그래서 나온것이 PCSS 다. PCSS 를 보기전에, 우리가 알아야할 용어들이 있다. 바로 Umbra 와 Penumbra 다. 출처 : Wikipedia : Umbra, penumbra and antumbra Soft Shadow 가 표현하는 부드러운 부분의 그림자는 위 그림에서도 보이듯이 Penumbra 라고 한다. PCSS 에서는 부드러운 부분의 그림자를 Penumbra 라고 한다. PCSS 에서는 Penumbra 의 크기를 사용하여 PCF 의 샘플링 범위를 정해준다. 우선 PCSS 의 Penumbra 를 계산하는 방법을 보자. 출처 : Siggraph 2005 : Percentage-Closer Soft Shadows 맨위의 노란색으로 표시된 부분은 광원을 뜻하며, 일정한 범위로 빛을 비추는 Area Light 로 가정한 후 계산한다. W - light 는 Area Light 의 범위를 뜻한다. 중간에 있는 Blocker 는 빛을 가리는 물체를 뜻하며, d blocker 는 가리는 물체와 빛과의 거리, d receiver 는 그림자가 비추는 물체와 광원 사이의 거리를 뜻한다. 그림자를 받는 부분과 빛을 가리는 물체와 광원을 서로 평행하다고 가정해서 계산한다. 2차원의 그림을 3차원으로 바꿔보자. 출처 : NVidia : Percentage-Closer Soft Shadows 보통 Pixel Shader 에서 그림자의 비춘 정도를 계산하기 때문에 Receiver 의 작은 부분을 기준으로 그림이 그려져 있다. 작은 부분을 기준으로 Area Light 와의 frustum 과 Blocker 가 얼마나 충돌되는지 체크한다. 우리는 Shadow Map 을 사용하기 때문에 아래 그림이 조금 더 실제 계산과 비슷하다. 출처 : NVidia : Percentage-Closer Soft Shadows 그래서 Shadow Map 의 빨간색으로 하이라이트 된 부분을 샘플링해 얼마나 가리고 있는지를 확인한다. 그러면 빛을 받는 정도를 알 수 있게 되는것이다. 해당 부분을 적당히 샘플링한 다음 평균을 구해서 PCF 로 샘플링하는 범위를 계산한다. 계산하여 PCF 에서 범위를 사용해 계산한다. 코드를 보면서 이해해보자. float PCSS_Shadow(float2 uv, float z, float2 dz_duv, float zEye) { // ------------------------ // STEP 1: blocker search // ------------------------ float accumBlockerDepth = 0; float numBlockers = 0; float2 searchRegionRadiusUV = SearchRegionRadiusUV(zEye); FindBlocker(accumBlockerDepth, numBlockers, g_shadowMap, uv, z, dz_duv, searchRegionRadiusUV); // Early out if not in the penumbra if (numBlockers == 0) return 1.0; else if (numBlockers == BLOCKER_SEARCH_COUNT) return 0.0; // ------------------------ // STEP 2: penumbra size // ------------------------ float avgBlockerDepth = accumBlockerDepth / numBlockers; float avgBlockerDepthWorld = ZClipToZEye(avgBlockerDepth); float2 penumbraRadiusUV = PenumbraRadiusUV(zEye, avgBlockerDepthWorld); float2 filterRadiusUV = ProjectToLightUV(penumbraRadiusUV, zEye); // ------------------------ // STEP 3: filtering // ------------------------ return PCF_Filter(uv, z, dz_duv, filterRadiusUV); } 출처 : Github NVIDIAGameWorks : D3DSamples 해당 픽셀이 어두워지는 정도를 반환하는 PCSS 계산 함수다. 코드의 주석에서는 계산을 세단계로 나눈다. 첫번째로는 Shadow Map 을 샘플링해서 얼마나 빛이 얼마나 가려지는지 계산한다. 이를 STEP 1: blocker search 라고 표기해놓았고, 두번째는 PCF 에서 샘플링할 범위를 결정하는 넓이를 계산한다. 이를 STEP 2: penumbra size 라고 한다. 세번째로는 PCF 를 계산해서 가려지는 정도를 반환한다. 자세한 코드는 출처에서 SoftShadows 항목을 들어가면 볼 수 있다. PCSS 의 장점은 아무래도 확실한 Soft Shadow 를 구현했다는 점이다. 비록 대략적으로 가정한 부분이 많지만 장면별로 잘 맞춰주기만 한다면 괜찮은 결과가 나올것 같다. 하지만 샘플링 횟수가 꽤나 된다. PCF 만 하더라도 가볍지는 않은 편인데, Blocker 를 계산하느라 더 많이 샘플링을 한다. 하지만 잘 만들어진 게임과 요즘의 GPU 에서는 아주 큰 오버헤드는 없는걸로 보인다. (Redit : Nvidia HFTS (The Division)) 참조 Siggraph 2005 : Percentage-Closer Soft Shadows NVidia : Percentage-Closer Soft Shadows Github NVidiaGameWorks : D3DSample</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Shadow Map Filtering</title>
      
      <link href="https://hrmrzizon.github.io/2017/12/19/shadow-map-filtering/" rel="alternate" type="text/html" title="Shadow Map Filtering" />
      <published>2017-12-19T00:00:00+00:00</published>
      <updated>2017-12-19T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2017/12/19/shadow-map-filtering</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2017/12/19/shadow-map-filtering/">&lt;p&gt;&lt;a href=&quot;/2017/11/30/what-is-shadow-mapping/&quot;&gt;What is Shadow Mapping&lt;/a&gt; 에서 &lt;em&gt;Shadow Mapping&lt;/em&gt; 에 대한 간단한 번역 &amp;amp; 설명을 적어놓았다. 해당 글에서 &lt;em&gt;PCF&lt;/em&gt; 를 잠깐 언급했었다. 이 글에서는 &lt;em&gt;PCF&lt;/em&gt; 를 포함해서 &lt;em&gt;Shadow Map&lt;/em&gt; 을 필터링하는 방법에 대해서 알아보겠다.&lt;/p&gt;

&lt;p&gt;첫번째는 &lt;em&gt;PCF&lt;/em&gt; 다. 풀어쓰면 &lt;em&gt;Percentage Closer Filtering&lt;/em&gt; 이라는 단어가 되며, &lt;em&gt;Shadow Map&lt;/em&gt; 을 여러번 샘플링해 &lt;em&gt;Percentage&lt;/em&gt; 를 소숫점으로 나타내서 &lt;em&gt;Shadow&lt;/em&gt; 가 생긴 정도를 나타내주는 &lt;em&gt;Filtering&lt;/em&gt; 기법이다. 쉽게 이해할 수 있도록 아래 그림을 보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/PCF_Scheme.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://graphics.pixar.com/library/ShadowMaps/paper.pdf&quot;&gt;Pixar : Rendering Antialiased Shadows with Depth Maps&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위의 a) 는 아무것도 필터링 하지 않을 때의 &lt;em&gt;ShadowMap&lt;/em&gt; 샘플링하는 것을 보여주고, 아래 b) 는 &lt;em&gt;PCF&lt;/em&gt; 를 사용해 샘플링하는 것을 보여준다. 위 그림에서 &lt;em&gt;Surface at z = 49.8&lt;/em&gt; 은 그림자를 처리할 표면의 &lt;em&gt;Depth&lt;/em&gt; 또는 &lt;em&gt;Z&lt;/em&gt; 을 뜻한다. 그리고 &lt;em&gt;Light-Space&lt;/em&gt; 를 기준으로 해당 값보다 &lt;em&gt;Depth&lt;/em&gt; 값이 멀다고 판단될시에는 처리하지 않고, 가깝다고 판단될 때는 처리하는 걸로 해준다. &lt;em&gt;Shadow Map&lt;/em&gt; 에서 한 부분만 샘플링해서 하는 것이 윗 부분의 그림이고, 한 부분이 아닌 근처의 여러 부분을 샘플링해서 값을 구하는 것이 &lt;em&gt;PCF&lt;/em&gt; 다.&lt;/p&gt;

&lt;p&gt;아래에 &lt;em&gt;PCF&lt;/em&gt; 를 사용하는 코드가 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-HLSL&quot;&gt;float PCF_FILTER( float2 tex, float fragDepth )
{
    //PreShader - This should all be optimized away by the compiler
    //====================================
    float fStartOffset = BoxFilterStart( fFilterWidth );
    float texOffset = 1.0f / fTextureWidth;
    //====================================

    fragDepth -= 0.0001f;
    tex -= fStartOffset * texOffset;

    float lit = 0.0f;
		for( int i = 0; i &amp;lt; fFilterWidth; ++i )
			for( int j = 0; j &amp;lt; fFilterWidth; ++j )
			{
				lit += texShadowMap.SampleCmpLevelZero(
                                FILT_PCF,
                                float2( tex.x + i * texOffset, tex.y + j * texOffset ),
                                fragDepth
                              );
			}
	return lit / ( fFilterWidth * fFilterWidth );
}
&lt;/code&gt;&lt;/pre&gt;

&lt;center&gt;출처 : &lt;a href=&quot;http://developer.download.nvidia.com/SDK/10/direct3d/screenshots/samples/VarianceShadowMapping.html&quot;&gt;NVidia : Variance Shadow Mapping Website&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;자세한 코드는 위의 출처에서 코드를 구해서 보면 될듯하다. &lt;em&gt;Texture2D::SampleCmpLevelZero&lt;/em&gt; 는 &lt;em&gt;MipMap&lt;/em&gt; 참조 레벨은 0으로 한채 텍스쳐의 값을 샘플링하여 주어진 인자와 비교하여 &lt;em&gt;Sampler&lt;/em&gt; 에 정해준 방식에 적합하면 1, 적합하지 않으면 0을 반환해준다.&lt;/p&gt;

&lt;p&gt;해당 그림에서는 평균을 구하는 방법을 표기해놓았으나 다른 &lt;em&gt;NDF&lt;/em&gt; 를 써서 구현할 수도 있다.(&lt;em&gt;Gaussian Distribution&lt;/em&gt;) 또한 규칙적으로 샘플링하는게 아닌 &lt;em&gt;jitter&lt;/em&gt; 를 사용해서 샘플링할 수도 있다고 한다. 일반적으로 &lt;em&gt;Poisson disk Distribution&lt;/em&gt; 을 사용한다고 한다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;PCF&lt;/em&gt; 의 단순한 방법으로 &lt;em&gt;Shadow Map&lt;/em&gt; 을 &lt;em&gt;AntiAliasing&lt;/em&gt; 할 수 있다. 하지만 &lt;em&gt;Shadow Map&lt;/em&gt; 의 샘플링 횟수가 &lt;em&gt;PCF Kernel&lt;/em&gt;(3x3, 5x5..) 이 커지면 커질수록 많아지기 때문에 꽤나 큰 &lt;em&gt;PCF Kernel&lt;/em&gt; 에서는 샘플링 부하가 걸릴 수 있다. 성능상 단점이 크나 &lt;em&gt;PCF&lt;/em&gt; 는 굉장히 많이 사용되는 기법 중에 하나라고 한다.&lt;/p&gt;

&lt;p&gt;다음은 &lt;em&gt;Variance Shadow Map&lt;/em&gt; 이다. 이는 &lt;a href=&quot;https://en.wikipedia.org/wiki/Chebyshev%27s_inequality&quot;&gt;&lt;em&gt;Chebyshev’s Inequality&lt;/em&gt;&lt;/a&gt; 라는 통계학의 개념을 사용해 &lt;em&gt;Filtering&lt;/em&gt; 해준다. 먼저 &lt;em&gt;Shadow Map&lt;/em&gt; 을 저장할 때 &lt;em&gt;Depth&lt;/em&gt; 만 저장하는게 아닌 &lt;em&gt;Depth&lt;/em&gt; 의 제곱값 또한 같이 저장한다. &lt;em&gt;Filtering&lt;/em&gt; 에 쓰일 공식을 위해 같이 넣는다.&lt;/p&gt;

&lt;p&gt;다음은 아래 코드와 같이 공식을 계산해준다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-HLSL&quot;&gt;float VSM_FILTER( float2 tex, float fragDepth )
{
    float lit = (float)0.0f;
    float2 moments = texShadowMap.Sample( FILT_LINEAR,    float3( tex, 0.0f ) );

    float E_x2 = moments.y;
    float Ex_2 = moments.x * moments.x;
    float variance = E_x2 - Ex_2;    
    float mD = (moments.x - fragDepth );
    float mD_2 = mD * mD;
    float p = variance / (variance + mD_2 );
    lit = max( p, fragDepth &amp;lt;= moments.x );

    return lit;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;center&gt;출처 : &lt;a href=&quot;http://developer.download.nvidia.com/SDK/10/direct3d/screenshots/samples/VarianceShadowMapping.html&quot;&gt;NVidia : Variance Shadow Mapping Website&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;눈여겨 볼것은 샘플러를 &lt;em&gt;Linear&lt;/em&gt; 하게 설정해놓는 것이다.&lt;/p&gt;

&lt;p&gt;하지만 &lt;em&gt;VSM&lt;/em&gt; 은 큰 단점이 하나 있다. 바로 &lt;em&gt;Light Leaking&lt;/em&gt; 이 일어나는 것이다. 이는 &lt;a href=&quot;https://http.download.nvidia.com/developer/presentations/2006/gdc/2006-GDC-Variance-Shadow-Maps.pdf&quot;&gt;GDC 2006 : Variance Shadow Map&lt;/a&gt; 에서 참조할 수 있다. 이를 해결 하는 근본적인 방법은 없다고 한다.&lt;/p&gt;

&lt;p&gt;두가지 기법의 차이는 텍스쳐 샘플링을 더 많이 하느냐, 메모리를 2배로 늘려주느냐의 차이에 있다. 속도를 따지면 &lt;em&gt;VSM&lt;/em&gt; 이 빠르다고 한다. 하지만 굳이 퍼포먼스를 낼 필요가 없다면 &lt;em&gt;PCF&lt;/em&gt; 를 사용하는 것도 나쁜 선택은 아닐것 같다. 선택에 대한 궁금증은 &lt;a href=&quot;https://www.opengl.org/discussion_boards/showthread.php/177219-Shadow-filtering-PCF-better-than-VSM&quot;&gt;OpenGL Forum : Shadow filtering: PCF better than VSM? &lt;/a&gt; 글을 참조하길 바란다.&lt;/p&gt;

&lt;h2&gt;참조&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://graphics.pixar.com/library/ShadowMaps/paper.pdf&quot;&gt;Pixar : Rendering Antialiased Shadows with Depth Maps&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://msdn.microsoft.com/ko-kr/library/windows/desktop/bb509697.aspx&quot;&gt;MSDN : SampleCmpLevelZero&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.punkuser.net/vsm/&quot;&gt;Variance Shadow Maps&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://http.download.nvidia.com/developer/presentations/2006/gdc/2006-GDC-Variance-Shadow-Maps.pdf&quot;&gt;GDC 2006 : Variance Shadow Maps&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Chebyshev%27s_inequality&quot;&gt;Wikipedia : Chebyshev’s inequality&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://developer.nvidia.com/gpugems/GPUGems/gpugems_ch11.html&quot;&gt;GPU Gems : Shadow Map Antialiasing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://developer.download.nvidia.com/SDK/10/direct3d/screenshots/samples/VarianceShadowMapping.html&quot;&gt;NVidia : Variance Shadow Mapping Website&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/TheRealMJP/Shadows&quot;&gt;Github : TheRealMJP - Shadows&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.opengl.org/discussion_boards/showthread.php/177219-Shadow-filtering-PCF-better-than-VSM&quot;&gt;OpenGL Forum : Shadow filtering: PCF better than VSM? &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="shader" />
      
        <category term="shadow" />
      
        <category term="rendering" />
      
        <category term="vsm" />
      

      

      
        <summary type="html">What is Shadow Mapping 에서 Shadow Mapping 에 대한 간단한 번역 &amp;amp; 설명을 적어놓았다. 해당 글에서 PCF 를 잠깐 언급했었다. 이 글에서는 PCF 를 포함해서 Shadow Map 을 필터링하는 방법에 대해서 알아보겠다. 첫번째는 PCF 다. 풀어쓰면 Percentage Closer Filtering 이라는 단어가 되며, Shadow Map 을 여러번 샘플링해 Percentage 를 소숫점으로 나타내서 Shadow 가 생긴 정도를 나타내주는 Filtering 기법이다. 쉽게 이해할 수 있도록 아래 그림을 보자. 출처 : Pixar : Rendering Antialiased Shadows with Depth Maps 위의 a) 는 아무것도 필터링 하지 않을 때의 ShadowMap 샘플링하는 것을 보여주고, 아래 b) 는 PCF 를 사용해 샘플링하는 것을 보여준다. 위 그림에서 Surface at z = 49.8 은 그림자를 처리할 표면의 Depth 또는 Z 을 뜻한다. 그리고 Light-Space 를 기준으로 해당 값보다 Depth 값이 멀다고 판단될시에는 처리하지 않고, 가깝다고 판단될 때는 처리하는 걸로 해준다. Shadow Map 에서 한 부분만 샘플링해서 하는 것이 윗 부분의 그림이고, 한 부분이 아닌 근처의 여러 부분을 샘플링해서 값을 구하는 것이 PCF 다. 아래에 PCF 를 사용하는 코드가 있다. float PCF_FILTER( float2 tex, float fragDepth ) { //PreShader - This should all be optimized away by the compiler //==================================== float fStartOffset = BoxFilterStart( fFilterWidth ); float texOffset = 1.0f / fTextureWidth; //==================================== fragDepth -= 0.0001f; tex -= fStartOffset * texOffset; float lit = 0.0f; for( int i = 0; i &amp;lt; fFilterWidth; ++i ) for( int j = 0; j &amp;lt; fFilterWidth; ++j ) { lit += texShadowMap.SampleCmpLevelZero( FILT_PCF, float2( tex.x + i * texOffset, tex.y + j * texOffset ), fragDepth ); } return lit / ( fFilterWidth * fFilterWidth ); } 출처 : NVidia : Variance Shadow Mapping Website 자세한 코드는 위의 출처에서 코드를 구해서 보면 될듯하다. Texture2D::SampleCmpLevelZero 는 MipMap 참조 레벨은 0으로 한채 텍스쳐의 값을 샘플링하여 주어진 인자와 비교하여 Sampler 에 정해준 방식에 적합하면 1, 적합하지 않으면 0을 반환해준다. 해당 그림에서는 평균을 구하는 방법을 표기해놓았으나 다른 NDF 를 써서 구현할 수도 있다.(Gaussian Distribution) 또한 규칙적으로 샘플링하는게 아닌 jitter 를 사용해서 샘플링할 수도 있다고 한다. 일반적으로 Poisson disk Distribution 을 사용한다고 한다. PCF 의 단순한 방법으로 Shadow Map 을 AntiAliasing 할 수 있다. 하지만 Shadow Map 의 샘플링 횟수가 PCF Kernel(3x3, 5x5..) 이 커지면 커질수록 많아지기 때문에 꽤나 큰 PCF Kernel 에서는 샘플링 부하가 걸릴 수 있다. 성능상 단점이 크나 PCF 는 굉장히 많이 사용되는 기법 중에 하나라고 한다. 다음은 Variance Shadow Map 이다. 이는 Chebyshev’s Inequality 라는 통계학의 개념을 사용해 Filtering 해준다. 먼저 Shadow Map 을 저장할 때 Depth 만 저장하는게 아닌 Depth 의 제곱값 또한 같이 저장한다. Filtering 에 쓰일 공식을 위해 같이 넣는다. 다음은 아래 코드와 같이 공식을 계산해준다. float VSM_FILTER( float2 tex, float fragDepth ) { float lit = (float)0.0f; float2 moments = texShadowMap.Sample( FILT_LINEAR, float3( tex, 0.0f ) ); float E_x2 = moments.y; float Ex_2 = moments.x * moments.x; float variance = E_x2 - Ex_2; float mD = (moments.x - fragDepth ); float mD_2 = mD * mD; float p = variance / (variance + mD_2 ); lit = max( p, fragDepth &amp;lt;= moments.x ); return lit; } 출처 : NVidia : Variance Shadow Mapping Website 눈여겨 볼것은 샘플러를 Linear 하게 설정해놓는 것이다. 하지만 VSM 은 큰 단점이 하나 있다. 바로 Light Leaking 이 일어나는 것이다. 이는 GDC 2006 : Variance Shadow Map 에서 참조할 수 있다. 이를 해결 하는 근본적인 방법은 없다고 한다. 두가지 기법의 차이는 텍스쳐 샘플링을 더 많이 하느냐, 메모리를 2배로 늘려주느냐의 차이에 있다. 속도를 따지면 VSM 이 빠르다고 한다. 하지만 굳이 퍼포먼스를 낼 필요가 없다면 PCF 를 사용하는 것도 나쁜 선택은 아닐것 같다. 선택에 대한 궁금증은 OpenGL Forum : Shadow filtering: PCF better than VSM? 글을 참조하길 바란다. 참조 Pixar : Rendering Antialiased Shadows with Depth Maps MSDN : SampleCmpLevelZero Variance Shadow Maps GDC 2006 : Variance Shadow Maps Wikipedia : Chebyshev’s inequality GPU Gems : Shadow Map Antialiasing NVidia : Variance Shadow Mapping Website Github : TheRealMJP - Shadows OpenGL Forum : Shadow filtering: PCF better than VSM?</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Cascaded Shadow Mapping</title>
      
      <link href="https://hrmrzizon.github.io/2017/12/17/cascaded-shadow-mapping/" rel="alternate" type="text/html" title="Cascaded Shadow Mapping" />
      <published>2017-12-17T00:00:00+00:00</published>
      <updated>2017-12-17T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2017/12/17/cascaded-shadow-mapping</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2017/12/17/cascaded-shadow-mapping/">&lt;p&gt;&lt;a href=&quot;/2017/11/30/what-is-shadow-mapping/&quot;&gt;What is Shadow Mapping&lt;/a&gt; 에서 &lt;em&gt;Shadow Mapping&lt;/em&gt; 에 대한 간단한 번역 &amp;amp; 설명을 적어놓았다. 이번 글에서는 &lt;em&gt;Shadow Mapping&lt;/em&gt; 을 효과적으로 사용하기 위한 &lt;em&gt;Cascaded Shadow Mapping&lt;/em&gt; 에 대하여 적어보겠다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Cascaded Shadow Mapping&lt;/em&gt; 을 구글 번역기에 돌려보면 &lt;em&gt;“계단식 그림자 매핑”&lt;/em&gt; 이라고 나온다. 조금 직관적이지 않은 말이지만 뜻 자체는 맞다. 간단하게 &lt;em&gt;Cascaded Shadow Mapping&lt;/em&gt; 에 대하여 말하자면 넓은 환경의 그림자를 위해 거리에(거의 &lt;em&gt;Depth&lt;/em&gt;) 따라서 여러개의 &lt;em&gt;Shadow Map&lt;/em&gt; 을 생성하는 방법이다.&lt;/p&gt;

&lt;p&gt;넓은 범위의 &lt;em&gt;Directional Light&lt;/em&gt; 가 닿는 그림자를 정확하게 표현하려면 꽤나 큰 크기의 &lt;em&gt;Shadow Map&lt;/em&gt; 을 사용해야 한다. 하지만 &lt;em&gt;Cascaded Shadow Mapping&lt;/em&gt; 을 사용한다면 여러개의 &lt;em&gt;Shadow Map&lt;/em&gt; 을 사용하여 보다 조금의 메모리를 사용하여 넓은 범위의 그림자를 표현할 수 있다.&lt;/p&gt;

&lt;h2&gt;Shadow-map generation&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Cascaded Shadow Mapping&lt;/em&gt; 을 위한 &lt;em&gt;Shadow Map&lt;/em&gt; 생성은 앞서쓴 &lt;a href=&quot;/2017/11/30/what-is-shadow-mapping/&quot;&gt;글&lt;/a&gt;에서 설명한 방법과 거의 유사하다. 앞서 여러개의 &lt;em&gt;Shadow Map&lt;/em&gt; 을 생성하여 그림자를 표현한다고 언급했었다. 여러개의 &lt;em&gt;Shaodw Map&lt;/em&gt; 을 생성하는 기준은 &lt;em&gt;View frustum&lt;/em&gt; 을 &lt;em&gt;Depth&lt;/em&gt; 를 기준으로 여러개로 쪼개어 각 쪼개진 &lt;em&gt;frustum&lt;/em&gt; 을 기준으로 &lt;em&gt;Shadow Map&lt;/em&gt; 을 그린다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;frustum&lt;/em&gt; 은 보통 &lt;em&gt;Depth&lt;/em&gt; 값을 정하거나 어떤 알고리즘을 사용하여 쪼갠다. 이는 다음 포스팅에서 언급할 예정이다. &lt;em&gt;frustum&lt;/em&gt; 을 쪼개주면 다음은 쪼개진 &lt;em&gt;Camera View frustum&lt;/em&gt; 의 각각의 8개의 꼭지점들을 &lt;em&gt;Light-Space&lt;/em&gt; 로 변환한다. 변환된 각각 꼭지점으로 2차원의 &lt;em&gt;aligned axis bounding box&lt;/em&gt; 의 위치를 구해준다. 가장 작은 X,Y 값과 가장 큰 X, Y 값을 구해주면 된다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/CSM_EffectOfCropMatrix.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://developer.download.nvidia.com/SDK/10.5/opengl/src/cascaded_shadow_maps/doc/cascaded_shadow_maps.pdf&quot;&gt;NVidia : Cascaded Shadow Mapping&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위 그림에서 XY 평면에서의 빨간색 선으로 되어있는 사각형이 언급한 &lt;em&gt;aligned axis bounding box&lt;/em&gt; 를 말한다. 이 &lt;em&gt;AABB&lt;/em&gt; 는 아래에서 특정한 행렬을 만들때 쓰인다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://developer.download.nvidia.com/SDK/10.5/opengl/src/cascaded_shadow_maps/doc/cascaded_shadow_maps.pdf&quot;&gt;NVidia : Cascaded Shadow Maps  &lt;/a&gt; 에서는 이 &lt;em&gt;Light-Space&lt;/em&gt; 로 변환하는 &lt;em&gt;MVP 변환&lt;/em&gt; 에서 &lt;em&gt;Projection&lt;/em&gt; 변환을 바꿔준다고 설명한다. 두개의 행렬이 나오는데, 하나는 직교 투영 행렬로(&lt;em&gt;orthogonal projection&lt;/em&gt;) 나눠진 &lt;em&gt;frustum&lt;/em&gt; 의 &lt;em&gt;Far&lt;/em&gt; 값과 &lt;em&gt;Near&lt;/em&gt; 값을 통해 생성해준다. 그리고 나머지 하나는 &lt;em&gt;Crop Matrix&lt;/em&gt; 라는 변환 행렬이다.&lt;/p&gt;

&lt;p&gt;위에서 구한 &lt;em&gt;Light-Space&lt;/em&gt; 의 &lt;em&gt;AABB&lt;/em&gt; 값을 통해 &lt;em&gt;Crop Matrix&lt;/em&gt; 를 계산한다. 아래 그림에서나오는 Mx, My 와 mx, my 는 각각 Maximum X,Y, Minimum X,Y 를 뜻한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/CSM_CropMatrixCalc.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://developer.download.nvidia.com/SDK/10.5/opengl/src/cascaded_shadow_maps/doc/cascaded_shadow_maps.pdf&quot;&gt;NVidia : Cascaded Shadow Mapping&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 계산된 &lt;em&gt;Crop Matrix&lt;/em&gt; 의 역할은 해당 &lt;em&gt;AABB&lt;/em&gt; 로 &lt;em&gt;Shadow Map&lt;/em&gt; 이 그려질 범위를 결정해주는 역할을 한다. 다만 범위가 아주 정확하지는 않다. 아래 그림을 보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/CSM_FarMiddleNear.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://ogldev.atspace.co.uk/www/tutorial49/tutorial49.html&quot;&gt;OGLdev : Cascaded Shadow Mapping&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위 그림과 같이 보통은 겹치는 부분이 생긴다. 사용시에는 &lt;em&gt;Depth&lt;/em&gt; 에 따라서 다르게 사용하기 때문에 크게 문제는 없다. 사용시에는 &lt;em&gt;Depth&lt;/em&gt; 값에 따라서 다른 텍스쳐를 가져오는 것과 텍스쳐를 샘플링할때 UV 값을 정점의 위치를 &lt;em&gt;Light-Space&lt;/em&gt; 로 변환해서 변환된 정점의 위치의 X,Y 좌표를 UV 값으로 사용하면 된다. 다만 각각의 &lt;em&gt;Shadow Map&lt;/em&gt; 마다 변환 행렬은 &lt;em&gt;Crop Matrix&lt;/em&gt; 때문에 다르기 때문에 따로 접근해야 한다.&lt;/p&gt;

&lt;p&gt;자세한 사용법을 알고 싶으면 &lt;a href=&quot;http://developer.download.nvidia.com/SDK/10/Samples/cascaded_shadow_maps.zip&quot;&gt;NVidia : Cascaded Shadow Map Example&lt;/a&gt;에서 소스를 받아 보면 된다.&lt;/p&gt;

&lt;h2&gt;추가&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Cascaded Shadow Map&lt;/em&gt; 을 &lt;em&gt;1 pass&lt;/em&gt; 로 그리는 방법은 간단하다. 우선 &lt;em&gt;Shadow Map&lt;/em&gt; 들을 &lt;em&gt;TextureArray&lt;/em&gt; 를 통해 저장하고, &lt;em&gt;RenderTarget&lt;/em&gt; 을 &lt;em&gt;Geometry Shader&lt;/em&gt; 에서 각각의 렌더타겟별로 지오메트리를 추가해주어 각각의 &lt;em&gt;Pixel Shader&lt;/em&gt; 를 실행시키면 된다. 자세한 코드는 &lt;a href=&quot;https://www.slideshare.net/dgtman/implements-cascaded-shadow-maps-with-using-texture-array&quot;&gt;여기&lt;/a&gt;에서 볼 수 있다.&lt;/p&gt;

&lt;h2&gt;참조&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://developer.download.nvidia.com/SDK/10.5/opengl/src/cascaded_shadow_maps/doc/cascaded_shadow_maps.pdf&quot;&gt;NVidia : Cascaded Shadow Maps  &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/ee416307.aspx&quot;&gt;MSDN : Cascaded Shadow Maps&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/TheRealMJP/Shadows&quot;&gt;Github : TheRealMJP - Shadows&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://ogldev.atspace.co.uk/www/tutorial49/tutorial49.html&quot;&gt;OGLDev : Cascaded Shadow Mapping&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.slideshare.net/dgtman/implements-cascaded-shadow-maps-with-using-texture-array&quot;&gt;Slideshare : implements Cascaded Shadow Maps with using TexturArray(한글)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="shader" />
      
        <category term="shadow" />
      
        <category term="rendering" />
      
        <category term="csm" />
      

      

      
        <summary type="html">What is Shadow Mapping 에서 Shadow Mapping 에 대한 간단한 번역 &amp;amp; 설명을 적어놓았다. 이번 글에서는 Shadow Mapping 을 효과적으로 사용하기 위한 Cascaded Shadow Mapping 에 대하여 적어보겠다. Cascaded Shadow Mapping 을 구글 번역기에 돌려보면 “계단식 그림자 매핑” 이라고 나온다. 조금 직관적이지 않은 말이지만 뜻 자체는 맞다. 간단하게 Cascaded Shadow Mapping 에 대하여 말하자면 넓은 환경의 그림자를 위해 거리에(거의 Depth) 따라서 여러개의 Shadow Map 을 생성하는 방법이다. 넓은 범위의 Directional Light 가 닿는 그림자를 정확하게 표현하려면 꽤나 큰 크기의 Shadow Map 을 사용해야 한다. 하지만 Cascaded Shadow Mapping 을 사용한다면 여러개의 Shadow Map 을 사용하여 보다 조금의 메모리를 사용하여 넓은 범위의 그림자를 표현할 수 있다. Shadow-map generation Cascaded Shadow Mapping 을 위한 Shadow Map 생성은 앞서쓴 글에서 설명한 방법과 거의 유사하다. 앞서 여러개의 Shadow Map 을 생성하여 그림자를 표현한다고 언급했었다. 여러개의 Shaodw Map 을 생성하는 기준은 View frustum 을 Depth 를 기준으로 여러개로 쪼개어 각 쪼개진 frustum 을 기준으로 Shadow Map 을 그린다. frustum 은 보통 Depth 값을 정하거나 어떤 알고리즘을 사용하여 쪼갠다. 이는 다음 포스팅에서 언급할 예정이다. frustum 을 쪼개주면 다음은 쪼개진 Camera View frustum 의 각각의 8개의 꼭지점들을 Light-Space 로 변환한다. 변환된 각각 꼭지점으로 2차원의 aligned axis bounding box 의 위치를 구해준다. 가장 작은 X,Y 값과 가장 큰 X, Y 값을 구해주면 된다. 출처 : NVidia : Cascaded Shadow Mapping 위 그림에서 XY 평면에서의 빨간색 선으로 되어있는 사각형이 언급한 aligned axis bounding box 를 말한다. 이 AABB 는 아래에서 특정한 행렬을 만들때 쓰인다. NVidia : Cascaded Shadow Maps 에서는 이 Light-Space 로 변환하는 MVP 변환 에서 Projection 변환을 바꿔준다고 설명한다. 두개의 행렬이 나오는데, 하나는 직교 투영 행렬로(orthogonal projection) 나눠진 frustum 의 Far 값과 Near 값을 통해 생성해준다. 그리고 나머지 하나는 Crop Matrix 라는 변환 행렬이다. 위에서 구한 Light-Space 의 AABB 값을 통해 Crop Matrix 를 계산한다. 아래 그림에서나오는 Mx, My 와 mx, my 는 각각 Maximum X,Y, Minimum X,Y 를 뜻한다. 출처 : NVidia : Cascaded Shadow Mapping 이렇게 계산된 Crop Matrix 의 역할은 해당 AABB 로 Shadow Map 이 그려질 범위를 결정해주는 역할을 한다. 다만 범위가 아주 정확하지는 않다. 아래 그림을 보자. 출처 : OGLdev : Cascaded Shadow Mapping 위 그림과 같이 보통은 겹치는 부분이 생긴다. 사용시에는 Depth 에 따라서 다르게 사용하기 때문에 크게 문제는 없다. 사용시에는 Depth 값에 따라서 다른 텍스쳐를 가져오는 것과 텍스쳐를 샘플링할때 UV 값을 정점의 위치를 Light-Space 로 변환해서 변환된 정점의 위치의 X,Y 좌표를 UV 값으로 사용하면 된다. 다만 각각의 Shadow Map 마다 변환 행렬은 Crop Matrix 때문에 다르기 때문에 따로 접근해야 한다. 자세한 사용법을 알고 싶으면 NVidia : Cascaded Shadow Map Example에서 소스를 받아 보면 된다. 추가 Cascaded Shadow Map 을 1 pass 로 그리는 방법은 간단하다. 우선 Shadow Map 들을 TextureArray 를 통해 저장하고, RenderTarget 을 Geometry Shader 에서 각각의 렌더타겟별로 지오메트리를 추가해주어 각각의 Pixel Shader 를 실행시키면 된다. 자세한 코드는 여기에서 볼 수 있다. 참조 NVidia : Cascaded Shadow Maps MSDN : Cascaded Shadow Maps Github : TheRealMJP - Shadows OGLDev : Cascaded Shadow Mapping Slideshare : implements Cascaded Shadow Maps with using TexturArray(한글)</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">What Is Shadow Mapping</title>
      
      <link href="https://hrmrzizon.github.io/2017/11/30/what-is-shadow-mapping/" rel="alternate" type="text/html" title="What Is Shadow Mapping" />
      <published>2017-11-30T00:00:00+00:00</published>
      <updated>2017-11-30T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2017/11/30/what-is-shadow-mapping</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2017/11/30/what-is-shadow-mapping/">&lt;p&gt;※ 이 글은 &lt;a href=&quot;http://www.opengl-tutorial.org/intermediate-tutorials/tutorial-16-shadow-mapping/&quot;&gt;opengl-tutorial : shadow mapping&lt;/a&gt; 게시물을 참고하여 쓰여졌습니다. 자세한 내용은 원문을 보는게 좋습니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Shadow Mapping&lt;/em&gt; 실시간으로 그림자를 구현하기 위한 방법 중에 가장 널리 알려진 방법이다. 다른 방법들보다 구현하기 조금 쉬운편이긴 하나 이 방법은 완벽하지가 않기 때문에 방법 자체로는 완벽한 모습을 보이기 어렵고 다른 방법과 같이 사용하여 부족한 부분을 보완하여 사용해야 한다.&lt;/p&gt;

&lt;p&gt;일반적으로 &lt;em&gt;Shadow Mapping&lt;/em&gt; 이라 말하면 아는 사람은 머릿속에 쉽게 떠오르는 방식이 있다. 빛의 반대쪽 방향에서 충분히 멀리 떨어져 한번 오브젝트를 그린다. 이때 &lt;em&gt;Pixel Shader&lt;/em&gt; 를 null 로 설정해서 &lt;em&gt;Depth Buffer&lt;/em&gt; 의 데이터만 가져온다. 또는 &lt;em&gt;Pixel Shader&lt;/em&gt; 의 출력을 &lt;em&gt;Depth&lt;/em&gt; 로 해도 된다. 그러면 보통 아래와 비슷한 2D 텍스쳐를 얻게 된다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/OGLTuto_DepthTexture.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://www.opengl-tutorial.org/intermediate-tutorials/tutorial-16-shadow-mapping/&quot;&gt;opengl-tutorial&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;검은색에 가까워질수록(0에 가까워질수록) 해당 오브젝트의 위치가 가깝고, 흰색에 가까워질수록(1에 가까워질수록) 물체가 먼것이다. 오브젝트의 &lt;em&gt;Depth&lt;/em&gt; 를 렌더링할 때 정점에 사용되는 &lt;em&gt;MVP&lt;/em&gt; 변환 중 &lt;em&gt;View&lt;/em&gt; 변환은 임의의 위치와 빛의 방향을 계산하여 적용해준다. &lt;em&gt;Camera&lt;/em&gt; 를 기준으로 한게 아닌 &lt;em&gt;Light&lt;/em&gt; 의 방향을 기준으로 하여 관련된 것을 &lt;em&gt;Light-Space&lt;/em&gt; 라고 명명하는 경우도 더러 있다.&lt;/p&gt;

&lt;p&gt;이제 생성된 &lt;em&gt;Shadow Map&lt;/em&gt; 을 사용하는 방법에 대해 알아보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/OGLTuto_lightandshadow.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://www.opengl-tutorial.org/intermediate-tutorials/tutorial-16-shadow-mapping/&quot;&gt;opengl-tutorial&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위 그림에서 노란색으로 보이는 표면은 빛이 닿는 부분이고, 검은색으로 보이는 표면은 어떤 오브젝트에 의해 가려져 그림자가 드리운 표면이다. 해당 그림 위의 &lt;em&gt;Depth Buffer&lt;/em&gt; 를 응용하여 위처럼 가려지는 표면과 안가려지는 표면을 알아낼 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Depth Buffer&lt;/em&gt; 는 &lt;em&gt;Light-Space&lt;/em&gt; 를 기준으로 데이터를 저장하고 있다. 그리고 &lt;em&gt;Shader&lt;/em&gt; 에서는 &lt;em&gt;Local-Space&lt;/em&gt; 로 정점의 위치가 들어오기 때문에 &lt;em&gt;Depth&lt;/em&gt; 값을 비교하려면 두 값을 같은 공간으로 맞춰주어야 한다. &lt;a href=&quot;http://www.opengl-tutorial.org/intermediate-tutorials/tutorial-16-shadow-mapping/&quot;&gt;OpenGL Tutorial : Shadow Mapping&lt;/a&gt; 에서는 &lt;em&gt;bias&lt;/em&gt; 행렬과 &lt;em&gt;Light-Space&lt;/em&gt; 가 적용된 행렬을 합성하여 입력으로 들어온 정점 데이터를 &lt;em&gt;Light-Space&lt;/em&gt; 기준으로 바꿔준다.&lt;/p&gt;

&lt;p&gt;그 다음 정점의 &lt;em&gt;Depth&lt;/em&gt;(&lt;em&gt;Z&lt;/em&gt;) 값과 &lt;em&gt;Depth Buffer&lt;/em&gt; 에서 샘플링한 &lt;em&gt;Depth&lt;/em&gt;(&lt;em&gt;Z&lt;/em&gt;) 값을 비교하여 현재 정점의 &lt;em&gt;Depth&lt;/em&gt; 값이 더 크면(멀면) 그림자를 적용시킨다. 이러면 기본적인 &lt;em&gt;Shadwo Mapping&lt;/em&gt; 의 이론은 끝이다. 아래 간단한 &lt;em&gt;GLSL&lt;/em&gt; 코드가 있다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C&quot;&gt;vec4 ShadowCoord = DepthBiasMVP * vec4(vertexPosition_modelspace, 1);

float visibility = 1.0;

if (texture( shadowMap, ShadowCoord.xy ).z &amp;lt; ShadowCoord.z) {
    visibility = 0.5;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;정점의 위치를 변환시키고, &lt;em&gt;Depth&lt;/em&gt; 값에 따라 &lt;em&gt;visibility&lt;/em&gt; 값을 변경시켜 그림자를 적용시킨다. 하지만 위에서도 언급했지만 &lt;em&gt;Shadow Mapping&lt;/em&gt; 자체에는 조금 문제가 있다고 언급했다. 해당 코드의 결과를 보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/OGLTuto_1rstTry.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://www.opengl-tutorial.org/intermediate-tutorials/tutorial-16-shadow-mapping/&quot;&gt;opengl-tutorial&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위 그림은 굉장히 난장판이다. 세가지의 문제가 있는데 사진의 전체를 봐도 쉽게 알 수 있는 빛이 닿는 영역이 그림자 처리되는 것, &lt;em&gt;Shadow acne&lt;/em&gt; 가 생겼다고 말한다. 그리고 왼쪽아래 구석부분에 아주 조금 빛이 들어오는 것처럼 처리되는 것이 있다. 이는 &lt;em&gt;Peter Panning&lt;/em&gt; 이라고 부른다. 그리고 마지막으로 그림자와 빛이 닿는 부분의 경계가 울퉁불퉁한게 보일 것이다. 이를 계단현상, &lt;em&gt;aliasing&lt;/em&gt; 이라고 부르는데 흔히 게임에서 적용되는 &lt;em&gt;antialiasing&lt;/em&gt; 의 반대말이 맞다.&lt;/p&gt;

&lt;p&gt;첫번째로 해결할 문제는 &lt;em&gt;Shadow acne&lt;/em&gt; 다. 이 문제는 아래 그림을 보면 쉽게 이해가 된다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/OGLTuto_shadow-acne.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://www.opengl-tutorial.org/intermediate-tutorials/tutorial-16-shadow-mapping/&quot;&gt;opengl-tutorial&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;사선으로 나와있는 노란색 선들은 &lt;em&gt;Shadow Map&lt;/em&gt; 을 기준으로 &lt;em&gt;Light-Space&lt;/em&gt; 로 변환한 정점의 &lt;em&gt;Depth&lt;/em&gt; 값의 기준을 뜻한다. 그리고 표면 자체는 &lt;em&gt;Shadow Map&lt;/em&gt; 의 기준이 된다. 그림의 검은색 부분은 빛이 닿는 부분임에도 불구하고 그림자로 처리되는 부분인데, 이를 없에기 위해서는 값을 비교할때 단순하게 &lt;em&gt;bias&lt;/em&gt; 를 더해주면 된다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C&quot;&gt;float bias = 0.005;
float visibility = 1.0;

if (texture( shadowMap, ShadowCoord.xy ).z &amp;lt; ShadowCoord.z-bias) {
    visibility = 0.5;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;이렇게 적용시키면 평면에서의 &lt;em&gt;acne&lt;/em&gt; 들은 제거가 가능하지만 곡면에서의 &lt;em&gt;acne&lt;/em&gt; 들이 제거가 안되기 때문에 &lt;em&gt;bias&lt;/em&gt; 를 조금 수정해준다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C&quot;&gt;float bias = 0.005*tan(acos(cosTheta)); // cosTheta is dot( n,l ), clamped between 0 and 1
bias = clamp(bias, 0,0.01);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;이러면 &lt;em&gt;Shadow acne&lt;/em&gt; 들은 제거된다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/OGLTuto_VariableBias.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://www.opengl-tutorial.org/intermediate-tutorials/tutorial-16-shadow-mapping/&quot;&gt;opengl-tutorial&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;다음은 &lt;em&gt;Peter Panning&lt;/em&gt; 을 언급할 차례다. &lt;a href=&quot;http://www.opengl-tutorial.org/intermediate-tutorials/tutorial-16-shadow-mapping/&quot;&gt;OpenGL Tutorial&lt;/a&gt; 에서는 이 문제의 해결책으로 굉장히 단순한 방법을 제시한다. &lt;em&gt;Peter Panning&lt;/em&gt; 이 생기지 않도록 충분히 두꺼운 오브젝트를 배치하는 것이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/OGLTuto_NoPeterPanning.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://www.opengl-tutorial.org/intermediate-tutorials/tutorial-16-shadow-mapping/&quot;&gt;opengl-tutorial&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 쉽게 해결된다.&lt;/p&gt;

&lt;p&gt;마지막으로 다룰 문제는 &lt;em&gt;aliasing&lt;/em&gt; 이다. 이는 &lt;em&gt;Shadow Mapping&lt;/em&gt; 의 고질적인 문제로써 &lt;em&gt;anti-alisasing&lt;/em&gt; 기법을 통해 해결해왔다.&lt;/p&gt;

&lt;p&gt;첫번째로 &lt;em&gt;Shadow Map&lt;/em&gt; 을 샘플링할 때 일반적인 색을 가져오는 샘플링이 아닌 다른 방식을 사용한다. &lt;em&gt;Shadow Map&lt;/em&gt; 을 한번 샘플링할 때 하드웨어에서 주변의 텍셀을 샘플링해 주변 텍셀과 비교를 수행해 모든 비교결과를 이중선형 보간을 적용한 결과를 주는 샘플링 방식을 사용한다고 한다. 만약 이중선형 보간을 사용하지 않는다면 &lt;em&gt;Point Sampling&lt;/em&gt; 을 여러번 하여 결과들을 사용해 &lt;em&gt;PCF&lt;/em&gt; 를 적용시켜주면 된다. 이렇게 해주면 조금 부드러운 결과가 나오게 된다.&lt;/p&gt;

&lt;p&gt;하지만 이로써는 만족할만한 결과를 얻을 수 없어 주변을 여러번 샘플링해 값을 가져온다. 미리 생성된 &lt;em&gt;offset&lt;/em&gt; 을 사용해 기준 &lt;em&gt;UV&lt;/em&gt; 주변을 샘플링한다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C&quot;&gt;for (int i=0;i&amp;lt;4;i++){
  if ( texture( shadowMap, ShadowCoord.xy + poissonDisk[i]/700.0 ).z  &amp;lt;  ShadowCoord.z-bias ){
    visibility-=0.2;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;미리 생성된 &lt;em&gt;offset&lt;/em&gt; 은 &lt;em&gt;Poisson Disc&lt;/em&gt; 방식으로 생성된듯하다. &lt;em&gt;visibility&lt;/em&gt; 변수는 색의 어두움을 결정하는 변수로 한번 &lt;em&gt;Depth Test&lt;/em&gt; 에 걸리면 0.2를 줄여 0.2 ~ 1 사이의 값을 가진다.&lt;/p&gt;

&lt;p&gt;이렇게 두가지 방식으로 &lt;em&gt;anti-aliasing&lt;/em&gt; 을 해주면 제법 그럴듯한 결과가 나온다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/OGLTuto_SoftShadows_Wide.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://www.opengl-tutorial.org/intermediate-tutorials/tutorial-16-shadow-mapping/&quot;&gt;opengl-tutorial&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;또한 &lt;em&gt;UV&lt;/em&gt; 좌표에 &lt;em&gt;offset&lt;/em&gt; 을 주는 방법은 꽤나 많다. 위의 방법은 랜덤으로 고정된 부분만 체크하지만 이 방법에 임의로 &lt;em&gt;offset&lt;/em&gt; 돌려주는 방법도 있다.&lt;/p&gt;

&lt;h2&gt;참조&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.opengl-tutorial.org/kr/intermediate-tutorials/tutorial-16-shadow-mapping/&quot;&gt;OpenGL Tutorial : Tutorial 16 Shadow mapping&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://ogldev.atspace.co.uk/www/tutorial42/tutorial42.html&quot;&gt;OGLdev : Percentage Closer Filtering&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://developer.nvidia.com/gpugems/GPUGems/gpugems_ch11.html&quot;&gt;GPU Gems : Chapter 11. Shadow Map Antialiasing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Supersampling#Poisson_disc&quot;&gt;Wikipedia : SuperSampling#poisson_disc&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="shader" />
      
        <category term="shadow" />
      
        <category term="rendering" />
      

      

      
        <summary type="html">※ 이 글은 opengl-tutorial : shadow mapping 게시물을 참고하여 쓰여졌습니다. 자세한 내용은 원문을 보는게 좋습니다. Shadow Mapping 실시간으로 그림자를 구현하기 위한 방법 중에 가장 널리 알려진 방법이다. 다른 방법들보다 구현하기 조금 쉬운편이긴 하나 이 방법은 완벽하지가 않기 때문에 방법 자체로는 완벽한 모습을 보이기 어렵고 다른 방법과 같이 사용하여 부족한 부분을 보완하여 사용해야 한다. 일반적으로 Shadow Mapping 이라 말하면 아는 사람은 머릿속에 쉽게 떠오르는 방식이 있다. 빛의 반대쪽 방향에서 충분히 멀리 떨어져 한번 오브젝트를 그린다. 이때 Pixel Shader 를 null 로 설정해서 Depth Buffer 의 데이터만 가져온다. 또는 Pixel Shader 의 출력을 Depth 로 해도 된다. 그러면 보통 아래와 비슷한 2D 텍스쳐를 얻게 된다. 출처 : opengl-tutorial 검은색에 가까워질수록(0에 가까워질수록) 해당 오브젝트의 위치가 가깝고, 흰색에 가까워질수록(1에 가까워질수록) 물체가 먼것이다. 오브젝트의 Depth 를 렌더링할 때 정점에 사용되는 MVP 변환 중 View 변환은 임의의 위치와 빛의 방향을 계산하여 적용해준다. Camera 를 기준으로 한게 아닌 Light 의 방향을 기준으로 하여 관련된 것을 Light-Space 라고 명명하는 경우도 더러 있다. 이제 생성된 Shadow Map 을 사용하는 방법에 대해 알아보자. 출처 : opengl-tutorial 위 그림에서 노란색으로 보이는 표면은 빛이 닿는 부분이고, 검은색으로 보이는 표면은 어떤 오브젝트에 의해 가려져 그림자가 드리운 표면이다. 해당 그림 위의 Depth Buffer 를 응용하여 위처럼 가려지는 표면과 안가려지는 표면을 알아낼 수 있다. Depth Buffer 는 Light-Space 를 기준으로 데이터를 저장하고 있다. 그리고 Shader 에서는 Local-Space 로 정점의 위치가 들어오기 때문에 Depth 값을 비교하려면 두 값을 같은 공간으로 맞춰주어야 한다. OpenGL Tutorial : Shadow Mapping 에서는 bias 행렬과 Light-Space 가 적용된 행렬을 합성하여 입력으로 들어온 정점 데이터를 Light-Space 기준으로 바꿔준다. 그 다음 정점의 Depth(Z) 값과 Depth Buffer 에서 샘플링한 Depth(Z) 값을 비교하여 현재 정점의 Depth 값이 더 크면(멀면) 그림자를 적용시킨다. 이러면 기본적인 Shadwo Mapping 의 이론은 끝이다. 아래 간단한 GLSL 코드가 있다. vec4 ShadowCoord = DepthBiasMVP * vec4(vertexPosition_modelspace, 1); float visibility = 1.0; if (texture( shadowMap, ShadowCoord.xy ).z &amp;lt; ShadowCoord.z) { visibility = 0.5; } 정점의 위치를 변환시키고, Depth 값에 따라 visibility 값을 변경시켜 그림자를 적용시킨다. 하지만 위에서도 언급했지만 Shadow Mapping 자체에는 조금 문제가 있다고 언급했다. 해당 코드의 결과를 보자. 출처 : opengl-tutorial 위 그림은 굉장히 난장판이다. 세가지의 문제가 있는데 사진의 전체를 봐도 쉽게 알 수 있는 빛이 닿는 영역이 그림자 처리되는 것, Shadow acne 가 생겼다고 말한다. 그리고 왼쪽아래 구석부분에 아주 조금 빛이 들어오는 것처럼 처리되는 것이 있다. 이는 Peter Panning 이라고 부른다. 그리고 마지막으로 그림자와 빛이 닿는 부분의 경계가 울퉁불퉁한게 보일 것이다. 이를 계단현상, aliasing 이라고 부르는데 흔히 게임에서 적용되는 antialiasing 의 반대말이 맞다. 첫번째로 해결할 문제는 Shadow acne 다. 이 문제는 아래 그림을 보면 쉽게 이해가 된다. 출처 : opengl-tutorial 사선으로 나와있는 노란색 선들은 Shadow Map 을 기준으로 Light-Space 로 변환한 정점의 Depth 값의 기준을 뜻한다. 그리고 표면 자체는 Shadow Map 의 기준이 된다. 그림의 검은색 부분은 빛이 닿는 부분임에도 불구하고 그림자로 처리되는 부분인데, 이를 없에기 위해서는 값을 비교할때 단순하게 bias 를 더해주면 된다. float bias = 0.005; float visibility = 1.0; if (texture( shadowMap, ShadowCoord.xy ).z &amp;lt; ShadowCoord.z-bias) { visibility = 0.5; } 이렇게 적용시키면 평면에서의 acne 들은 제거가 가능하지만 곡면에서의 acne 들이 제거가 안되기 때문에 bias 를 조금 수정해준다. float bias = 0.005*tan(acos(cosTheta)); // cosTheta is dot( n,l ), clamped between 0 and 1 bias = clamp(bias, 0,0.01); 이러면 Shadow acne 들은 제거된다. 출처 : opengl-tutorial 다음은 Peter Panning 을 언급할 차례다. OpenGL Tutorial 에서는 이 문제의 해결책으로 굉장히 단순한 방법을 제시한다. Peter Panning 이 생기지 않도록 충분히 두꺼운 오브젝트를 배치하는 것이다. 출처 : opengl-tutorial 이렇게 쉽게 해결된다. 마지막으로 다룰 문제는 aliasing 이다. 이는 Shadow Mapping 의 고질적인 문제로써 anti-alisasing 기법을 통해 해결해왔다. 첫번째로 Shadow Map 을 샘플링할 때 일반적인 색을 가져오는 샘플링이 아닌 다른 방식을 사용한다. Shadow Map 을 한번 샘플링할 때 하드웨어에서 주변의 텍셀을 샘플링해 주변 텍셀과 비교를 수행해 모든 비교결과를 이중선형 보간을 적용한 결과를 주는 샘플링 방식을 사용한다고 한다. 만약 이중선형 보간을 사용하지 않는다면 Point Sampling 을 여러번 하여 결과들을 사용해 PCF 를 적용시켜주면 된다. 이렇게 해주면 조금 부드러운 결과가 나오게 된다. 하지만 이로써는 만족할만한 결과를 얻을 수 없어 주변을 여러번 샘플링해 값을 가져온다. 미리 생성된 offset 을 사용해 기준 UV 주변을 샘플링한다. for (int i=0;i&amp;lt;4;i++){ if ( texture( shadowMap, ShadowCoord.xy + poissonDisk[i]/700.0 ).z &amp;lt; ShadowCoord.z-bias ){ visibility-=0.2; } } 미리 생성된 offset 은 Poisson Disc 방식으로 생성된듯하다. visibility 변수는 색의 어두움을 결정하는 변수로 한번 Depth Test 에 걸리면 0.2를 줄여 0.2 ~ 1 사이의 값을 가진다. 이렇게 두가지 방식으로 anti-aliasing 을 해주면 제법 그럴듯한 결과가 나온다. 출처 : opengl-tutorial 또한 UV 좌표에 offset 을 주는 방법은 꽤나 많다. 위의 방법은 랜덤으로 고정된 부분만 체크하지만 이 방법에 임의로 offset 돌려주는 방법도 있다. 참조 OpenGL Tutorial : Tutorial 16 Shadow mapping OGLdev : Percentage Closer Filtering GPU Gems : Chapter 11. Shadow Map Antialiasing Wikipedia : SuperSampling#poisson_disc</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Using Compute Shader In Unity</title>
      
      <link href="https://hrmrzizon.github.io/2017/11/29/using-compute-shader-in-unity/" rel="alternate" type="text/html" title="Using Compute Shader In Unity" />
      <published>2017-11-29T00:00:00+00:00</published>
      <updated>2017-11-29T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2017/11/29/using-compute-shader-in-unity</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2017/11/29/using-compute-shader-in-unity/">&lt;p&gt;&lt;em&gt;Compute Shader&lt;/em&gt; 는 &lt;em&gt;DirectX 11&lt;/em&gt; 의 등장과 함께 본격적으로 쓰이기 시작했다. 지금은 &lt;em&gt;GPGPU&lt;/em&gt; 의 본격적인 기능으로 CPU 에서 처리하기 힘든 계산량을 책임지는 중요한 기능으로 자리잡았다. 실시간으로 현실적인 그래픽을 구현하기 위해 요즘의 게임들은 &lt;em&gt;Compute Shader&lt;/em&gt; 를 사용해서 여러 계산을 한다. 조금이라도 퍼포먼스가 필요하다면 당연히 쓰게되는 것이다.&lt;/p&gt;

&lt;p&gt;사용하는 방법 자체는 간단하지만 &lt;em&gt;Compute Shader&lt;/em&gt; 를 사용해 어떤 기능을 구현하는지가 중요하다. 간단하게 사용방법부터 알아보자. Unity 에서는 &lt;em&gt;Compute Shader&lt;/em&gt; 를 위한 파일을 생성해야 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/create_computeshader.png&quot; alt=&quot;create computeshader&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;프로젝트창에서 위 그림과 같이 생성해주면 된다. 그러면 아래와 같은 기본소스로 파일이 생성된다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C&quot;&gt;// Each #kernel tells which function to compile; you can have many kernels
#pragma kernel CSMain

// Create a RenderTexture with enableRandomWrite flag and set it
// with cs.SetTexture
RWTexture2D&amp;lt;float4&amp;gt; Result;

[numthreads(8,8,1)]
void CSMain (uint3 id : SV_DispatchThreadID)
{
	// TODO: insert actual code here!

	Result[id.xy] = float4(id.x &amp;amp; id.y, (id.x &amp;amp; 15)/15.0, (id.y &amp;amp; 15)/15.0, 0.0);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;위의 소스는 &lt;em&gt;HLSL&lt;/em&gt; 로 코딩된 소스로 &lt;em&gt;DirectX 11&lt;/em&gt; 을 기준으로 코딩되어 있다. &lt;em&gt;UnityCG&lt;/em&gt; 파일안의 코드를 이용하면 &lt;em&gt;GLSL&lt;/em&gt; 로 자동 컨버팅이 되기도 한다. 직접 &lt;em&gt;GLSL&lt;/em&gt; 코드로 코딩하고 싶다면 &lt;em&gt;GLSLPROGRAM&lt;/em&gt; 과 &lt;em&gt;ENDGLSL&lt;/em&gt; 로 코드를 감싸주면 간단하게 해결된다.&lt;/p&gt;

&lt;p&gt;내용은 간단하다. 각 텍셀별로 접근이 가능한 &lt;em&gt;Texture&lt;/em&gt; 를 이용해서(&lt;em&gt;DirectX&lt;/em&gt; 에서는 UAV 라고 칭한다.) &lt;em&gt;Texture&lt;/em&gt; 에 값을 채운다. &lt;em&gt;HLSL&lt;/em&gt; 의 자세한 문법과 사용방법은 &lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/ff471569.aspx&quot;&gt;MSDN : SV_GroupIndex&lt;/a&gt;, &lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/bb509647.aspx&quot;&gt;MSDN : Semantics &lt;/a&gt; 들을 참고하길 바란다.&lt;/p&gt;

&lt;p&gt;또한 쉐이더에서 뿐만아니라 &lt;em&gt;Unity&lt;/em&gt; 스크립트상에서도 데이터들을 연결해주어야 한다. 사용하는 유형은 간단하다. &lt;strong&gt;UnityEngine.Texture&lt;/strong&gt; 에서 파생된 텍스쳐들, &lt;strong&gt;UnityEngine.RenderTexture&lt;/strong&gt;, &lt;strong&gt;UnityEngine.ComputeBuffer&lt;/strong&gt; 정도면 모든 활용이 가능하다. &lt;strong&gt;UnityEngine.RenderTexture&lt;/strong&gt; 에서는 &lt;em&gt;Cubemap&lt;/em&gt; 도 지원하니 간단하게 쓸 수 있다. 해당 인스턴스를 넘겨주는 방법은 아래와 같다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C#&quot;&gt;ComputeShader shader = ...;
RenderTexture rt = ...;

shader.SetTexture(&quot;Result&quot;, rt);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;코드에서의 변수명을 맞추어 넣어주거나 해쉬값을 미리 가져와 넣어주면 된다. 다른 유형의 데이터들도 이런 방법으로 넣을 수 있다. 데이터를 넣어주면 다음은 &lt;em&gt;Compute Shader&lt;/em&gt; 를 실행하여 결과를 얻어야 한다. 간단하게 함수호출만 해주면 된다. 방법은 아래와 같다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C#&quot;&gt;ComputeShader shader = ...;
RenderTexture rt = ...;
int kernelIndex = shader.FindKernel(&quot;CSMain&quot;);

shader.Dispatch(kernelIndex, rt.width / 8, rt.height / 8, 1);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;해당 &lt;em&gt;Compute Shader&lt;/em&gt; 소스는 텍스쳐안에 값을 채우는 코드이기 때문에 위와같이 해주었다. &lt;a href=&quot;https://docs.unity3d.com/ScriptReference/ComputeShader.Dispatch.html&quot;&gt;Unity Reference : ComputeShader.Dispatch&lt;/a&gt; 와 위의 &lt;em&gt;Compute Shader&lt;/em&gt; 소스를 참고하면 알겠지만 최대 3차원의 방식으로 &lt;em&gt;Compute Shader&lt;/em&gt; 의 그룹을 설정하여 계산이 가능하다.  &lt;em&gt;Compute Shader&lt;/em&gt; 소스의 &lt;em&gt;[numthreads(8,8,1)]&lt;/em&gt; 는 한 그룹의 &lt;em&gt;Thread&lt;/em&gt; 갯수를 나타내고, &lt;em&gt;ComputeShader.Dispatch&lt;/em&gt; 메소드는 몇개의 그룹을 실행시키는지 넘겨주는 메소드다. 아래 그림을 보면 조금더 쉽게 이해가 가능하다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://msdn.microsoft.com/dynimg/IC520438.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/ff471569.aspx&quot;&gt;MSDN&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Compute Shader&lt;/em&gt; 는 &lt;em&gt;DirectX 11&lt;/em&gt; 이상, &lt;em&gt;Vulkan&lt;/em&gt;,  &lt;em&gt;OpenGL 4.3&lt;/em&gt; 이상, &lt;em&gt;OpenGL ES 3.0&lt;/em&gt; 이상, &lt;em&gt;Metal&lt;/em&gt; 에서 사용가능하다. 그 아래의 플랫폼은 지원하지 않는다. 또 유의해야 할점은 그래픽 드라이버별로 지원 기능이 조금씩 다를 수 있으니 기능을 유의하며 사용해야한다. &lt;a href=&quot;https://docs.unity3d.com/Manual/ComputeShaders.html&quot;&gt;Unity Manual : ComptuteShader&lt;/a&gt; 에서 조금 참고할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;/2017/08/01/using-compute-buffer-in-unity/&quot;&gt;Using Compute Buffer in Unity&lt;/a&gt; 에서 관련된 내용을 언급했으니 같이 보면 좋을듯 하다.&lt;/p&gt;

&lt;h2&gt;참조&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/ff471569.aspx&quot;&gt;MSDN : SV_GroupIndex&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/bb509647.aspx&quot;&gt;MSDN : Semantics &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.unity3d.com/Manual/ComputeShaders.html&quot;&gt;Unity Manual : ComptuteShader&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="unity" />
      
        <category term="shader" />
      
        <category term="hlsl" />
      

      

      
        <summary type="html">Compute Shader 는 DirectX 11 의 등장과 함께 본격적으로 쓰이기 시작했다. 지금은 GPGPU 의 본격적인 기능으로 CPU 에서 처리하기 힘든 계산량을 책임지는 중요한 기능으로 자리잡았다. 실시간으로 현실적인 그래픽을 구현하기 위해 요즘의 게임들은 Compute Shader 를 사용해서 여러 계산을 한다. 조금이라도 퍼포먼스가 필요하다면 당연히 쓰게되는 것이다. 사용하는 방법 자체는 간단하지만 Compute Shader 를 사용해 어떤 기능을 구현하는지가 중요하다. 간단하게 사용방법부터 알아보자. Unity 에서는 Compute Shader 를 위한 파일을 생성해야 한다. 프로젝트창에서 위 그림과 같이 생성해주면 된다. 그러면 아래와 같은 기본소스로 파일이 생성된다. // Each #kernel tells which function to compile; you can have many kernels #pragma kernel CSMain // Create a RenderTexture with enableRandomWrite flag and set it // with cs.SetTexture RWTexture2D&amp;lt;float4&amp;gt; Result; [numthreads(8,8,1)] void CSMain (uint3 id : SV_DispatchThreadID) { // TODO: insert actual code here! Result[id.xy] = float4(id.x &amp;amp; id.y, (id.x &amp;amp; 15)/15.0, (id.y &amp;amp; 15)/15.0, 0.0); } 위의 소스는 HLSL 로 코딩된 소스로 DirectX 11 을 기준으로 코딩되어 있다. UnityCG 파일안의 코드를 이용하면 GLSL 로 자동 컨버팅이 되기도 한다. 직접 GLSL 코드로 코딩하고 싶다면 GLSLPROGRAM 과 ENDGLSL 로 코드를 감싸주면 간단하게 해결된다. 내용은 간단하다. 각 텍셀별로 접근이 가능한 Texture 를 이용해서(DirectX 에서는 UAV 라고 칭한다.) Texture 에 값을 채운다. HLSL 의 자세한 문법과 사용방법은 MSDN : SV_GroupIndex, MSDN : Semantics 들을 참고하길 바란다. 또한 쉐이더에서 뿐만아니라 Unity 스크립트상에서도 데이터들을 연결해주어야 한다. 사용하는 유형은 간단하다. UnityEngine.Texture 에서 파생된 텍스쳐들, UnityEngine.RenderTexture, UnityEngine.ComputeBuffer 정도면 모든 활용이 가능하다. UnityEngine.RenderTexture 에서는 Cubemap 도 지원하니 간단하게 쓸 수 있다. 해당 인스턴스를 넘겨주는 방법은 아래와 같다. ComputeShader shader = ...; RenderTexture rt = ...; shader.SetTexture(&quot;Result&quot;, rt); 코드에서의 변수명을 맞추어 넣어주거나 해쉬값을 미리 가져와 넣어주면 된다. 다른 유형의 데이터들도 이런 방법으로 넣을 수 있다. 데이터를 넣어주면 다음은 Compute Shader 를 실행하여 결과를 얻어야 한다. 간단하게 함수호출만 해주면 된다. 방법은 아래와 같다. ComputeShader shader = ...; RenderTexture rt = ...; int kernelIndex = shader.FindKernel(&quot;CSMain&quot;); shader.Dispatch(kernelIndex, rt.width / 8, rt.height / 8, 1); 해당 Compute Shader 소스는 텍스쳐안에 값을 채우는 코드이기 때문에 위와같이 해주었다. Unity Reference : ComputeShader.Dispatch 와 위의 Compute Shader 소스를 참고하면 알겠지만 최대 3차원의 방식으로 Compute Shader 의 그룹을 설정하여 계산이 가능하다. Compute Shader 소스의 [numthreads(8,8,1)] 는 한 그룹의 Thread 갯수를 나타내고, ComputeShader.Dispatch 메소드는 몇개의 그룹을 실행시키는지 넘겨주는 메소드다. 아래 그림을 보면 조금더 쉽게 이해가 가능하다. 출처 : MSDN Compute Shader 는 DirectX 11 이상, Vulkan, OpenGL 4.3 이상, OpenGL ES 3.0 이상, Metal 에서 사용가능하다. 그 아래의 플랫폼은 지원하지 않는다. 또 유의해야 할점은 그래픽 드라이버별로 지원 기능이 조금씩 다를 수 있으니 기능을 유의하며 사용해야한다. Unity Manual : ComptuteShader 에서 조금 참고할 수 있다. Using Compute Buffer in Unity 에서 관련된 내용을 언급했으니 같이 보면 좋을듯 하다. 참조 MSDN : SV_GroupIndex MSDN : Semantics Unity Manual : ComptuteShader</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Drawinstanced Vs Merged Instancing</title>
      
      <link href="https://hrmrzizon.github.io/2017/11/18/DrawInstanced-vs-Merged-Instancing/" rel="alternate" type="text/html" title="Drawinstanced Vs Merged Instancing" />
      <published>2017-11-18T00:00:00+00:00</published>
      <updated>2017-11-18T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2017/11/18/DrawInstanced-vs-Merged-Instancing</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2017/11/18/DrawInstanced-vs-Merged-Instancing/">&lt;p&gt;&lt;a href=&quot;https://www.gdcvault.com/play/1020624/Advanced-Visual-Effects-with-DirectX&quot;&gt;GDC 2014 : Vertex Sahder Tricks&lt;/a&gt; 슬라이드에 따르면 &lt;em&gt;DrawInstanced&lt;/em&gt; 함수를 사용하여 인스턴싱을 하는것보다 &lt;em&gt;vertexID&lt;/em&gt; 를 사용하여 인스턴싱을 하는것이 빠르다고 한다. &lt;em&gt;vertexID&lt;/em&gt; 를 쓰는 방법은 굉장히 단순하다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;VSOutput VS(uint id : SV_VertexID)
{
    VSOutput output;

    /*
        ...
    */

    return output;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;SV_VertexID&lt;/em&gt; &lt;em&gt;Semantic&lt;/em&gt; 을 사용하여 값을 접근하기만 하면 된다. &lt;em&gt;vertexID&lt;/em&gt; 는 말그대로 버텍스별 인덱스를 뜻한다. &lt;em&gt;SRV&lt;/em&gt; 나 &lt;em&gt;UAV&lt;/em&gt; 와 함께 사용하여 &lt;em&gt;Instancing&lt;/em&gt; 을 하면된다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/gdc2014_vertexshadertricks_23.png&quot; alt=&quot;Merge Instancing Performance&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://www.gdcvault.com/play/1020624/Advanced-Visual-Effects-with-DirectX&quot;&gt;GDC 2014 : Vertex Sahder Tricks&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;그림을 보면 AMD GPU 에서 확실히 퍼포먼스 차이가 난것을 확인할 수 있다. &lt;del&gt;스피커가 AMD 소속이라는 게 포인트&lt;/del&gt;&lt;/p&gt;

&lt;h1&gt;참조 자료&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.gdcvault.com/play/1020624/Advanced-Visual-Effects-with-DirectX&quot;&gt;GDC 2014 : Vertex Sahder Tricks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="render" />
      
        <category term="hlsl" />
      

      

      
        <summary type="html">GDC 2014 : Vertex Sahder Tricks 슬라이드에 따르면 DrawInstanced 함수를 사용하여 인스턴싱을 하는것보다 vertexID 를 사용하여 인스턴싱을 하는것이 빠르다고 한다. vertexID 를 쓰는 방법은 굉장히 단순하다. VSOutput VS(uint id : SV_VertexID) { VSOutput output; /* ... */ return output; } SV_VertexID Semantic 을 사용하여 값을 접근하기만 하면 된다. vertexID 는 말그대로 버텍스별 인덱스를 뜻한다. SRV 나 UAV 와 함께 사용하여 Instancing 을 하면된다. 출처 : GDC 2014 : Vertex Sahder Tricks 그림을 보면 AMD GPU 에서 확실히 퍼포먼스 차이가 난것을 확인할 수 있다. 스피커가 AMD 소속이라는 게 포인트 참조 자료 GDC 2014 : Vertex Sahder Tricks</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Hbao Plus Analysis 3</title>
      
      <link href="https://hrmrzizon.github.io/2017/11/15/hbao-plus-analysis-3/" rel="alternate" type="text/html" title="Hbao Plus Analysis 3" />
      <published>2017-11-15T00:00:00+00:00</published>
      <updated>2017-11-15T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2017/11/15/hbao-plus-analysis-3</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2017/11/15/hbao-plus-analysis-3/">&lt;p&gt;&lt;strong&gt;HBAO+ 3.1 버젼을 기준으로 글이 작성되었습니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이전 &lt;a href=&quot;/2017/11/15/hbao-plus-analysis-2/&quot;&gt;hbao plus analysis 2&lt;/a&gt; 글에서 &lt;em&gt;Horizon based ambient occlusion&lt;/em&gt; 와 &lt;em&gt;Cross Bilateral Filter&lt;/em&gt; 대해서 알아보았다. 이번 글에서는 부록의 느낌으로 &lt;em&gt;HLSL&lt;/em&gt; 코드를 읽으면서 생소했던 기타 기법들에 대해서 써볼 것이다.&lt;/p&gt;

&lt;p&gt;첫번째로 &lt;em&gt;Full Screen Triangle&lt;/em&gt; 이라는 기법이다. 알고마면 굉장히 단순한 개념으로, 화면을 모두 덮는 한개의 삼각형을 그려서 모든 픽셀에 쉐이더를 돌릴 수 있게 해주는 기법이다. 아래 슬라이드를 보면 쉽게 이해가 갈것이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/vertex-shader-tricks-by-bill-bilodeau-amd-at-gdc14-14-638.jpg&quot; alt=&quot;Full Screen Triangle&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://www.gdcvault.com/play/1020624/Advanced-Visual-Effects-with-DirectX&quot;&gt;GDC 2014 : Vertex Sahder Tricks&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;단순하지만 처음 봤을 때는 조금 신박하게 느껴질 수도 있다. 두번째로는 모든 계산에 최대한 &lt;em&gt;HLSL Intrisic&lt;/em&gt; 을 사용한다. 특히 벡터와 벡터사이의 거리를 계산할때 &lt;em&gt;dot product&lt;/em&gt; 를 써서 하는게 정말 많았다. 어셈블리 레벨에서 달라지는것 같긴하나 정확한 이유는 알지 못했다. 추측해보면 GPU 에서 해당 명령어가 있지 않을까.. 라고 생각한다.&lt;/p&gt;

&lt;p&gt;세번째도 위의 것과 비슷하다. 대부분의 데이터에 &lt;em&gt;MAD&lt;/em&gt; 방식을 사용해서 계산한다. 하지만 이는 거의 공식적으로 정해진게 있다. &lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/ff471418.aspx&quot;&gt;MSDN : mad  function&lt;/a&gt; 레퍼런스에서도 나오듯이 어떤 GPU 에서는 위에서 추측한대로 하드웨어에서 지원하는 명령어라고 한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;…
Shaders can then take advantage of potential performance improvements by using a native mad instruction (versus mul + add) on some hardware.
…&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;또한 &lt;em&gt;HBAO+&lt;/em&gt; 소스에서 찾은 주석에는 &lt;em&gt;GK104&lt;/em&gt; 부터 특정 구간에서 10% 퍼포먼스 이득이 있다고 쓰여져 있다.&lt;/p&gt;

&lt;p&gt;네번째는 나누기를 절대 쓰지 않는다. 나머지 연산(mod, A % B)는 간혹 쓰이지만 나누기는 절대로 쓰이지 않았었다. 혹시라도 필요하다면 전부 &lt;em&gt;Constant Buffer&lt;/em&gt; 에 CPU 에서 역수를 취해서 넘겨주는 방식으로 되어 있었다. 이도 역시 하드웨어에서 동작하는 부분을 알고 짠듯하다.&lt;/p&gt;

&lt;p&gt;다섯번째는 &lt;em&gt;HLSL&lt;/em&gt; 코드를 &lt;em&gt;cpp&lt;/em&gt; 소스에 &lt;em&gt;include&lt;/em&gt; 하여 &lt;em&gt;Constant Buffer&lt;/em&gt; 값을 갱신하는 코드였다. 여태까지 예전의 &lt;em&gt;DirectX&lt;/em&gt; 소스만 보거나 &lt;em&gt;Unity&lt;/em&gt; 에서만 작업을 해서 그런지 이런 기능은 굉장히 낯설었다.&lt;/p&gt;

&lt;h1&gt;참조 자료&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://docs.nvidia.com/gameworks/content/gameworkslibrary/visualfx/hbao/index.html&quot;&gt;NVIDIA HBAO+&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.gdcvault.com/play/1020624/Advanced-Visual-Effects-with-DirectX&quot;&gt;GDC 2014 : Vertex Sahder Tricks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/ff471418.aspx&quot;&gt;MSDN : mad function&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="render" />
      
        <category term="hlsl" />
      
        <category term="analysis" />
      
        <category term="hbao+" />
      

      

      
        <summary type="html">HBAO+ 3.1 버젼을 기준으로 글이 작성되었습니다. 이전 hbao plus analysis 2 글에서 Horizon based ambient occlusion 와 Cross Bilateral Filter 대해서 알아보았다. 이번 글에서는 부록의 느낌으로 HLSL 코드를 읽으면서 생소했던 기타 기법들에 대해서 써볼 것이다. 첫번째로 Full Screen Triangle 이라는 기법이다. 알고마면 굉장히 단순한 개념으로, 화면을 모두 덮는 한개의 삼각형을 그려서 모든 픽셀에 쉐이더를 돌릴 수 있게 해주는 기법이다. 아래 슬라이드를 보면 쉽게 이해가 갈것이다. 출처 : GDC 2014 : Vertex Sahder Tricks 단순하지만 처음 봤을 때는 조금 신박하게 느껴질 수도 있다. 두번째로는 모든 계산에 최대한 HLSL Intrisic 을 사용한다. 특히 벡터와 벡터사이의 거리를 계산할때 dot product 를 써서 하는게 정말 많았다. 어셈블리 레벨에서 달라지는것 같긴하나 정확한 이유는 알지 못했다. 추측해보면 GPU 에서 해당 명령어가 있지 않을까.. 라고 생각한다. 세번째도 위의 것과 비슷하다. 대부분의 데이터에 MAD 방식을 사용해서 계산한다. 하지만 이는 거의 공식적으로 정해진게 있다. MSDN : mad function 레퍼런스에서도 나오듯이 어떤 GPU 에서는 위에서 추측한대로 하드웨어에서 지원하는 명령어라고 한다. … Shaders can then take advantage of potential performance improvements by using a native mad instruction (versus mul + add) on some hardware. … 또한 HBAO+ 소스에서 찾은 주석에는 GK104 부터 특정 구간에서 10% 퍼포먼스 이득이 있다고 쓰여져 있다. 네번째는 나누기를 절대 쓰지 않는다. 나머지 연산(mod, A % B)는 간혹 쓰이지만 나누기는 절대로 쓰이지 않았었다. 혹시라도 필요하다면 전부 Constant Buffer 에 CPU 에서 역수를 취해서 넘겨주는 방식으로 되어 있었다. 이도 역시 하드웨어에서 동작하는 부분을 알고 짠듯하다. 다섯번째는 HLSL 코드를 cpp 소스에 include 하여 Constant Buffer 값을 갱신하는 코드였다. 여태까지 예전의 DirectX 소스만 보거나 Unity 에서만 작업을 해서 그런지 이런 기능은 굉장히 낯설었다. 참조 자료 NVIDIA HBAO+ GDC 2014 : Vertex Sahder Tricks MSDN : mad function</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Hbao Plus Analysis 2</title>
      
      <link href="https://hrmrzizon.github.io/2017/11/15/hbao-plus-analysis-2/" rel="alternate" type="text/html" title="Hbao Plus Analysis 2" />
      <published>2017-11-15T00:00:00+00:00</published>
      <updated>2017-11-15T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2017/11/15/hbao-plus-analysis-2</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2017/11/15/hbao-plus-analysis-2/">&lt;p&gt;&lt;strong&gt;HBAO+ 3.1 버젼을 기준으로 글이 작성되었습니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이전 &lt;a href=&quot;/2017/11/15/hbao-plus-analysis-1/&quot;&gt;hbao plus analysis 1&lt;/a&gt; 글에서 &lt;em&gt;HBAO+&lt;/em&gt; 에서 &lt;em&gt;Linearize Depth&lt;/em&gt; 와 &lt;em&gt;Deinterleaved Texturing&lt;/em&gt; 에 대해서 알아보았다. 이번 글에서는 &lt;em&gt;HBAO+&lt;/em&gt; 의 핵심 알고리즘인 &lt;em&gt;Horizon Based Ambient Occlusion&lt;/em&gt; 와 AO 블러에 사용되는 &lt;em&gt;Cross Bilateral Filter&lt;/em&gt; 에 대해서 알아볼것이다.&lt;/p&gt;

&lt;h2&gt;Horizon Based Ambient Occlusion&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Horizon Based Ambient Occlusion&lt;/em&gt; 은 xy 평면과(horizon) Depth 값을 사용해서 &lt;em&gt;AO&lt;/em&gt; 를 계산한다. 슬라이드에서 가져온 일부를 보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/hbao_siggraph08_05.png&quot; alt=&quot;Horizon Mapping&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://developer.download.nvidia.com/presentations/2008/SIGGRAPH/HBAO_SIG08b.pdf&quot;&gt;Siggraph 2008 : Image-Space Horizon-Based Ambient Occlusion&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;해당 슬라이드에서는 xy평면을 단순하게 1차원인 x축만으로 나타냈다. &lt;em&gt;HBAO&lt;/em&gt; 는 그림에 나오는 &lt;em&gt;horizon angle&lt;/em&gt; 을 사용하여 &lt;em&gt;AO&lt;/em&gt; 값을 구한다. 자세한 방법은 아래 슬라이드를 보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/hbao_siggraph08_12.png&quot; alt=&quot;Horizon-Based AO&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://developer.download.nvidia.com/presentations/2008/SIGGRAPH/HBAO_SIG08b.pdf&quot;&gt;Siggraph 2008 : Image-Space Horizon-Based Ambient Occlusion&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;슬라이드에서는 표면의 접선을 나타내는 &lt;em&gt;Tangent&lt;/em&gt; 벡터와 &lt;em&gt;Horizon&lt;/em&gt; 벡터를 사용해서 &lt;em&gt;sin&lt;/em&gt; 의 차이로 &lt;em&gt;AO&lt;/em&gt; 를 계산한다고 설명되어 있다. &lt;em&gt;Horizon&lt;/em&gt; 벡터는 &lt;em&gt;Depth&lt;/em&gt; 와 화면의 좌표를 구해서 샘플링하는 위치값을 구하고 기준이 되는 위치값의 차이를 통해 구한다. &lt;em&gt;HBAO+&lt;/em&gt; 코드에서는 입력을 받은 &lt;em&gt;Normal&lt;/em&gt; 벡터와 &lt;em&gt;Horizon&lt;/em&gt; 벡터에 &lt;em&gt;dot&lt;/em&gt; 을 사용해 &lt;em&gt;cos&lt;/em&gt; 값을 구하고 변환해준다. 이렇게 한번 &lt;em&gt;AO&lt;/em&gt; 값을 구한다.&lt;/p&gt;

&lt;p&gt;보다 정확한 &lt;em&gt;AO&lt;/em&gt; 값을 구하기 위해서는 전방위로 탐색할 필요가 있다. 정해진 방향으로 샘플링을 해도 오차가 생길 수 있고 완전히 랜덤하게 방향을 정해도 부정확한 결과를 얻을 수 있다. 그래서 &lt;em&gt;HBAO&lt;/em&gt; 는 랜덤하게 방향을 정하나 그 방향 벡터를 정해진 각도로 돌려주어 그나마 정확한 결과를 얻으려 한다. 슬라이드를 보고 넘어가자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/hbao_siggraph08_14.png&quot; alt=&quot;Sampling the Depth Image&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://developer.download.nvidia.com/presentations/2008/SIGGRAPH/HBAO_SIG08b.pdf&quot;&gt;Siggraph 2008 : Image-Space Horizon-Based Ambient Occlusion&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;핵심적인 개념은 모두 설명했지만 만족할만한 결과를 얻기 위해 여러가지 보정 방법들이 필요하다. 그래서 &lt;em&gt;HBAO&lt;/em&gt; 에서는 두가지 보정을 해주는 개념을 설명한다. &lt;em&gt;HBAO&lt;/em&gt; 는 방향을 설정해주고 해당 방향으로 한번만 샘플링 하는게 아니라 여러번 샘플링 한다. 그러므로 거리에 따른 감쇠(attenuation)가 필요하다. 방법은 간단하다. &lt;em&gt;AO&lt;/em&gt; 를 계산할때 구했던 &lt;em&gt;Horizon&lt;/em&gt; 벡터의 크기에 따라서 &lt;em&gt;AO&lt;/em&gt; 값을 줄여준다. 나머지 한가지는 &lt;em&gt;Horizon&lt;/em&gt; 벡터와 &lt;em&gt;Tangent&lt;/em&gt; 벡터를 이용해 구하는 실질적인 &lt;em&gt;AO&lt;/em&gt; 값에 &lt;em&gt;Bias&lt;/em&gt; 로 낮은 &lt;em&gt;AO&lt;/em&gt; 값들을 무시하는 방법이다. &lt;em&gt;Bias&lt;/em&gt; 가 없이 &lt;em&gt;AO&lt;/em&gt; 를 생성하게 되면 노이즈가 생기기 때문이다. 또한 &lt;em&gt;Bias&lt;/em&gt; 로 생긴 수학적 오차는 코드에서 따로 보정해주기 때문에 크게 문제는 없다.&lt;/p&gt;

&lt;h2&gt;Cross Bilateral Filter&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;SSAO&lt;/em&gt; 의 결과에는 일반적으로 블러를 먹이게 된다. 대부분 근사에 기반한 계산이기 때문이다. &lt;em&gt;HBAO&lt;/em&gt; 에서는 &lt;em&gt;Depth&lt;/em&gt; 를 이용한 방법을 소개한다. 바로 &lt;em&gt;Cross Bilateral Filter&lt;/em&gt; 다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Cross Bilateral Filter&lt;/em&gt; 은 &lt;em&gt;Gaussian Filter&lt;/em&gt; 와 비슷한 필터로, &lt;em&gt;Gaussian Filter&lt;/em&gt; 는 샘플링할 위치의 거리에 따라 점차 가중치가 줄어드는 필터라면, &lt;em&gt;Bilateral Filter&lt;/em&gt; 는 위치에 따라 가중치가 줄어드는게 아닌 각 위치별로 가지고 있는 한개의 스칼라값에 차이에 따라서 가중치를 정하는 필터다. &lt;em&gt;Cross&lt;/em&gt; 단어를 붙인 이유는 왼쪽과 오른쪽 방향의 필터와 위와 아래의 필터를 따로하기 때문에 &lt;em&gt;Cross&lt;/em&gt; 라는 단어를 붙인 듯 하다. &lt;em&gt;HBAO+&lt;/em&gt; 코드에서도 X 축과 Y 축을 기준으로 하는 블러 소스가 나누어져 있다. &lt;em&gt;HBAO&lt;/em&gt; 에는 한개의 스칼라 값을 &lt;em&gt;Depth&lt;/em&gt; 를 기준으로 계산한다. 그래서 크게 튀는 부분의 결과는 많이 반영하지 않아 전체적으로 뿌옇게 바뀐다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/hbao_siggraph08_28.png&quot; alt=&quot;Sampling the Depth Image&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://developer.download.nvidia.com/presentations/2008/SIGGRAPH/HBAO_SIG08b.pdf&quot;&gt;Siggraph 2008 : Image-Space Horizon-Based Ambient Occlusion&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1&gt;참조 자료&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://docs.nvidia.com/gameworks/content/gameworkslibrary/visualfx/hbao/index.html&quot;&gt;NVidia HBAO+&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://developer.download.nvidia.com/presentations/2008/SIGGRAPH/HBAO_SIG08b.pdf&quot;&gt;Image-Space Horizon Based Ambient Occlusion&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Gaussian_filter&quot;&gt;Wikipedia : Gaussian Filter&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Bilateral_filter&quot;&gt;Wikipedia : Bilateral Filter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="render" />
      
        <category term="analysis" />
      
        <category term="hbao+" />
      
        <category term="bilateral_filter" />
      

      

      
        <summary type="html">HBAO+ 3.1 버젼을 기준으로 글이 작성되었습니다. 이전 hbao plus analysis 1 글에서 HBAO+ 에서 Linearize Depth 와 Deinterleaved Texturing 에 대해서 알아보았다. 이번 글에서는 HBAO+ 의 핵심 알고리즘인 Horizon Based Ambient Occlusion 와 AO 블러에 사용되는 Cross Bilateral Filter 에 대해서 알아볼것이다. Horizon Based Ambient Occlusion Horizon Based Ambient Occlusion 은 xy 평면과(horizon) Depth 값을 사용해서 AO 를 계산한다. 슬라이드에서 가져온 일부를 보자. 출처 : Siggraph 2008 : Image-Space Horizon-Based Ambient Occlusion 해당 슬라이드에서는 xy평면을 단순하게 1차원인 x축만으로 나타냈다. HBAO 는 그림에 나오는 horizon angle 을 사용하여 AO 값을 구한다. 자세한 방법은 아래 슬라이드를 보자. 출처 : Siggraph 2008 : Image-Space Horizon-Based Ambient Occlusion 슬라이드에서는 표면의 접선을 나타내는 Tangent 벡터와 Horizon 벡터를 사용해서 sin 의 차이로 AO 를 계산한다고 설명되어 있다. Horizon 벡터는 Depth 와 화면의 좌표를 구해서 샘플링하는 위치값을 구하고 기준이 되는 위치값의 차이를 통해 구한다. HBAO+ 코드에서는 입력을 받은 Normal 벡터와 Horizon 벡터에 dot 을 사용해 cos 값을 구하고 변환해준다. 이렇게 한번 AO 값을 구한다. 보다 정확한 AO 값을 구하기 위해서는 전방위로 탐색할 필요가 있다. 정해진 방향으로 샘플링을 해도 오차가 생길 수 있고 완전히 랜덤하게 방향을 정해도 부정확한 결과를 얻을 수 있다. 그래서 HBAO 는 랜덤하게 방향을 정하나 그 방향 벡터를 정해진 각도로 돌려주어 그나마 정확한 결과를 얻으려 한다. 슬라이드를 보고 넘어가자. 출처 : Siggraph 2008 : Image-Space Horizon-Based Ambient Occlusion 핵심적인 개념은 모두 설명했지만 만족할만한 결과를 얻기 위해 여러가지 보정 방법들이 필요하다. 그래서 HBAO 에서는 두가지 보정을 해주는 개념을 설명한다. HBAO 는 방향을 설정해주고 해당 방향으로 한번만 샘플링 하는게 아니라 여러번 샘플링 한다. 그러므로 거리에 따른 감쇠(attenuation)가 필요하다. 방법은 간단하다. AO 를 계산할때 구했던 Horizon 벡터의 크기에 따라서 AO 값을 줄여준다. 나머지 한가지는 Horizon 벡터와 Tangent 벡터를 이용해 구하는 실질적인 AO 값에 Bias 로 낮은 AO 값들을 무시하는 방법이다. Bias 가 없이 AO 를 생성하게 되면 노이즈가 생기기 때문이다. 또한 Bias 로 생긴 수학적 오차는 코드에서 따로 보정해주기 때문에 크게 문제는 없다. Cross Bilateral Filter SSAO 의 결과에는 일반적으로 블러를 먹이게 된다. 대부분 근사에 기반한 계산이기 때문이다. HBAO 에서는 Depth 를 이용한 방법을 소개한다. 바로 Cross Bilateral Filter 다. Cross Bilateral Filter 은 Gaussian Filter 와 비슷한 필터로, Gaussian Filter 는 샘플링할 위치의 거리에 따라 점차 가중치가 줄어드는 필터라면, Bilateral Filter 는 위치에 따라 가중치가 줄어드는게 아닌 각 위치별로 가지고 있는 한개의 스칼라값에 차이에 따라서 가중치를 정하는 필터다. Cross 단어를 붙인 이유는 왼쪽과 오른쪽 방향의 필터와 위와 아래의 필터를 따로하기 때문에 Cross 라는 단어를 붙인 듯 하다. HBAO+ 코드에서도 X 축과 Y 축을 기준으로 하는 블러 소스가 나누어져 있다. HBAO 에는 한개의 스칼라 값을 Depth 를 기준으로 계산한다. 그래서 크게 튀는 부분의 결과는 많이 반영하지 않아 전체적으로 뿌옇게 바뀐다. 출처 : Siggraph 2008 : Image-Space Horizon-Based Ambient Occlusion 참조 자료 NVidia HBAO+ Image-Space Horizon Based Ambient Occlusion Wikipedia : Gaussian Filter Wikipedia : Bilateral Filter</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Hbao Plus Analysis 1</title>
      
      <link href="https://hrmrzizon.github.io/2017/11/15/hbao-plus-analysis-1/" rel="alternate" type="text/html" title="Hbao Plus Analysis 1" />
      <published>2017-11-15T00:00:00+00:00</published>
      <updated>2017-11-15T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2017/11/15/hbao-plus-analysis-1</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2017/11/15/hbao-plus-analysis-1/">&lt;p&gt;&lt;strong&gt;HBAO+ 3.1 버젼을 기준으로 글이 작성되었습니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이전 &lt;a href=&quot;/2017/11/15/hbao-plus-analysis-0/&quot;&gt;hbao plus analysis 0&lt;/a&gt; 글에서 &lt;em&gt;HBAO+&lt;/em&gt; 을 알기위한 기본적인 개념들에 대해서 살펴보았다. 이번 글에서는 &lt;em&gt;HBAO+&lt;/em&gt; 의 구조와 &lt;em&gt;Linearize Depth&lt;/em&gt; 와 &lt;em&gt;Deinterleaved Texturing&lt;/em&gt; 에 대해서 알아보겠다.&lt;/p&gt;

&lt;h2&gt;&lt;em&gt;HBAO+&lt;/em&gt; Pipeline&lt;/h2&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/hbao+_pipeline_with_input_normals.png&quot; alt=&quot;hbao+ with input normal&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://docs.nvidia.com/gameworks/content/gameworkslibrary/visualfx/hbao/index.html&quot;&gt;NVIDIA HBAO+&lt;/a&gt;
&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/hbao+_pipeline_without_input_normals.png&quot; alt=&quot;hbao+ without input normal&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://docs.nvidia.com/gameworks/content/gameworkslibrary/visualfx/hbao/index.html&quot;&gt;NVIDIA HBAO+&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;그림이 두개가 있다. 하나는 &lt;em&gt;GBuffer&lt;/em&gt; 를 사용할 시 &lt;em&gt;World-Space Normal&lt;/em&gt; 버퍼와 &lt;em&gt;Depth Buffer&lt;/em&gt; 를 넘겨주어 계산하는 방식과 입력으로 &lt;em&gt;Depth Buffer&lt;/em&gt; 만 넘겨서 &lt;em&gt;Normal&lt;/em&gt; 데이터를 계산하는 두가지 방식에 대한 파이프라인이다. 두가지의 차이는 &lt;em&gt;Normal&lt;/em&gt; 데이터에 대한 처리방식만 다르다. 나머지 계산은 다를게 없다.&lt;/p&gt;

&lt;h2&gt;Linearize Depths&lt;/h2&gt;

&lt;p&gt;코드를 보면 가장 처음에 시작하는 단계는 바로 &lt;em&gt;Linearize Depths&lt;/em&gt; 다. 이는 꽤나 알려진 방법이다. 하지만 필자는 &lt;em&gt;HBAO+&lt;/em&gt; 를 볼때 처음 봤기에 어느 정도의 설명을 해놓아야겠다. &lt;em&gt;Linearize Depths&lt;/em&gt; 를 알기 위해선 입력된 정점의 위치를 &lt;em&gt;Clipping-Space&lt;/em&gt; 로 변환하는 방법이 어떻게 이루어지는지 알고 있어야 한다.&lt;/p&gt;

&lt;p&gt;일반적인 오브젝트를 렌더링 할때는 &lt;em&gt;Shader&lt;/em&gt; 에 입력으로 들어오는 정점의 기준 공간은 &lt;em&gt;Model-Space&lt;/em&gt;(또는 &lt;em&gt;Local-Space&lt;/em&gt;) &lt;em&gt;Position&lt;/em&gt; 이다. 그래서 &lt;em&gt;MVP&lt;/em&gt; 변환을 통해 &lt;em&gt;Rasterizer&lt;/em&gt; 가 처리할 수 있도록 &lt;em&gt;Clipping-Space&lt;/em&gt; 로 &lt;em&gt;Rasterizer&lt;/em&gt; 로 넘어가기 전에 변환해주어야 한다.(전체적인 내용은 &lt;a href=&quot;https://docs.google.com/presentation/d/10VzsjfifKJlRTHDlBq7e8vNBTu4D5jOWUF87KYYGwlk/edit#slide=id.g25f88339be_0_0&quot;&gt;Model, View, Projection 변환&lt;/a&gt; 에서 확인할 수 있다.
) 그래서 &lt;em&gt;Pixel Shader&lt;/em&gt; 로 넘어간 데이터들은 픽셀별로 들어가고, 픽셀별로 들어간 정점들의 위치는 &lt;em&gt;Clipping-Space&lt;/em&gt; 로 되어있다. 여기까지 이해했으면 아래 그림을 보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/Graphics3D_ClipVolume.png&quot; alt=&quot;frustum vs Clipping&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://www.ntu.edu.sg/home/ehchua/programming/opengl/CG_BasicsTheory.html&quot;&gt;3D Graphics with OpenGL Basic Theory&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위 그림은 &lt;em&gt;View frustum&lt;/em&gt; 과 &lt;em&gt;Clipping Volume&lt;/em&gt; 을 보여준다. &lt;em&gt;View frustum&lt;/em&gt; 은 &lt;em&gt;Perspective&lt;/em&gt; 방식으로 카메라가 실제로 보여주는 공간을 시각화 한것이고, &lt;em&gt;Clipping Volume&lt;/em&gt; 은 &lt;em&gt;MVP&lt;/em&gt; 변환에서 &lt;em&gt;Projection&lt;/em&gt; 행렬을 사용할시 &lt;em&gt;View frustum&lt;/em&gt; 에서 &lt;em&gt;Clipping Volume&lt;/em&gt; 으로 변환되는 볼륨을 시각화 한것이다. &lt;em&gt;Projection&lt;/em&gt; 변환은 아래와 같다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/projection_matrix.png&quot; alt=&quot;perspective projection matrix&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://stackoverflow.com/questions/6652253/getting-the-true-z-value-from-the-depth-buffer
&quot;&gt;Stackoverflow : Getting the true z value from the depth buffer&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Perspective Projection&lt;/em&gt; 은 &lt;em&gt;frustum&lt;/em&gt; 기준 위치를 &lt;em&gt;Cube&lt;/em&gt; 기준 위치로 바꾸는 연산이기 때문에 실제 좌표의 왜곡이 발생한다. 우리는 Z(Detph) 값이 어떤식으로 왜곡되는지 알아야 한다. 우선 &lt;em&gt;Clipping-Space&lt;/em&gt; 로 변환할때, &lt;em&gt;Perspective&lt;/em&gt; 형식의 &lt;em&gt;View frustum&lt;/em&gt; 의 &lt;em&gt;zNear&lt;/em&gt;, &lt;em&gt;zFar&lt;/em&gt; 사이의 Z 값을 [0~1] 값으로 매핑한다. 그러면 &lt;em&gt;zNear&lt;/em&gt;, &lt;em&gt;zFar&lt;/em&gt; 값을에 따라서 실제 좌표가 바뀐다. 그리고 값 자체가 실제 Z 값과 선형적으로 매핑되지 않는다. 아래 그림을 보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/nonlinearDepth.png&quot; alt=&quot;non linear depth&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://computergraphics.stackexchange.com/questions/5116/how-am-i-able-to-perform-perspective-projection-without-a-near-plane&quot;&gt;Computer Graphics StackExchange : How am I able to perform perspective projection without a near plane?&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;그림이 조금 헷갈릴수도 있다. 세로축의 &lt;em&gt;d&lt;/em&gt; 값은 &lt;em&gt;Projection&lt;/em&gt; 을 한 Z, &lt;em&gt;Depth&lt;/em&gt; 값이고 가로축은 &lt;em&gt;World-Space&lt;/em&gt; 의 Z 값이다. 조금 헷갈릴수도 있는 부분은 세로축의 기준값이 윗부분이 0이고 아랫부분이 1이다. 이 부분은 신경써서 봐야한다. 이해했다면 변경된 &lt;em&gt;Depth&lt;/em&gt; 값은 실제 Z 값과 선형적인 관계가 아니고, 실제 Z 값으로 복원하려면 여러 연산을 해야하기에 &lt;em&gt;HBAO+&lt;/em&gt; 에서는 &lt;em&gt;Depth&lt;/em&gt; 값들을 &lt;em&gt;Linearize&lt;/em&gt; 하는 과정을 맨 처음에 넣은 것이다. 실제 Z 값으로 복원하는 이유는 간단하다. &lt;em&gt;Linear&lt;/em&gt; 하지 않은 &lt;em&gt;Depth&lt;/em&gt; 값을 연산시에 사용하면 보다 부정확한 결과가 나오기 때문이다. 특히 &lt;em&gt;SSAO&lt;/em&gt; 연산을 할때는 &lt;em&gt;Depth&lt;/em&gt; 값이 기본이 되기 때문에 해주어야 한다.&lt;/p&gt;

&lt;p&gt;이 단계에서의 결론은 간단하다. &lt;em&gt;Clipping-Space&lt;/em&gt; 의 &lt;em&gt;Depth&lt;/em&gt; 값을 &lt;em&gt;View-Space&lt;/em&gt; 의 Z 값으로 변환하는 단계다. 처리하는 코드는 다른 단계에 비해 짧다. 만약에 넘겨준 &lt;em&gt;Depth&lt;/em&gt; 데이터들이 &lt;em&gt;View-Space&lt;/em&gt; 인 경우에는 옵션을 통해 처리할 수 있다.&lt;/p&gt;

&lt;h2&gt;Deintereaved Texturing&lt;/h2&gt;

&lt;p&gt;위의 그림에는 &lt;em&gt;Generate HBAO+&lt;/em&gt; 라고 단순히 뭉뚱그려서 표현했지만 그 안에는 단순한 &lt;em&gt;Horizon based ambient occlusion(HBAO)&lt;/em&gt; 계산만 있지는 않다. &lt;em&gt;Deintereaved Texturing&lt;/em&gt; 이라는 테크닉과 함께 &lt;em&gt;HBAO&lt;/em&gt; 를 계산한다. &lt;em&gt;Computer Engineering&lt;/em&gt; 분야의 지식을 응용한 이론으로 개인적으로 이 이론을 접했을 떄 꽤나 충격이였다. 자세한 설명은 &lt;a href=&quot;https://developer.nvidia.com/sites/default/files/akamai/gamedev/docs/BAVOIL_ParticleShadowsAndCacheEfficientPost.pdf&quot;&gt;GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing&lt;/a&gt; 슬라이드의 몇장과 함께 보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/gdc2013_ParticleShadowsAndCacheEfficientPost_51.png&quot; alt=&quot;gdc2013_ParticleShadowsAndCacheEfficientPost_51&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://developer.nvidia.com/sites/default/files/akamai/gamedev/docs/BAVOIL_ParticleShadowsAndCacheEfficientPost.pdf&quot;&gt;GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Deintereaved Texturing&lt;/em&gt; 의 방법은 간단하다. 텍스쳐를 여러장으로 나누어 샘플링을 한 후 각각의 나눠진 텍스쳐를 샘플링한 결과를 하나로 합친다. 슬라이드에는 &lt;em&gt;Post-Processing&lt;/em&gt; 을 기준으로 설명이 되어있다. 이점은 생각하면서 보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/gdc2013_ParticleShadowsAndCacheEfficientPost_52.png&quot; alt=&quot;gdc2013_ParticleShadowsAndCacheEfficientPost_52&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://developer.nvidia.com/sites/default/files/akamai/gamedev/docs/BAVOIL_ParticleShadowsAndCacheEfficientPost.pdf&quot;&gt;GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;한 텍스쳐를 여러장으로 나누는건 &lt;em&gt;Multiple Render Target&lt;/em&gt; 을 사용해서 나눈다. 슬라이드는 4개를 기준으로 설명했지만 &lt;em&gt;DirectX10&lt;/em&gt; 부터는 최대 8개까지 지원하기 때문에 16개로 나누어 샘플링한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/gdc2013_ParticleShadowsAndCacheEfficientPost_53.png&quot; alt=&quot;gdc2013_ParticleShadowsAndCacheEfficientPost_53&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://developer.nvidia.com/sites/default/files/akamai/gamedev/docs/BAVOIL_ParticleShadowsAndCacheEfficientPost.pdf&quot;&gt;GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;다음은 나누어진 각각의 텍스쳐를 샘플링하여 원하는 알고리즘으로 결과를 낸다. 조각난 텍스쳐 한개당 한번 &lt;em&gt;DrawCall&lt;/em&gt; 을 걸어준다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/gdc2013_ParticleShadowsAndCacheEfficientPost_54.png&quot; alt=&quot;gdc2013_ParticleShadowsAndCacheEfficientPost_54&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://developer.nvidia.com/sites/default/files/akamai/gamedev/docs/BAVOIL_ParticleShadowsAndCacheEfficientPost.pdf&quot;&gt;GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Deintereave&lt;/em&gt; 를 하기전까지는 넓은 범위의 텍스쳐를 샘플링하여 캐시 효율이 많이 떨어졌지만 텍스쳐를 나누어 각각 할때마다 처리를 하게되니 캐시 효율의 이득을 얻었다. 또한 각각의 &lt;em&gt;DrawCall&lt;/em&gt; 마다 텍스쳐의 용량이 조금만 필요하게 되니 대역폭의 이득도 얻게 된다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/gdc2013_ParticleShadowsAndCacheEfficientPost_55.png&quot; alt=&quot;gdc2013_ParticleShadowsAndCacheEfficientPost_55&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://developer.nvidia.com/sites/default/files/akamai/gamedev/docs/BAVOIL_ParticleShadowsAndCacheEfficientPost.pdf&quot;&gt;GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;한번의 &lt;em&gt;DrawCall&lt;/em&gt; 로 나누어진 결과들을 합친다. &lt;em&gt;Deintereaved Texturing&lt;/em&gt; 은 여기서 끝이다. 실제로 &lt;em&gt;HBAO+&lt;/em&gt; 는 16개의 텍스쳐로 나누어 샘플링한다. &lt;em&gt;Multiple Render Target&lt;/em&gt; 이 8개까지 지원되어 16개로 &lt;em&gt;Deintereave&lt;/em&gt; 하려면 2번 &lt;em&gt;DrawCall&lt;/em&gt; 을 해야한다. 또한 샘플링은 16번 &lt;em&gt;DrawCall&lt;/em&gt; 을 하여 계산한다. 그래서 한번 &lt;em&gt;Deintereaved Texturing&lt;/em&gt; 을 사용하여 &lt;em&gt;Post-Processing&lt;/em&gt; 처리하려면 약 20번의 &lt;em&gt;DrawCall&lt;/em&gt; 을 계산해야 한다. 절대적으로 큰 숫자가 아니기 때문에 크게 신경쓸 필요는 없어보인다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/gdc2013_ParticleShadowsAndCacheEfficientPost_62.png&quot; alt=&quot;gdc2013_ParticleShadowsAndCacheEfficientPost_62&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://developer.nvidia.com/sites/default/files/akamai/gamedev/docs/BAVOIL_ParticleShadowsAndCacheEfficientPost.pdf&quot;&gt;GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;엄청난 성과를 거둔게 보인다. 캐시 히트 확률이 굉장히 올라갔고, 시간도 많이 절약했다. &lt;em&gt;HBAO+&lt;/em&gt; 의 성능향상을 시켜준 것이 이 &lt;em&gt;Deinterleaved Texturing&lt;/em&gt; 인듯하다.&lt;/p&gt;

&lt;h2&gt;Reconstruction of Normal&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;HBAO+&lt;/em&gt; 는 기본적으로 &lt;em&gt;Depth&lt;/em&gt; 와 &lt;em&gt;Normal&lt;/em&gt; 을 통해서 계산한다. 그렇기 때문에 외부에서 &lt;em&gt;Normal&lt;/em&gt; 데이터를 넣어주거나 직접 만들어야 한다. 보통 &lt;em&gt;Deffered Rendering&lt;/em&gt; 을 차용하는 시스템들은 간단하게 &lt;em&gt;GBuffer&lt;/em&gt; 의 &lt;em&gt;Normal&lt;/em&gt; 데이터만 넣어주면 된다. &lt;em&gt;Normal&lt;/em&gt; 데이터를 가져오는 코드가 있으니 조금만 수정하여 사용하면 된다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Normal&lt;/em&gt; 데이터가 없는 경우에는 라이브러리 내에 직접 계산한다. 계산하는 픽셀을 기준으로 상하,좌우별로 &lt;em&gt;Depth&lt;/em&gt; 와 화면상의 좌표계를 이용하여 &lt;em&gt;View-Space&lt;/em&gt; 의 위치를 구한다음 위치가 상하, 좌우별로 가까운 픽셀의 위치 오프셋을 사용해 외적하여 &lt;em&gt;Normal&lt;/em&gt; 값을 구한다.&lt;/p&gt;

&lt;h1&gt;참조 자료&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://docs.nvidia.com/gameworks/content/gameworkslibrary/visualfx/hbao/index.html&quot;&gt;NVidia HBAO+&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://developer.nvidia.com/sites/default/files/akamai/gameworks/samples/DeinterleavedTexturing.pdf&quot;&gt;NVidia : Deintereaved Texturing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://developer.nvidia.com/sites/default/files/akamai/gamedev/docs/BAVOIL_ParticleShadowsAndCacheEfficientPost.pdf&quot;&gt;GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.gdcvault.com/play/1017623/Advanced-Visual-Effects-with-DirectX&quot;&gt;GDCVault : Particle Shadows &amp;amp; Cache-Efficient Post-Processing Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="render" />
      
        <category term="analysis" />
      
        <category term="hbao+" />
      
        <category term="linearize_depth" />
      
        <category term="deintereaved_texturing" />
      

      

      
        <summary type="html">HBAO+ 3.1 버젼을 기준으로 글이 작성되었습니다. 이전 hbao plus analysis 0 글에서 HBAO+ 을 알기위한 기본적인 개념들에 대해서 살펴보았다. 이번 글에서는 HBAO+ 의 구조와 Linearize Depth 와 Deinterleaved Texturing 에 대해서 알아보겠다. HBAO+ Pipeline 출처 : NVIDIA HBAO+ 출처 : NVIDIA HBAO+ 그림이 두개가 있다. 하나는 GBuffer 를 사용할 시 World-Space Normal 버퍼와 Depth Buffer 를 넘겨주어 계산하는 방식과 입력으로 Depth Buffer 만 넘겨서 Normal 데이터를 계산하는 두가지 방식에 대한 파이프라인이다. 두가지의 차이는 Normal 데이터에 대한 처리방식만 다르다. 나머지 계산은 다를게 없다. Linearize Depths 코드를 보면 가장 처음에 시작하는 단계는 바로 Linearize Depths 다. 이는 꽤나 알려진 방법이다. 하지만 필자는 HBAO+ 를 볼때 처음 봤기에 어느 정도의 설명을 해놓아야겠다. Linearize Depths 를 알기 위해선 입력된 정점의 위치를 Clipping-Space 로 변환하는 방법이 어떻게 이루어지는지 알고 있어야 한다. 일반적인 오브젝트를 렌더링 할때는 Shader 에 입력으로 들어오는 정점의 기준 공간은 Model-Space(또는 Local-Space) Position 이다. 그래서 MVP 변환을 통해 Rasterizer 가 처리할 수 있도록 Clipping-Space 로 Rasterizer 로 넘어가기 전에 변환해주어야 한다.(전체적인 내용은 Model, View, Projection 변환 에서 확인할 수 있다. ) 그래서 Pixel Shader 로 넘어간 데이터들은 픽셀별로 들어가고, 픽셀별로 들어간 정점들의 위치는 Clipping-Space 로 되어있다. 여기까지 이해했으면 아래 그림을 보자. 출처 : 3D Graphics with OpenGL Basic Theory 위 그림은 View frustum 과 Clipping Volume 을 보여준다. View frustum 은 Perspective 방식으로 카메라가 실제로 보여주는 공간을 시각화 한것이고, Clipping Volume 은 MVP 변환에서 Projection 행렬을 사용할시 View frustum 에서 Clipping Volume 으로 변환되는 볼륨을 시각화 한것이다. Projection 변환은 아래와 같다. 출처 : Stackoverflow : Getting the true z value from the depth buffer Perspective Projection 은 frustum 기준 위치를 Cube 기준 위치로 바꾸는 연산이기 때문에 실제 좌표의 왜곡이 발생한다. 우리는 Z(Detph) 값이 어떤식으로 왜곡되는지 알아야 한다. 우선 Clipping-Space 로 변환할때, Perspective 형식의 View frustum 의 zNear, zFar 사이의 Z 값을 [0~1] 값으로 매핑한다. 그러면 zNear, zFar 값을에 따라서 실제 좌표가 바뀐다. 그리고 값 자체가 실제 Z 값과 선형적으로 매핑되지 않는다. 아래 그림을 보자. 출처 : Computer Graphics StackExchange : How am I able to perform perspective projection without a near plane? 그림이 조금 헷갈릴수도 있다. 세로축의 d 값은 Projection 을 한 Z, Depth 값이고 가로축은 World-Space 의 Z 값이다. 조금 헷갈릴수도 있는 부분은 세로축의 기준값이 윗부분이 0이고 아랫부분이 1이다. 이 부분은 신경써서 봐야한다. 이해했다면 변경된 Depth 값은 실제 Z 값과 선형적인 관계가 아니고, 실제 Z 값으로 복원하려면 여러 연산을 해야하기에 HBAO+ 에서는 Depth 값들을 Linearize 하는 과정을 맨 처음에 넣은 것이다. 실제 Z 값으로 복원하는 이유는 간단하다. Linear 하지 않은 Depth 값을 연산시에 사용하면 보다 부정확한 결과가 나오기 때문이다. 특히 SSAO 연산을 할때는 Depth 값이 기본이 되기 때문에 해주어야 한다. 이 단계에서의 결론은 간단하다. Clipping-Space 의 Depth 값을 View-Space 의 Z 값으로 변환하는 단계다. 처리하는 코드는 다른 단계에 비해 짧다. 만약에 넘겨준 Depth 데이터들이 View-Space 인 경우에는 옵션을 통해 처리할 수 있다. Deintereaved Texturing 위의 그림에는 Generate HBAO+ 라고 단순히 뭉뚱그려서 표현했지만 그 안에는 단순한 Horizon based ambient occlusion(HBAO) 계산만 있지는 않다. Deintereaved Texturing 이라는 테크닉과 함께 HBAO 를 계산한다. Computer Engineering 분야의 지식을 응용한 이론으로 개인적으로 이 이론을 접했을 떄 꽤나 충격이였다. 자세한 설명은 GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing 슬라이드의 몇장과 함께 보자. 출처 : GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing Deintereaved Texturing 의 방법은 간단하다. 텍스쳐를 여러장으로 나누어 샘플링을 한 후 각각의 나눠진 텍스쳐를 샘플링한 결과를 하나로 합친다. 슬라이드에는 Post-Processing 을 기준으로 설명이 되어있다. 이점은 생각하면서 보자. 출처 : GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing 한 텍스쳐를 여러장으로 나누는건 Multiple Render Target 을 사용해서 나눈다. 슬라이드는 4개를 기준으로 설명했지만 DirectX10 부터는 최대 8개까지 지원하기 때문에 16개로 나누어 샘플링한다. 출처 : GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing 다음은 나누어진 각각의 텍스쳐를 샘플링하여 원하는 알고리즘으로 결과를 낸다. 조각난 텍스쳐 한개당 한번 DrawCall 을 걸어준다. 출처 : GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing Deintereave 를 하기전까지는 넓은 범위의 텍스쳐를 샘플링하여 캐시 효율이 많이 떨어졌지만 텍스쳐를 나누어 각각 할때마다 처리를 하게되니 캐시 효율의 이득을 얻었다. 또한 각각의 DrawCall 마다 텍스쳐의 용량이 조금만 필요하게 되니 대역폭의 이득도 얻게 된다. 출처 : GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing 한번의 DrawCall 로 나누어진 결과들을 합친다. Deintereaved Texturing 은 여기서 끝이다. 실제로 HBAO+ 는 16개의 텍스쳐로 나누어 샘플링한다. Multiple Render Target 이 8개까지 지원되어 16개로 Deintereave 하려면 2번 DrawCall 을 해야한다. 또한 샘플링은 16번 DrawCall 을 하여 계산한다. 그래서 한번 Deintereaved Texturing 을 사용하여 Post-Processing 처리하려면 약 20번의 DrawCall 을 계산해야 한다. 절대적으로 큰 숫자가 아니기 때문에 크게 신경쓸 필요는 없어보인다. 출처 : GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing 엄청난 성과를 거둔게 보인다. 캐시 히트 확률이 굉장히 올라갔고, 시간도 많이 절약했다. HBAO+ 의 성능향상을 시켜준 것이 이 Deinterleaved Texturing 인듯하다. Reconstruction of Normal HBAO+ 는 기본적으로 Depth 와 Normal 을 통해서 계산한다. 그렇기 때문에 외부에서 Normal 데이터를 넣어주거나 직접 만들어야 한다. 보통 Deffered Rendering 을 차용하는 시스템들은 간단하게 GBuffer 의 Normal 데이터만 넣어주면 된다. Normal 데이터를 가져오는 코드가 있으니 조금만 수정하여 사용하면 된다. Normal 데이터가 없는 경우에는 라이브러리 내에 직접 계산한다. 계산하는 픽셀을 기준으로 상하,좌우별로 Depth 와 화면상의 좌표계를 이용하여 View-Space 의 위치를 구한다음 위치가 상하, 좌우별로 가까운 픽셀의 위치 오프셋을 사용해 외적하여 Normal 값을 구한다. 참조 자료 NVidia HBAO+ NVidia : Deintereaved Texturing GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing GDCVault : Particle Shadows &amp;amp; Cache-Efficient Post-Processing Video</summary>
      

      
      
    </entry>
  
  
</feed>

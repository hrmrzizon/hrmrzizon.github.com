<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="ko">
  <generator uri="http://jekyllrb.com" version="3.6.2">Jekyll</generator>
  
  
  <link href="https://hrmrzizon.github.io/feed.xml" rel="self" type="application/atom+xml" />
  <link href="https://hrmrzizon.github.io/" rel="alternate" type="text/html" hreflang="ko" />
  <updated>2017-12-24T06:23:05+00:00</updated>
  <id>https://hrmrzizon.github.io//</id>

  
    <title type="html">Appocrypha</title>
  

  
    <subtitle>store limitless knowledges</subtitle>
  

  
    <author>
        <name>Su-Hyeok Kim</name>
      
      
    </author>
  

  
  
    <entry>
      
      <title type="html">Cascaded Shadow Mapping</title>
      
      <link href="https://hrmrzizon.github.io/2017/12/17/cascaded-shadow-mapping/" rel="alternate" type="text/html" title="Cascaded Shadow Mapping" />
      <published>2017-12-17T00:00:00+00:00</published>
      <updated>2017-12-17T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2017/12/17/cascaded-shadow-mapping</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2017/12/17/cascaded-shadow-mapping/">&lt;p&gt;&lt;a href=&quot;{ post_url 2017-11-30-what-is-shadow-mapping }&quot;&gt;What is Shadow Mapping&lt;/a&gt; 에서 &lt;em&gt;Shadow Mapping&lt;/em&gt; 에 대한 간단한 번역 &amp;amp; 설명을 적어놓았다. 이번 글에서는 &lt;em&gt;Shadow Mapping&lt;/em&gt; 을 효과적으로 사용하기 위한 &lt;em&gt;Cascaded Shadow Mapping&lt;/em&gt; 에 대하여 적어보겠다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Cascaded Shadow Mapping&lt;/em&gt; 을 구글 번역기에 돌려보면 &lt;em&gt;“계단식 그림자 매핑”&lt;/em&gt; 이라고 나온다. 조금 직관적이지 않은 말이지만 뜻 자체는 맞다. 간단하게 &lt;em&gt;Cascaded Shadow Mapping&lt;/em&gt; 에 대하여 말하자면 넓은 환경의 그림자를 위해 거리에(거의 &lt;em&gt;Depth&lt;/em&gt;) 따라서 여러개의 &lt;em&gt;Shadow Map&lt;/em&gt; 을 생성하는 방법이다.&lt;/p&gt;

&lt;p&gt;넓은 범위의 &lt;em&gt;Directional Light&lt;/em&gt; 가 닿는 그림자를 정확하게 표현하려면 꽤나 큰 크기의 &lt;em&gt;Shadow Map&lt;/em&gt; 을 사용해야 한다. 하지만 &lt;em&gt;Cascaded Shadow Mapping&lt;/em&gt; 을 사용한다면 여러개의 &lt;em&gt;Shadow Map&lt;/em&gt; 을 사용하여 보다 조금의 메모리를 사용하여 넓은 범위의 그림자를 표현할 수 있다.&lt;/p&gt;

&lt;h2&gt;Shadow-map generation&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Cascaded Shadow Mapping&lt;/em&gt; 을 위한 &lt;em&gt;Shadow Map&lt;/em&gt; 생성은 앞서쓴 &lt;a href=&quot;{ post_url 2017-11-30-what-is-shadow-mapping }&quot;&gt;글&lt;/a&gt;에서 설명한 방법과 거의 유사하다. 앞서 여러개의 &lt;em&gt;Shadow Map&lt;/em&gt; 을 생성하여 그림자를 표현한다고 언급했었다. 여러개의 &lt;em&gt;Shaodw Map&lt;/em&gt; 을 생성하는 기준은 &lt;em&gt;View Frustom&lt;/em&gt; 을 &lt;em&gt;Depth&lt;/em&gt; 를 기준으로 여러개로 쪼개어 각 쪼개진 &lt;em&gt;Frustom&lt;/em&gt; 을 기준으로 &lt;em&gt;Shadow Map&lt;/em&gt; 을 그린다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Frustom&lt;/em&gt; 은 보통 &lt;em&gt;Depth&lt;/em&gt; 값을 정하거나 어떤 알고리즘을 사용하여 쪼갠다. 이는 다음 포스팅에서 언급할 예정이다. &lt;em&gt;Frustom&lt;/em&gt; 을 쪼개주면 다음은 쪼개진 &lt;em&gt;Camera View Frustom&lt;/em&gt; 의 각각의 8개의 꼭지점들을 &lt;em&gt;Light-Space&lt;/em&gt; 로 변환한다. 변환된 각각 꼭지점으로 2차원의 &lt;em&gt;aligned axis bounding box&lt;/em&gt; 의 위치를 구해준다. 가장 작은 X,Y 값과 가장 큰 X, Y 값을 구해주면 된다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/CSM_EffectOfCropMatrix.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://developer.download.nvidia.com/SDK/10.5/opengl/src/cascaded_shadow_maps/doc/cascaded_shadow_maps.pdf&quot;&gt;NVidia : Cascaded Shadow Mapping&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위 그림에서 XY 평면에서의 빨간색 선으로 되어있는 사각형이 언급한 &lt;em&gt;aligned axis bounding box&lt;/em&gt; 를 말한다. 이 &lt;em&gt;AABB&lt;/em&gt; 는 아래에서 특정한 행렬을 만들때 쓰인다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://developer.download.nvidia.com/SDK/10.5/opengl/src/cascaded_shadow_maps/doc/cascaded_shadow_maps.pdf&quot;&gt;NVidia : Cascaded Shadow Maps  &lt;/a&gt; 에서는 이 &lt;em&gt;Light-Space&lt;/em&gt; 로 변환하는 &lt;em&gt;MVP 변환&lt;/em&gt; 에서 &lt;em&gt;Projection&lt;/em&gt; 변환을 바꿔준다고 설명한다. 두개의 행렬이 나오는데, 하나는 직교 투영 행렬로(&lt;em&gt;orthogonal projection&lt;/em&gt;) 나눠진 &lt;em&gt;Frustom&lt;/em&gt; 의 &lt;em&gt;Far&lt;/em&gt; 값과 &lt;em&gt;Near&lt;/em&gt; 값을 통해 생성해준다. 그리고 나머지 하나는 &lt;em&gt;Crop Matrix&lt;/em&gt; 라는 변환 행렬이다.&lt;/p&gt;

&lt;p&gt;위에서 구한 &lt;em&gt;Light-Space&lt;/em&gt; 의 &lt;em&gt;AABB&lt;/em&gt; 값을 통해 &lt;em&gt;Crop Matrix&lt;/em&gt; 를 계산한다. 아래 그림에서나오는 Mx, My 와 mx, my 는 각각 Maximum X,Y, Minimum X,Y 를 뜻한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/CSM_CropMatrixCalc.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://developer.download.nvidia.com/SDK/10.5/opengl/src/cascaded_shadow_maps/doc/cascaded_shadow_maps.pdf&quot;&gt;NVidia : Cascaded Shadow Mapping&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 계산된 &lt;em&gt;Crop Matrix&lt;/em&gt; 의 역할은 해당 &lt;em&gt;AABB&lt;/em&gt; 로 &lt;em&gt;Shadow Map&lt;/em&gt; 이 그려질 범위를 결정해주는 역할을 한다. 다만 범위가 아주 정확하지는 않다. 아래 그림을 보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/CSM_FarMiddleNear.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://ogldev.atspace.co.uk/www/tutorial49/tutorial49.html&quot;&gt;OGLdev : Cascaded Shadow Mapping&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위 그림과 같이 보통은 겹치는 부분이 생긴다. 사용시에는 &lt;em&gt;Depth&lt;/em&gt; 에 따라서 다르게 사용하기 때문에 크게 문제는 없다. 사용시에는 &lt;em&gt;Depth&lt;/em&gt; 값에 따라서 다른 텍스쳐를 가져오는 것과 텍스쳐를 샘플링할때 UV 값을 정점의 위치를 &lt;em&gt;Light-Space&lt;/em&gt; 로 변환해서 변환된 정점의 위치의 X,Y 좌표를 UV 값으로 사용하면 된다. 다만 각각의 &lt;em&gt;Shadow Map&lt;/em&gt; 마다 변환 행렬은 &lt;em&gt;Crop Matrix&lt;/em&gt; 때문에 다르기 때문에 따로 접근해야 한다.&lt;/p&gt;

&lt;p&gt;자세한 사용법을 알고 싶으면 &lt;a href=&quot;http://developer.download.nvidia.com/SDK/10/Samples/cascaded_shadow_maps.zip&quot;&gt;NVidia : Cascaded Shadow Map Example&lt;/a&gt;에서 소스를 받아 보면 된다.&lt;/p&gt;

&lt;h2&gt;참조&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://developer.download.nvidia.com/SDK/10.5/opengl/src/cascaded_shadow_maps/doc/cascaded_shadow_maps.pdf&quot;&gt;NVidia : Cascaded Shadow Maps  &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/ee416307.aspx&quot;&gt;MSDN : Cascaded Shadow Maps&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/TheRealMJP/Shadows&quot;&gt;Github : TheRealMJP - Shadows&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://ogldev.atspace.co.uk/www/tutorial49/tutorial49.html&quot;&gt;OGLDev : Cascaded Shadow Mapping&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="shader" />
      
        <category term="shadow" />
      
        <category term="rendering" />
      
        <category term="csm" />
      

      

      
        <summary type="html">What is Shadow Mapping 에서 Shadow Mapping 에 대한 간단한 번역 &amp;amp; 설명을 적어놓았다. 이번 글에서는 Shadow Mapping 을 효과적으로 사용하기 위한 Cascaded Shadow Mapping 에 대하여 적어보겠다. Cascaded Shadow Mapping 을 구글 번역기에 돌려보면 “계단식 그림자 매핑” 이라고 나온다. 조금 직관적이지 않은 말이지만 뜻 자체는 맞다. 간단하게 Cascaded Shadow Mapping 에 대하여 말하자면 넓은 환경의 그림자를 위해 거리에(거의 Depth) 따라서 여러개의 Shadow Map 을 생성하는 방법이다. 넓은 범위의 Directional Light 가 닿는 그림자를 정확하게 표현하려면 꽤나 큰 크기의 Shadow Map 을 사용해야 한다. 하지만 Cascaded Shadow Mapping 을 사용한다면 여러개의 Shadow Map 을 사용하여 보다 조금의 메모리를 사용하여 넓은 범위의 그림자를 표현할 수 있다. Shadow-map generation Cascaded Shadow Mapping 을 위한 Shadow Map 생성은 앞서쓴 글에서 설명한 방법과 거의 유사하다. 앞서 여러개의 Shadow Map 을 생성하여 그림자를 표현한다고 언급했었다. 여러개의 Shaodw Map 을 생성하는 기준은 View Frustom 을 Depth 를 기준으로 여러개로 쪼개어 각 쪼개진 Frustom 을 기준으로 Shadow Map 을 그린다. Frustom 은 보통 Depth 값을 정하거나 어떤 알고리즘을 사용하여 쪼갠다. 이는 다음 포스팅에서 언급할 예정이다. Frustom 을 쪼개주면 다음은 쪼개진 Camera View Frustom 의 각각의 8개의 꼭지점들을 Light-Space 로 변환한다. 변환된 각각 꼭지점으로 2차원의 aligned axis bounding box 의 위치를 구해준다. 가장 작은 X,Y 값과 가장 큰 X, Y 값을 구해주면 된다. 출처 : NVidia : Cascaded Shadow Mapping 위 그림에서 XY 평면에서의 빨간색 선으로 되어있는 사각형이 언급한 aligned axis bounding box 를 말한다. 이 AABB 는 아래에서 특정한 행렬을 만들때 쓰인다. NVidia : Cascaded Shadow Maps 에서는 이 Light-Space 로 변환하는 MVP 변환 에서 Projection 변환을 바꿔준다고 설명한다. 두개의 행렬이 나오는데, 하나는 직교 투영 행렬로(orthogonal projection) 나눠진 Frustom 의 Far 값과 Near 값을 통해 생성해준다. 그리고 나머지 하나는 Crop Matrix 라는 변환 행렬이다. 위에서 구한 Light-Space 의 AABB 값을 통해 Crop Matrix 를 계산한다. 아래 그림에서나오는 Mx, My 와 mx, my 는 각각 Maximum X,Y, Minimum X,Y 를 뜻한다. 출처 : NVidia : Cascaded Shadow Mapping 이렇게 계산된 Crop Matrix 의 역할은 해당 AABB 로 Shadow Map 이 그려질 범위를 결정해주는 역할을 한다. 다만 범위가 아주 정확하지는 않다. 아래 그림을 보자. 출처 : OGLdev : Cascaded Shadow Mapping 위 그림과 같이 보통은 겹치는 부분이 생긴다. 사용시에는 Depth 에 따라서 다르게 사용하기 때문에 크게 문제는 없다. 사용시에는 Depth 값에 따라서 다른 텍스쳐를 가져오는 것과 텍스쳐를 샘플링할때 UV 값을 정점의 위치를 Light-Space 로 변환해서 변환된 정점의 위치의 X,Y 좌표를 UV 값으로 사용하면 된다. 다만 각각의 Shadow Map 마다 변환 행렬은 Crop Matrix 때문에 다르기 때문에 따로 접근해야 한다. 자세한 사용법을 알고 싶으면 NVidia : Cascaded Shadow Map Example에서 소스를 받아 보면 된다. 참조 NVidia : Cascaded Shadow Maps MSDN : Cascaded Shadow Maps Github : TheRealMJP - Shadows OGLDev : Cascaded Shadow Mapping</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">What Is Shadow Mapping</title>
      
      <link href="https://hrmrzizon.github.io/2017/11/30/what-is-shadow-mapping/" rel="alternate" type="text/html" title="What Is Shadow Mapping" />
      <published>2017-11-30T00:00:00+00:00</published>
      <updated>2017-11-30T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2017/11/30/what-is-shadow-mapping</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2017/11/30/what-is-shadow-mapping/">&lt;p&gt;※ 이 글은 &lt;a href=&quot;http://www.opengl-tutorial.org/intermediate-tutorials/tutorial-16-shadow-mapping/&quot;&gt;opengl-tutorial : shadow mapping&lt;/a&gt; 게시물을 참고하여 쓰여졌습니다. 자세한 내용은 원문을 보는게 좋습니다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Shadow Mapping&lt;/em&gt; 실시간으로 그림자를 구현하기 위한 방법 중에 가장 널리 알려진 방법이다. 다른 방법들보다 구현하기 조금 쉬운편이긴 하나 이 방법은 완벽하지가 않기 때문에 방법 자체로는 완벽한 모습을 보이기 어렵고 다른 방법과 같이 사용하여 부족한 부분을 보완하여 사용해야 한다.&lt;/p&gt;

&lt;p&gt;일반적으로 &lt;em&gt;Shadow Mapping&lt;/em&gt; 이라 말하면 아는 사람은 머릿속에 쉽게 떠오르는 방식이 있다. 빛의 반대쪽 방향에서 충분히 멀리 떨어져 한번 오브젝트를 그린다. 이때 &lt;em&gt;Pixel Shader&lt;/em&gt; 를 null 로 설정해서 &lt;em&gt;Depth Buffer&lt;/em&gt; 의 데이터만 가져온다. 또는 &lt;em&gt;Pixel Shader&lt;/em&gt; 의 출력을 &lt;em&gt;Depth&lt;/em&gt; 로 해도 된다. 그러면 보통 아래와 비슷한 2D 텍스쳐를 얻게 된다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/OGLTuto_DepthTexture.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://www.opengl-tutorial.org/intermediate-tutorials/tutorial-16-shadow-mapping/&quot;&gt;opengl-tutorial&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;검은색에 가까워질수록(0에 가까워질수록) 해당 오브젝트의 위치가 가깝고, 흰색에 가까워질수록(1에 가까워질수록) 물체가 먼것이다. 오브젝트의 &lt;em&gt;Depth&lt;/em&gt; 를 렌더링할 때 정점에 사용되는 &lt;em&gt;MVP&lt;/em&gt; 변환 중 &lt;em&gt;View&lt;/em&gt; 변환은 임의의 위치와 빛의 방향을 계산하여 적용해준다. &lt;em&gt;Camera&lt;/em&gt; 를 기준으로 한게 아닌 &lt;em&gt;Light&lt;/em&gt; 의 방향을 기준으로 하여 관련된 것을 &lt;em&gt;Light-Space&lt;/em&gt; 라고 명명하는 경우도 더러 있다.&lt;/p&gt;

&lt;p&gt;이제 생성된 &lt;em&gt;Shadow Map&lt;/em&gt; 을 사용하는 방법에 대해 알아보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/OGLTuto_lightandshadow.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://www.opengl-tutorial.org/intermediate-tutorials/tutorial-16-shadow-mapping/&quot;&gt;opengl-tutorial&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위 그림에서 노란색으로 보이는 표면은 빛이 닿는 부분이고, 검은색으로 보이는 표면은 어떤 오브젝트에 의해 가려져 그림자가 드리운 표면이다. 해당 그림 위의 &lt;em&gt;Depth Buffer&lt;/em&gt; 를 응용하여 위처럼 가려지는 표면과 안가려지는 표면을 알아낼 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Depth Buffer&lt;/em&gt; 는 &lt;em&gt;Light-Space&lt;/em&gt; 를 기준으로 데이터를 저장하고 있다. 그리고 &lt;em&gt;Shader&lt;/em&gt; 에서는 &lt;em&gt;Local-Space&lt;/em&gt; 로 정점의 위치가 들어오기 때문에 &lt;em&gt;Depth&lt;/em&gt; 값을 비교하려면 두 값을 같은 공간으로 맞춰주어야 한다. &lt;a href=&quot;http://www.opengl-tutorial.org/intermediate-tutorials/tutorial-16-shadow-mapping/&quot;&gt;OpenGL Tutorial : Shadow Mapping&lt;/a&gt; 에서는 &lt;em&gt;bias&lt;/em&gt; 행렬과 &lt;em&gt;Light-Space&lt;/em&gt; 가 적용된 행렬을 합성하여 입력으로 들어온 정점 데이터를 &lt;em&gt;Light-Space&lt;/em&gt; 기준으로 바꿔준다.&lt;/p&gt;

&lt;p&gt;그 다음 정점의 &lt;em&gt;Depth&lt;/em&gt;(&lt;em&gt;Z&lt;/em&gt;) 값과 &lt;em&gt;Depth Buffer&lt;/em&gt; 에서 샘플링한 &lt;em&gt;Depth&lt;/em&gt;(&lt;em&gt;Z&lt;/em&gt;) 값을 비교하여 현재 정점의 &lt;em&gt;Depth&lt;/em&gt; 값이 더 크면(멀면) 그림자를 적용시킨다. 이러면 기본적인 &lt;em&gt;Shadwo Mapping&lt;/em&gt; 의 이론은 끝이다. 아래 간단한 &lt;em&gt;GLSL&lt;/em&gt; 코드가 있다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C&quot;&gt;vec4 ShadowCoord = DepthBiasMVP * vec4(vertexPosition_modelspace, 1);

float visibility = 1.0;

if (texture( shadowMap, ShadowCoord.xy ).z &amp;lt; ShadowCoord.z) {
    visibility = 0.5;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;정점의 위치를 변환시키고, &lt;em&gt;Depth&lt;/em&gt; 값에 따라 &lt;em&gt;visibility&lt;/em&gt; 값을 변경시켜 그림자를 적용시킨다. 하지만 위에서도 언급했지만 &lt;em&gt;Shadow Mapping&lt;/em&gt; 자체에는 조금 문제가 있다고 언급했다. 해당 코드의 결과를 보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/OGLTuto_1rstTry.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://www.opengl-tutorial.org/intermediate-tutorials/tutorial-16-shadow-mapping/&quot;&gt;opengl-tutorial&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위 그림은 굉장히 난장판이다. 세가지의 문제가 있는데 사진의 전체를 봐도 쉽게 알 수 있는 빛이 닿는 영역이 그림자 처리되는 것, &lt;em&gt;Shadow acne&lt;/em&gt; 가 생겼다고 말한다. 그리고 왼쪽아래 구석부분에 아주 조금 빛이 들어오는 것처럼 처리되는 것이 있다. 이는 &lt;em&gt;Peter Panning&lt;/em&gt; 이라고 부른다. 그리고 마지막으로 그림자와 빛이 닿는 부분의 경계가 울퉁불퉁한게 보일 것이다. 이를 계단현상, &lt;em&gt;aliasing&lt;/em&gt; 이라고 부르는데 흔히 게임에서 적용되는 &lt;em&gt;antialiasing&lt;/em&gt; 의 반대말이 맞다.&lt;/p&gt;

&lt;p&gt;첫번째로 해결할 문제는 &lt;em&gt;Shadow acne&lt;/em&gt; 다. 이 문제는 아래 그림을 보면 쉽게 이해가 된다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/OGLTuto_shadow-acne.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://www.opengl-tutorial.org/intermediate-tutorials/tutorial-16-shadow-mapping/&quot;&gt;opengl-tutorial&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;사선으로 나와있는 노란색 선들은 &lt;em&gt;Shadow Map&lt;/em&gt; 을 기준으로 &lt;em&gt;Light-Space&lt;/em&gt; 로 변환한 정점의 &lt;em&gt;Depth&lt;/em&gt; 값의 기준을 뜻한다. 그리고 표면 자체는 &lt;em&gt;Shadow Map&lt;/em&gt; 의 기준이 된다. 그림의 검은색 부분은 빛이 닿는 부분임에도 불구하고 그림자로 처리되는 부분인데, 이를 없에기 위해서는 값을 비교할때 단순하게 &lt;em&gt;bias&lt;/em&gt; 를 더해주면 된다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C&quot;&gt;float bias = 0.005;
float visibility = 1.0;

if (texture( shadowMap, ShadowCoord.xy ).z &amp;lt; ShadowCoord.z-bias) {
    visibility = 0.5;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;이렇게 적용시키면 평면에서의 &lt;em&gt;acne&lt;/em&gt; 들은 제거가 가능하지만 곡면에서의 &lt;em&gt;acne&lt;/em&gt; 들이 제거가 안되기 때문에 &lt;em&gt;bias&lt;/em&gt; 를 조금 수정해준다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C&quot;&gt;float bias = 0.005*tan(acos(cosTheta)); // cosTheta is dot( n,l ), clamped between 0 and 1
bias = clamp(bias, 0,0.01);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;이러면 &lt;em&gt;Shadow acne&lt;/em&gt; 들은 제거된다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/OGLTuto_VariableBias.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://www.opengl-tutorial.org/intermediate-tutorials/tutorial-16-shadow-mapping/&quot;&gt;opengl-tutorial&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;다음은 &lt;em&gt;Peter Panning&lt;/em&gt; 을 언급할 차례다. &lt;a href=&quot;http://www.opengl-tutorial.org/intermediate-tutorials/tutorial-16-shadow-mapping/&quot;&gt;OpenGL Tutorial&lt;/a&gt; 에서는 이 문제의 해결책으로 굉장히 단순한 방법을 제시한다. &lt;em&gt;Peter Panning&lt;/em&gt; 이 생기지 않도록 충분히 두꺼운 오브젝트를 배치하는 것이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/OGLTuto_NoPeterPanning.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://www.opengl-tutorial.org/intermediate-tutorials/tutorial-16-shadow-mapping/&quot;&gt;opengl-tutorial&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 쉽게 해결된다.&lt;/p&gt;

&lt;p&gt;마지막으로 다룰 문제는 &lt;em&gt;aliasing&lt;/em&gt; 이다. 이는 &lt;em&gt;Shadow Mapping&lt;/em&gt; 의 고질적인 문제로써 &lt;em&gt;anti-alisasing&lt;/em&gt; 기법을 통해 해결해왔다.&lt;/p&gt;

&lt;p&gt;첫번째로 &lt;em&gt;Shadow Map&lt;/em&gt; 을 샘플링할 때 일반적인 색을 가져오는 샘플링이 아닌 다른 방식을 사용한다. &lt;em&gt;Shadow Map&lt;/em&gt; 을 한번 샘플링할 때 하드웨어에서 주변의 텍셀을 샘플링해 주변 텍셀과 비교를 수행해 모든 비교결과를 이중선형 보간을 적용한 결과를 주는 샘플링 방식을 사용한다고 한다. 만약 이중선형 보간을 사용하지 않는다면 &lt;em&gt;Point Sampling&lt;/em&gt; 을 여러번 하여 결과들을 사용해 &lt;em&gt;PCF&lt;/em&gt; 를 적용시켜주면 된다. 이렇게 해주면 조금 부드러운 결과가 나오게 된다.&lt;/p&gt;

&lt;p&gt;하지만 이로써는 만족할만한 결과를 얻을 수 없어 주변을 여러번 샘플링해 값을 가져온다. 미리 생성된 &lt;em&gt;offset&lt;/em&gt; 을 사용해 기준 &lt;em&gt;UV&lt;/em&gt; 주변을 샘플링한다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C&quot;&gt;for (int i=0;i&amp;lt;4;i++){
  if ( texture( shadowMap, ShadowCoord.xy + poissonDisk[i]/700.0 ).z  &amp;lt;  ShadowCoord.z-bias ){
    visibility-=0.2;
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;미리 생성된 &lt;em&gt;offset&lt;/em&gt; 은 &lt;em&gt;Poisson Disc&lt;/em&gt; 방식으로 생성된듯하다. &lt;em&gt;visibility&lt;/em&gt; 변수는 색의 어두움을 결정하는 변수로 한번 &lt;em&gt;Depth Test&lt;/em&gt; 에 걸리면 0.2를 줄여 0.2 ~ 1 사이의 값을 가진다.&lt;/p&gt;

&lt;p&gt;이렇게 두가지 방식으로 &lt;em&gt;anti-aliasing&lt;/em&gt; 을 해주면 제법 그럴듯한 결과가 나온다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/OGLTuto_SoftShadows_Wide.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://www.opengl-tutorial.org/intermediate-tutorials/tutorial-16-shadow-mapping/&quot;&gt;opengl-tutorial&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;또한 &lt;em&gt;UV&lt;/em&gt; 좌표에 &lt;em&gt;offset&lt;/em&gt; 을 주는 방법은 꽤나 많다. 위의 방법은 랜덤으로 고정된 부분만 체크하지만 이 방법에 임의로 &lt;em&gt;offset&lt;/em&gt; 돌려주는 방법도 있다.&lt;/p&gt;

&lt;h2&gt;참조&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.opengl-tutorial.org/kr/intermediate-tutorials/tutorial-16-shadow-mapping/&quot;&gt;OpenGL Tutorial : Tutorial 16 Shadow mapping&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://ogldev.atspace.co.uk/www/tutorial42/tutorial42.html&quot;&gt;OGLdev : Percentage Closer Filtering&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://developer.nvidia.com/gpugems/GPUGems/gpugems_ch11.html&quot;&gt;GPU Gems : Chapter 11. Shadow Map Antialiasing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Supersampling#Poisson_disc&quot;&gt;Wikipedia : SuperSampling#poisson_disc&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="shader" />
      
        <category term="shadow" />
      
        <category term="rendering" />
      

      

      
        <summary type="html">※ 이 글은 opengl-tutorial : shadow mapping 게시물을 참고하여 쓰여졌습니다. 자세한 내용은 원문을 보는게 좋습니다. Shadow Mapping 실시간으로 그림자를 구현하기 위한 방법 중에 가장 널리 알려진 방법이다. 다른 방법들보다 구현하기 조금 쉬운편이긴 하나 이 방법은 완벽하지가 않기 때문에 방법 자체로는 완벽한 모습을 보이기 어렵고 다른 방법과 같이 사용하여 부족한 부분을 보완하여 사용해야 한다. 일반적으로 Shadow Mapping 이라 말하면 아는 사람은 머릿속에 쉽게 떠오르는 방식이 있다. 빛의 반대쪽 방향에서 충분히 멀리 떨어져 한번 오브젝트를 그린다. 이때 Pixel Shader 를 null 로 설정해서 Depth Buffer 의 데이터만 가져온다. 또는 Pixel Shader 의 출력을 Depth 로 해도 된다. 그러면 보통 아래와 비슷한 2D 텍스쳐를 얻게 된다. 출처 : opengl-tutorial 검은색에 가까워질수록(0에 가까워질수록) 해당 오브젝트의 위치가 가깝고, 흰색에 가까워질수록(1에 가까워질수록) 물체가 먼것이다. 오브젝트의 Depth 를 렌더링할 때 정점에 사용되는 MVP 변환 중 View 변환은 임의의 위치와 빛의 방향을 계산하여 적용해준다. Camera 를 기준으로 한게 아닌 Light 의 방향을 기준으로 하여 관련된 것을 Light-Space 라고 명명하는 경우도 더러 있다. 이제 생성된 Shadow Map 을 사용하는 방법에 대해 알아보자. 출처 : opengl-tutorial 위 그림에서 노란색으로 보이는 표면은 빛이 닿는 부분이고, 검은색으로 보이는 표면은 어떤 오브젝트에 의해 가려져 그림자가 드리운 표면이다. 해당 그림 위의 Depth Buffer 를 응용하여 위처럼 가려지는 표면과 안가려지는 표면을 알아낼 수 있다. Depth Buffer 는 Light-Space 를 기준으로 데이터를 저장하고 있다. 그리고 Shader 에서는 Local-Space 로 정점의 위치가 들어오기 때문에 Depth 값을 비교하려면 두 값을 같은 공간으로 맞춰주어야 한다. OpenGL Tutorial : Shadow Mapping 에서는 bias 행렬과 Light-Space 가 적용된 행렬을 합성하여 입력으로 들어온 정점 데이터를 Light-Space 기준으로 바꿔준다. 그 다음 정점의 Depth(Z) 값과 Depth Buffer 에서 샘플링한 Depth(Z) 값을 비교하여 현재 정점의 Depth 값이 더 크면(멀면) 그림자를 적용시킨다. 이러면 기본적인 Shadwo Mapping 의 이론은 끝이다. 아래 간단한 GLSL 코드가 있다. vec4 ShadowCoord = DepthBiasMVP * vec4(vertexPosition_modelspace, 1); float visibility = 1.0; if (texture( shadowMap, ShadowCoord.xy ).z &amp;lt; ShadowCoord.z) { visibility = 0.5; } 정점의 위치를 변환시키고, Depth 값에 따라 visibility 값을 변경시켜 그림자를 적용시킨다. 하지만 위에서도 언급했지만 Shadow Mapping 자체에는 조금 문제가 있다고 언급했다. 해당 코드의 결과를 보자. 출처 : opengl-tutorial 위 그림은 굉장히 난장판이다. 세가지의 문제가 있는데 사진의 전체를 봐도 쉽게 알 수 있는 빛이 닿는 영역이 그림자 처리되는 것, Shadow acne 가 생겼다고 말한다. 그리고 왼쪽아래 구석부분에 아주 조금 빛이 들어오는 것처럼 처리되는 것이 있다. 이는 Peter Panning 이라고 부른다. 그리고 마지막으로 그림자와 빛이 닿는 부분의 경계가 울퉁불퉁한게 보일 것이다. 이를 계단현상, aliasing 이라고 부르는데 흔히 게임에서 적용되는 antialiasing 의 반대말이 맞다. 첫번째로 해결할 문제는 Shadow acne 다. 이 문제는 아래 그림을 보면 쉽게 이해가 된다. 출처 : opengl-tutorial 사선으로 나와있는 노란색 선들은 Shadow Map 을 기준으로 Light-Space 로 변환한 정점의 Depth 값의 기준을 뜻한다. 그리고 표면 자체는 Shadow Map 의 기준이 된다. 그림의 검은색 부분은 빛이 닿는 부분임에도 불구하고 그림자로 처리되는 부분인데, 이를 없에기 위해서는 값을 비교할때 단순하게 bias 를 더해주면 된다. float bias = 0.005; float visibility = 1.0; if (texture( shadowMap, ShadowCoord.xy ).z &amp;lt; ShadowCoord.z-bias) { visibility = 0.5; } 이렇게 적용시키면 평면에서의 acne 들은 제거가 가능하지만 곡면에서의 acne 들이 제거가 안되기 때문에 bias 를 조금 수정해준다. float bias = 0.005*tan(acos(cosTheta)); // cosTheta is dot( n,l ), clamped between 0 and 1 bias = clamp(bias, 0,0.01); 이러면 Shadow acne 들은 제거된다. 출처 : opengl-tutorial 다음은 Peter Panning 을 언급할 차례다. OpenGL Tutorial 에서는 이 문제의 해결책으로 굉장히 단순한 방법을 제시한다. Peter Panning 이 생기지 않도록 충분히 두꺼운 오브젝트를 배치하는 것이다. 출처 : opengl-tutorial 이렇게 쉽게 해결된다. 마지막으로 다룰 문제는 aliasing 이다. 이는 Shadow Mapping 의 고질적인 문제로써 anti-alisasing 기법을 통해 해결해왔다. 첫번째로 Shadow Map 을 샘플링할 때 일반적인 색을 가져오는 샘플링이 아닌 다른 방식을 사용한다. Shadow Map 을 한번 샘플링할 때 하드웨어에서 주변의 텍셀을 샘플링해 주변 텍셀과 비교를 수행해 모든 비교결과를 이중선형 보간을 적용한 결과를 주는 샘플링 방식을 사용한다고 한다. 만약 이중선형 보간을 사용하지 않는다면 Point Sampling 을 여러번 하여 결과들을 사용해 PCF 를 적용시켜주면 된다. 이렇게 해주면 조금 부드러운 결과가 나오게 된다. 하지만 이로써는 만족할만한 결과를 얻을 수 없어 주변을 여러번 샘플링해 값을 가져온다. 미리 생성된 offset 을 사용해 기준 UV 주변을 샘플링한다. for (int i=0;i&amp;lt;4;i++){ if ( texture( shadowMap, ShadowCoord.xy + poissonDisk[i]/700.0 ).z &amp;lt; ShadowCoord.z-bias ){ visibility-=0.2; } } 미리 생성된 offset 은 Poisson Disc 방식으로 생성된듯하다. visibility 변수는 색의 어두움을 결정하는 변수로 한번 Depth Test 에 걸리면 0.2를 줄여 0.2 ~ 1 사이의 값을 가진다. 이렇게 두가지 방식으로 anti-aliasing 을 해주면 제법 그럴듯한 결과가 나온다. 출처 : opengl-tutorial 또한 UV 좌표에 offset 을 주는 방법은 꽤나 많다. 위의 방법은 랜덤으로 고정된 부분만 체크하지만 이 방법에 임의로 offset 돌려주는 방법도 있다. 참조 OpenGL Tutorial : Tutorial 16 Shadow mapping OGLdev : Percentage Closer Filtering GPU Gems : Chapter 11. Shadow Map Antialiasing Wikipedia : SuperSampling#poisson_disc</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Using Compute Shader In Unity</title>
      
      <link href="https://hrmrzizon.github.io/2017/11/29/using-compute-shader-in-unity/" rel="alternate" type="text/html" title="Using Compute Shader In Unity" />
      <published>2017-11-29T00:00:00+00:00</published>
      <updated>2017-11-29T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2017/11/29/using-compute-shader-in-unity</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2017/11/29/using-compute-shader-in-unity/">&lt;p&gt;&lt;em&gt;Compute Shader&lt;/em&gt; 는 &lt;em&gt;DirectX 11&lt;/em&gt; 의 등장과 함께 본격적으로 쓰이기 시작했다. 지금은 &lt;em&gt;GPGPU&lt;/em&gt; 의 본격적인 기능으로 CPU 에서 처리하기 힘든 계산량을 책임지는 중요한 기능으로 자리잡았다. 실시간으로 현실적인 그래픽을 구현하기 위해 요즘의 게임들은 &lt;em&gt;Compute Shader&lt;/em&gt; 를 사용해서 여러 계산을 한다. 조금이라도 퍼포먼스가 필요하다면 당연히 쓰게되는 것이다.&lt;/p&gt;

&lt;p&gt;사용하는 방법 자체는 간단하지만 &lt;em&gt;Compute Shader&lt;/em&gt; 를 사용해 어떤 기능을 구현하는지가 중요하다. 간단하게 사용방법부터 알아보자. Unity 에서는 &lt;em&gt;Compute Shader&lt;/em&gt; 를 위한 파일을 생성해야 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/create_computeshader.png&quot; alt=&quot;create computeshader&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;프로젝트창에서 위 그림과 같이 생성해주면 된다. 그러면 아래와 같은 기본소스로 파일이 생성된다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C&quot;&gt;// Each #kernel tells which function to compile; you can have many kernels
#pragma kernel CSMain

// Create a RenderTexture with enableRandomWrite flag and set it
// with cs.SetTexture
RWTexture2D&amp;lt;float4&amp;gt; Result;

[numthreads(8,8,1)]
void CSMain (uint3 id : SV_DispatchThreadID)
{
	// TODO: insert actual code here!

	Result[id.xy] = float4(id.x &amp;amp; id.y, (id.x &amp;amp; 15)/15.0, (id.y &amp;amp; 15)/15.0, 0.0);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;위의 소스는 &lt;em&gt;HLSL&lt;/em&gt; 로 코딩된 소스로 &lt;em&gt;DirectX 11&lt;/em&gt; 을 기준으로 코딩되어 있다. &lt;em&gt;UnityCG&lt;/em&gt; 파일안의 코드를 이용하면 &lt;em&gt;GLSL&lt;/em&gt; 로 자동 컨버팅이 되기도 한다. 직접 &lt;em&gt;GLSL&lt;/em&gt; 코드로 코딩하고 싶다면 &lt;em&gt;GLSLPROGRAM&lt;/em&gt; 과 &lt;em&gt;ENDGLSL&lt;/em&gt; 로 코드를 감싸주면 간단하게 해결된다.&lt;/p&gt;

&lt;p&gt;내용은 간단하다. 각 텍셀별로 접근이 가능한 &lt;em&gt;Texture&lt;/em&gt; 를 이용해서(&lt;em&gt;DirectX&lt;/em&gt; 에서는 UAV 라고 칭한다.) &lt;em&gt;Texture&lt;/em&gt; 에 값을 채운다. &lt;em&gt;HLSL&lt;/em&gt; 의 자세한 문법과 사용방법은 &lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/ff471569.aspx&quot;&gt;MSDN : SV_GroupIndex&lt;/a&gt;, &lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/bb509647.aspx&quot;&gt;MSDN : Semantics &lt;/a&gt; 들을 참고하길 바란다.&lt;/p&gt;

&lt;p&gt;또한 쉐이더에서 뿐만아니라 &lt;em&gt;Unity&lt;/em&gt; 스크립트상에서도 데이터들을 연결해주어야 한다. 사용하는 유형은 간단하다. &lt;strong&gt;UnityEngine.Texture&lt;/strong&gt; 에서 파생된 텍스쳐들, &lt;strong&gt;UnityEngine.RenderTexture&lt;/strong&gt;, &lt;strong&gt;UnityEngine.ComputeBuffer&lt;/strong&gt; 정도면 모든 활용이 가능하다. &lt;strong&gt;UnityEngine.RenderTexture&lt;/strong&gt; 에서는 &lt;em&gt;Cubemap&lt;/em&gt; 도 지원하니 간단하게 쓸 수 있다. 해당 인스턴스를 넘겨주는 방법은 아래와 같다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C#&quot;&gt;ComputeShader shader = ...;
RenderTexture rt = ...;

shader.SetTexture(&quot;Result&quot;, rt);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;코드에서의 변수명을 맞추어 넣어주거나 해쉬값을 미리 가져와 넣어주면 된다. 다른 유형의 데이터들도 이런 방법으로 넣을 수 있다. 데이터를 넣어주면 다음은 &lt;em&gt;Compute Shader&lt;/em&gt; 를 실행하여 결과를 얻어야 한다. 간단하게 함수호출만 해주면 된다. 방법은 아래와 같다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C#&quot;&gt;ComputeShader shader = ...;
RenderTexture rt = ...;
int kernelIndex = shader.FindKernel(&quot;CSMain&quot;);

shader.Dispatch(kernelIndex, rt.width / 8, rt.height / 8, 1);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;해당 &lt;em&gt;Compute Shader&lt;/em&gt; 소스는 텍스쳐안에 값을 채우는 코드이기 때문에 위와같이 해주었다. &lt;a href=&quot;https://docs.unity3d.com/ScriptReference/ComputeShader.Dispatch.html&quot;&gt;Unity Reference : ComputeShader.Dispatch&lt;/a&gt; 와 위의 &lt;em&gt;Compute Shader&lt;/em&gt; 소스를 참고하면 알겠지만 최대 3차원의 방식으로 &lt;em&gt;Compute Shader&lt;/em&gt; 의 그룹을 설정하여 계산이 가능하다.  &lt;em&gt;Compute Shader&lt;/em&gt; 소스의 &lt;em&gt;[numthreads(8,8,1)]&lt;/em&gt; 는 한 그룹의 &lt;em&gt;Thread&lt;/em&gt; 갯수를 나타내고, &lt;em&gt;ComputeShader.Dispatch&lt;/em&gt; 메소드는 몇개의 그룹을 실행시키는지 넘겨주는 메소드다. 아래 그림을 보면 조금더 쉽게 이해가 가능하다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;https://msdn.microsoft.com/dynimg/IC520438.png&quot; alt=&quot;&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/ff471569.aspx&quot;&gt;MSDN&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Compute Shader&lt;/em&gt; 는 &lt;em&gt;DirectX 11&lt;/em&gt; 이상, &lt;em&gt;Vulkan&lt;/em&gt;,  &lt;em&gt;OpenGL 4.3&lt;/em&gt; 이상, &lt;em&gt;OpenGL ES 3.0&lt;/em&gt; 이상, &lt;em&gt;Metal&lt;/em&gt; 에서 사용가능하다. 그 아래의 플랫폼은 지원하지 않는다. 또 유의해야 할점은 그래픽 드라이버별로 지원 기능이 조금씩 다를 수 있으니 기능을 유의하며 사용해야한다. &lt;a href=&quot;https://docs.unity3d.com/Manual/ComputeShaders.html&quot;&gt;Unity Manual : ComptuteShader&lt;/a&gt; 에서 조금 참고할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;{ post_url 2017-08-01-using-compute-buffer-in-unity }&quot;&gt;Using Compute Buffer in Unity&lt;/a&gt; 에서 관련된 내용을 언급했으니 같이 보면 좋을듯 하다.&lt;/p&gt;

&lt;h2&gt;참조&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/ff471569.aspx&quot;&gt;MSDN : SV_GroupIndex&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/bb509647.aspx&quot;&gt;MSDN : Semantics &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.unity3d.com/Manual/ComputeShaders.html&quot;&gt;Unity Manual : ComptuteShader&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="unity" />
      
        <category term="shader" />
      
        <category term="hlsl" />
      

      

      
        <summary type="html">Compute Shader 는 DirectX 11 의 등장과 함께 본격적으로 쓰이기 시작했다. 지금은 GPGPU 의 본격적인 기능으로 CPU 에서 처리하기 힘든 계산량을 책임지는 중요한 기능으로 자리잡았다. 실시간으로 현실적인 그래픽을 구현하기 위해 요즘의 게임들은 Compute Shader 를 사용해서 여러 계산을 한다. 조금이라도 퍼포먼스가 필요하다면 당연히 쓰게되는 것이다. 사용하는 방법 자체는 간단하지만 Compute Shader 를 사용해 어떤 기능을 구현하는지가 중요하다. 간단하게 사용방법부터 알아보자. Unity 에서는 Compute Shader 를 위한 파일을 생성해야 한다. 프로젝트창에서 위 그림과 같이 생성해주면 된다. 그러면 아래와 같은 기본소스로 파일이 생성된다. // Each #kernel tells which function to compile; you can have many kernels #pragma kernel CSMain // Create a RenderTexture with enableRandomWrite flag and set it // with cs.SetTexture RWTexture2D&amp;lt;float4&amp;gt; Result; [numthreads(8,8,1)] void CSMain (uint3 id : SV_DispatchThreadID) { // TODO: insert actual code here! Result[id.xy] = float4(id.x &amp;amp; id.y, (id.x &amp;amp; 15)/15.0, (id.y &amp;amp; 15)/15.0, 0.0); } 위의 소스는 HLSL 로 코딩된 소스로 DirectX 11 을 기준으로 코딩되어 있다. UnityCG 파일안의 코드를 이용하면 GLSL 로 자동 컨버팅이 되기도 한다. 직접 GLSL 코드로 코딩하고 싶다면 GLSLPROGRAM 과 ENDGLSL 로 코드를 감싸주면 간단하게 해결된다. 내용은 간단하다. 각 텍셀별로 접근이 가능한 Texture 를 이용해서(DirectX 에서는 UAV 라고 칭한다.) Texture 에 값을 채운다. HLSL 의 자세한 문법과 사용방법은 MSDN : SV_GroupIndex, MSDN : Semantics 들을 참고하길 바란다. 또한 쉐이더에서 뿐만아니라 Unity 스크립트상에서도 데이터들을 연결해주어야 한다. 사용하는 유형은 간단하다. UnityEngine.Texture 에서 파생된 텍스쳐들, UnityEngine.RenderTexture, UnityEngine.ComputeBuffer 정도면 모든 활용이 가능하다. UnityEngine.RenderTexture 에서는 Cubemap 도 지원하니 간단하게 쓸 수 있다. 해당 인스턴스를 넘겨주는 방법은 아래와 같다. ComputeShader shader = ...; RenderTexture rt = ...; shader.SetTexture(&quot;Result&quot;, rt); 코드에서의 변수명을 맞추어 넣어주거나 해쉬값을 미리 가져와 넣어주면 된다. 다른 유형의 데이터들도 이런 방법으로 넣을 수 있다. 데이터를 넣어주면 다음은 Compute Shader 를 실행하여 결과를 얻어야 한다. 간단하게 함수호출만 해주면 된다. 방법은 아래와 같다. ComputeShader shader = ...; RenderTexture rt = ...; int kernelIndex = shader.FindKernel(&quot;CSMain&quot;); shader.Dispatch(kernelIndex, rt.width / 8, rt.height / 8, 1); 해당 Compute Shader 소스는 텍스쳐안에 값을 채우는 코드이기 때문에 위와같이 해주었다. Unity Reference : ComputeShader.Dispatch 와 위의 Compute Shader 소스를 참고하면 알겠지만 최대 3차원의 방식으로 Compute Shader 의 그룹을 설정하여 계산이 가능하다. Compute Shader 소스의 [numthreads(8,8,1)] 는 한 그룹의 Thread 갯수를 나타내고, ComputeShader.Dispatch 메소드는 몇개의 그룹을 실행시키는지 넘겨주는 메소드다. 아래 그림을 보면 조금더 쉽게 이해가 가능하다. 출처 : MSDN Compute Shader 는 DirectX 11 이상, Vulkan, OpenGL 4.3 이상, OpenGL ES 3.0 이상, Metal 에서 사용가능하다. 그 아래의 플랫폼은 지원하지 않는다. 또 유의해야 할점은 그래픽 드라이버별로 지원 기능이 조금씩 다를 수 있으니 기능을 유의하며 사용해야한다. Unity Manual : ComptuteShader 에서 조금 참고할 수 있다. Using Compute Buffer in Unity 에서 관련된 내용을 언급했으니 같이 보면 좋을듯 하다. 참조 MSDN : SV_GroupIndex MSDN : Semantics Unity Manual : ComptuteShader</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Drawinstanced Vs Merged Instancing</title>
      
      <link href="https://hrmrzizon.github.io/2017/11/18/DrawInstanced-vs-Merged-Instancing/" rel="alternate" type="text/html" title="Drawinstanced Vs Merged Instancing" />
      <published>2017-11-18T00:00:00+00:00</published>
      <updated>2017-11-18T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2017/11/18/DrawInstanced-vs-Merged-Instancing</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2017/11/18/DrawInstanced-vs-Merged-Instancing/">&lt;p&gt;&lt;a href=&quot;https://www.gdcvault.com/play/1020624/Advanced-Visual-Effects-with-DirectX&quot;&gt;GDC 2014 : Vertex Sahder Tricks&lt;/a&gt; 슬라이드에 따르면 &lt;em&gt;DrawInstanced&lt;/em&gt; 함수를 사용하여 인스턴싱을 하는것보다 &lt;em&gt;vertexID&lt;/em&gt; 를 사용하여 인스턴싱을 하는것이 빠르다고 한다. &lt;em&gt;vertexID&lt;/em&gt; 를 쓰는 방법은 굉장히 단순하다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;VSOutput VS(uint id : SV_VertexID)
{
    VSOutput output;

    /*
        ...
    */

    return output;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;SV_VertexID&lt;/em&gt; &lt;em&gt;Semantic&lt;/em&gt; 을 사용하여 값을 접근하기만 하면 된다. &lt;em&gt;vertexID&lt;/em&gt; 는 말그대로 버텍스별 인덱스를 뜻한다. &lt;em&gt;SRV&lt;/em&gt; 나 &lt;em&gt;UAV&lt;/em&gt; 와 함께 사용하여 &lt;em&gt;Instancing&lt;/em&gt; 을 하면된다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/gdc2014_vertexshadertricks_23.png&quot; alt=&quot;Merge Instancing Performance&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://www.gdcvault.com/play/1020624/Advanced-Visual-Effects-with-DirectX&quot;&gt;GDC 2014 : Vertex Sahder Tricks&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;그림을 보면 AMD GPU 에서 확실히 퍼포먼스 차이가 난것을 확인할 수 있다. &lt;del&gt;스피커가 AMD 소속이라는 게 포인트&lt;/del&gt;&lt;/p&gt;

&lt;h1&gt;참조 자료&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.gdcvault.com/play/1020624/Advanced-Visual-Effects-with-DirectX&quot;&gt;GDC 2014 : Vertex Sahder Tricks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="render" />
      
        <category term="hlsl" />
      

      

      
        <summary type="html">GDC 2014 : Vertex Sahder Tricks 슬라이드에 따르면 DrawInstanced 함수를 사용하여 인스턴싱을 하는것보다 vertexID 를 사용하여 인스턴싱을 하는것이 빠르다고 한다. vertexID 를 쓰는 방법은 굉장히 단순하다. VSOutput VS(uint id : SV_VertexID) { VSOutput output; /* ... */ return output; } SV_VertexID Semantic 을 사용하여 값을 접근하기만 하면 된다. vertexID 는 말그대로 버텍스별 인덱스를 뜻한다. SRV 나 UAV 와 함께 사용하여 Instancing 을 하면된다. 출처 : GDC 2014 : Vertex Sahder Tricks 그림을 보면 AMD GPU 에서 확실히 퍼포먼스 차이가 난것을 확인할 수 있다. 스피커가 AMD 소속이라는 게 포인트 참조 자료 GDC 2014 : Vertex Sahder Tricks</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Hbao Plus Analysis 3</title>
      
      <link href="https://hrmrzizon.github.io/2017/11/15/hbao-plus-analysis-3/" rel="alternate" type="text/html" title="Hbao Plus Analysis 3" />
      <published>2017-11-15T00:00:00+00:00</published>
      <updated>2017-11-15T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2017/11/15/hbao-plus-analysis-3</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2017/11/15/hbao-plus-analysis-3/">&lt;p&gt;&lt;strong&gt;HBAO+ 3.1 버젼을 기준으로 글이 작성되었습니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이전 &lt;a href=&quot;/2017/11/15/hbao-plus-analysis-2/&quot;&gt;hbao plus analysis 2&lt;/a&gt; 글에서 &lt;em&gt;Horizon based ambient occlusion&lt;/em&gt; 와 &lt;em&gt;Cross Bilateral Filter&lt;/em&gt; 대해서 알아보았다. 이번 글에서는 부록의 느낌으로 &lt;em&gt;HLSL&lt;/em&gt; 코드를 읽으면서 생소했던 기타 기법들에 대해서 써볼 것이다.&lt;/p&gt;

&lt;p&gt;첫번째로 &lt;em&gt;Full Screen Triangle&lt;/em&gt; 이라는 기법이다. 알고마면 굉장히 단순한 개념으로, 화면을 모두 덮는 한개의 삼각형을 그려서 모든 픽셀에 쉐이더를 돌릴 수 있게 해주는 기법이다. 아래 슬라이드를 보면 쉽게 이해가 갈것이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/vertex-shader-tricks-by-bill-bilodeau-amd-at-gdc14-14-638.jpg&quot; alt=&quot;Full Screen Triangle&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://www.gdcvault.com/play/1020624/Advanced-Visual-Effects-with-DirectX&quot;&gt;GDC 2014 : Vertex Sahder Tricks&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;단순하지만 처음 봤을 때는 조금 신박하게 느껴질 수도 있다. 두번째로는 모든 계산에 최대한 &lt;em&gt;HLSL Intrisic&lt;/em&gt; 을 사용한다. 특히 벡터와 벡터사이의 거리를 계산할때 &lt;em&gt;dot product&lt;/em&gt; 를 써서 하는게 정말 많았다. 어셈블리 레벨에서 달라지는것 같긴하나 정확한 이유는 알지 못했다. 추측해보면 GPU 에서 해당 명령어가 있지 않을까.. 라고 생각한다.&lt;/p&gt;

&lt;p&gt;세번째도 위의 것과 비슷하다. 대부분의 데이터에 &lt;em&gt;MAD&lt;/em&gt; 방식을 사용해서 계산한다. 하지만 이는 거의 공식적으로 정해진게 있다. &lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/ff471418.aspx&quot;&gt;MSDN : mad  function&lt;/a&gt; 레퍼런스에서도 나오듯이 어떤 GPU 에서는 위에서 추측한대로 하드웨어에서 지원하는 명령어라고 한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;…
Shaders can then take advantage of potential performance improvements by using a native mad instruction (versus mul + add) on some hardware.
…&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;또한 &lt;em&gt;HBAO+&lt;/em&gt; 소스에서 찾은 주석에는 &lt;em&gt;GK104&lt;/em&gt; 부터 특정 구간에서 10% 퍼포먼스 이득이 있다고 쓰여져 있다.&lt;/p&gt;

&lt;p&gt;네번째는 나누기를 절대 쓰지 않는다. 나머지 연산(mod, A % B)는 간혹 쓰이지만 나누기는 절대로 쓰이지 않았었다. 혹시라도 필요하다면 전부 &lt;em&gt;Constant Buffer&lt;/em&gt; 에 CPU 에서 역수를 취해서 넘겨주는 방식으로 되어 있었다. 이도 역시 하드웨어에서 동작하는 부분을 알고 짠듯하다.&lt;/p&gt;

&lt;p&gt;다섯번째는 &lt;em&gt;HLSL&lt;/em&gt; 코드를 &lt;em&gt;cpp&lt;/em&gt; 소스에 &lt;em&gt;include&lt;/em&gt; 하여 &lt;em&gt;Constant Buffer&lt;/em&gt; 값을 갱신하는 코드였다. 여태까지 예전의 &lt;em&gt;DirectX&lt;/em&gt; 소스만 보거나 &lt;em&gt;Unity&lt;/em&gt; 에서만 작업을 해서 그런지 이런 기능은 굉장히 낯설었다.&lt;/p&gt;

&lt;h1&gt;참조 자료&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://docs.nvidia.com/gameworks/content/gameworkslibrary/visualfx/hbao/index.html&quot;&gt;NVIDIA HBAO+&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.gdcvault.com/play/1020624/Advanced-Visual-Effects-with-DirectX&quot;&gt;GDC 2014 : Vertex Sahder Tricks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/ff471418.aspx&quot;&gt;MSDN : mad function&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="render" />
      
        <category term="hlsl" />
      
        <category term="analysis" />
      
        <category term="hbao+" />
      

      

      
        <summary type="html">HBAO+ 3.1 버젼을 기준으로 글이 작성되었습니다. 이전 hbao plus analysis 2 글에서 Horizon based ambient occlusion 와 Cross Bilateral Filter 대해서 알아보았다. 이번 글에서는 부록의 느낌으로 HLSL 코드를 읽으면서 생소했던 기타 기법들에 대해서 써볼 것이다. 첫번째로 Full Screen Triangle 이라는 기법이다. 알고마면 굉장히 단순한 개념으로, 화면을 모두 덮는 한개의 삼각형을 그려서 모든 픽셀에 쉐이더를 돌릴 수 있게 해주는 기법이다. 아래 슬라이드를 보면 쉽게 이해가 갈것이다. 출처 : GDC 2014 : Vertex Sahder Tricks 단순하지만 처음 봤을 때는 조금 신박하게 느껴질 수도 있다. 두번째로는 모든 계산에 최대한 HLSL Intrisic 을 사용한다. 특히 벡터와 벡터사이의 거리를 계산할때 dot product 를 써서 하는게 정말 많았다. 어셈블리 레벨에서 달라지는것 같긴하나 정확한 이유는 알지 못했다. 추측해보면 GPU 에서 해당 명령어가 있지 않을까.. 라고 생각한다. 세번째도 위의 것과 비슷하다. 대부분의 데이터에 MAD 방식을 사용해서 계산한다. 하지만 이는 거의 공식적으로 정해진게 있다. MSDN : mad function 레퍼런스에서도 나오듯이 어떤 GPU 에서는 위에서 추측한대로 하드웨어에서 지원하는 명령어라고 한다. … Shaders can then take advantage of potential performance improvements by using a native mad instruction (versus mul + add) on some hardware. … 또한 HBAO+ 소스에서 찾은 주석에는 GK104 부터 특정 구간에서 10% 퍼포먼스 이득이 있다고 쓰여져 있다. 네번째는 나누기를 절대 쓰지 않는다. 나머지 연산(mod, A % B)는 간혹 쓰이지만 나누기는 절대로 쓰이지 않았었다. 혹시라도 필요하다면 전부 Constant Buffer 에 CPU 에서 역수를 취해서 넘겨주는 방식으로 되어 있었다. 이도 역시 하드웨어에서 동작하는 부분을 알고 짠듯하다. 다섯번째는 HLSL 코드를 cpp 소스에 include 하여 Constant Buffer 값을 갱신하는 코드였다. 여태까지 예전의 DirectX 소스만 보거나 Unity 에서만 작업을 해서 그런지 이런 기능은 굉장히 낯설었다. 참조 자료 NVIDIA HBAO+ GDC 2014 : Vertex Sahder Tricks MSDN : mad function</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Hbao Plus Analysis 2</title>
      
      <link href="https://hrmrzizon.github.io/2017/11/15/hbao-plus-analysis-2/" rel="alternate" type="text/html" title="Hbao Plus Analysis 2" />
      <published>2017-11-15T00:00:00+00:00</published>
      <updated>2017-11-15T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2017/11/15/hbao-plus-analysis-2</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2017/11/15/hbao-plus-analysis-2/">&lt;p&gt;&lt;strong&gt;HBAO+ 3.1 버젼을 기준으로 글이 작성되었습니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이전 &lt;a href=&quot;/2017/11/15/hbao-plus-analysis-1/&quot;&gt;hbao plus analysis 1&lt;/a&gt; 글에서 &lt;em&gt;HBAO+&lt;/em&gt; 에서 &lt;em&gt;Linearize Depth&lt;/em&gt; 와 &lt;em&gt;Deinterleaved Texturing&lt;/em&gt; 에 대해서 알아보았다. 이번 글에서는 &lt;em&gt;HBAO+&lt;/em&gt; 의 핵심 알고리즘인 &lt;em&gt;Horizon Based Ambient Occlusion&lt;/em&gt; 와 AO 블러에 사용되는 &lt;em&gt;Cross Bilateral Filter&lt;/em&gt; 에 대해서 알아볼것이다.&lt;/p&gt;

&lt;h2&gt;Horizon Based Ambient Occlusion&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;Horizon Based Ambient Occlusion&lt;/em&gt; 은 xy 평면과(horizon) Depth 값을 사용해서 &lt;em&gt;AO&lt;/em&gt; 를 계산한다. 슬라이드에서 가져온 일부를 보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/hbao_siggraph08_05.png&quot; alt=&quot;Horizon Mapping&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://developer.download.nvidia.com/presentations/2008/SIGGRAPH/HBAO_SIG08b.pdf&quot;&gt;Siggraph 2008 : Image-Space Horizon-Based Ambient Occlusion&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;해당 슬라이드에서는 xy평면을 단순하게 1차원인 x축만으로 나타냈다. &lt;em&gt;HBAO&lt;/em&gt; 는 그림에 나오는 &lt;em&gt;horizon angle&lt;/em&gt; 을 사용하여 &lt;em&gt;AO&lt;/em&gt; 값을 구한다. 자세한 방법은 아래 슬라이드를 보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/hbao_siggraph08_12.png&quot; alt=&quot;Horizon-Based AO&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://developer.download.nvidia.com/presentations/2008/SIGGRAPH/HBAO_SIG08b.pdf&quot;&gt;Siggraph 2008 : Image-Space Horizon-Based Ambient Occlusion&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;슬라이드에서는 표면의 접선을 나타내는 &lt;em&gt;Tangent&lt;/em&gt; 벡터와 &lt;em&gt;Horizon&lt;/em&gt; 벡터를 사용해서 &lt;em&gt;sin&lt;/em&gt; 의 차이로 &lt;em&gt;AO&lt;/em&gt; 를 계산한다고 설명되어 있다. &lt;em&gt;Horizon&lt;/em&gt; 벡터는 &lt;em&gt;Depth&lt;/em&gt; 와 화면의 좌표를 구해서 샘플링하는 위치값을 구하고 기준이 되는 위치값의 차이를 통해 구한다. &lt;em&gt;HBAO+&lt;/em&gt; 코드에서는 입력을 받은 &lt;em&gt;Normal&lt;/em&gt; 벡터와 &lt;em&gt;Horizon&lt;/em&gt; 벡터에 &lt;em&gt;dot&lt;/em&gt; 을 사용해 &lt;em&gt;cos&lt;/em&gt; 값을 구하고 변환해준다. 이렇게 한번 &lt;em&gt;AO&lt;/em&gt; 값을 구한다.&lt;/p&gt;

&lt;p&gt;보다 정확한 &lt;em&gt;AO&lt;/em&gt; 값을 구하기 위해서는 전방위로 탐색할 필요가 있다. 정해진 방향으로 샘플링을 해도 오차가 생길 수 있고 완전히 랜덤하게 방향을 정해도 부정확한 결과를 얻을 수 있다. 그래서 &lt;em&gt;HBAO&lt;/em&gt; 는 랜덤하게 방향을 정하나 그 방향 벡터를 정해진 각도로 돌려주어 그나마 정확한 결과를 얻으려 한다. 슬라이드를 보고 넘어가자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/hbao_siggraph08_14.png&quot; alt=&quot;Sampling the Depth Image&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://developer.download.nvidia.com/presentations/2008/SIGGRAPH/HBAO_SIG08b.pdf&quot;&gt;Siggraph 2008 : Image-Space Horizon-Based Ambient Occlusion&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;핵심적인 개념은 모두 설명했지만 만족할만한 결과를 얻기 위해 여러가지 보정 방법들이 필요하다. 그래서 &lt;em&gt;HBAO&lt;/em&gt; 에서는 두가지 보정을 해주는 개념을 설명한다. &lt;em&gt;HBAO&lt;/em&gt; 는 방향을 설정해주고 해당 방향으로 한번만 샘플링 하는게 아니라 여러번 샘플링 한다. 그러므로 거리에 따른 감쇠(attenuation)가 필요하다. 방법은 간단하다. &lt;em&gt;AO&lt;/em&gt; 를 계산할때 구했던 &lt;em&gt;Horizon&lt;/em&gt; 벡터의 크기에 따라서 &lt;em&gt;AO&lt;/em&gt; 값을 줄여준다. 나머지 한가지는 &lt;em&gt;Horizon&lt;/em&gt; 벡터와 &lt;em&gt;Tangent&lt;/em&gt; 벡터를 이용해 구하는 실질적인 &lt;em&gt;AO&lt;/em&gt; 값에 &lt;em&gt;Bias&lt;/em&gt; 로 낮은 &lt;em&gt;AO&lt;/em&gt; 값들을 무시하는 방법이다. &lt;em&gt;Bias&lt;/em&gt; 가 없이 &lt;em&gt;AO&lt;/em&gt; 를 생성하게 되면 노이즈가 생기기 때문이다. 또한 &lt;em&gt;Bias&lt;/em&gt; 로 생긴 수학적 오차는 코드에서 따로 보정해주기 때문에 크게 문제는 없다.&lt;/p&gt;

&lt;h2&gt;Cross Bilateral Filter&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;SSAO&lt;/em&gt; 의 결과에는 일반적으로 블러를 먹이게 된다. 대부분 근사에 기반한 계산이기 때문이다. &lt;em&gt;HBAO&lt;/em&gt; 에서는 &lt;em&gt;Depth&lt;/em&gt; 를 이용한 방법을 소개한다. 바로 &lt;em&gt;Cross Bilateral Filter&lt;/em&gt; 다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Cross Bilateral Filter&lt;/em&gt; 은 &lt;em&gt;Gaussian Filter&lt;/em&gt; 와 비슷한 필터로, &lt;em&gt;Gaussian Filter&lt;/em&gt; 는 샘플링할 위치의 거리에 따라 점차 가중치가 줄어드는 필터라면, &lt;em&gt;Bilateral Filter&lt;/em&gt; 는 위치에 따라 가중치가 줄어드는게 아닌 각 위치별로 가지고 있는 한개의 스칼라값에 차이에 따라서 가중치를 정하는 필터다. &lt;em&gt;Cross&lt;/em&gt; 단어를 붙인 이유는 왼쪽과 오른쪽 방향의 필터와 위와 아래의 필터를 따로하기 때문에 &lt;em&gt;Cross&lt;/em&gt; 라는 단어를 붙인 듯 하다. &lt;em&gt;HBAO+&lt;/em&gt; 코드에서도 X 축과 Y 축을 기준으로 하는 블러 소스가 나누어져 있다. &lt;em&gt;HBAO&lt;/em&gt; 에는 한개의 스칼라 값을 &lt;em&gt;Depth&lt;/em&gt; 를 기준으로 계산한다. 그래서 크게 튀는 부분의 결과는 많이 반영하지 않아 전체적으로 뿌옇게 바뀐다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/hbao_siggraph08_28.png&quot; alt=&quot;Sampling the Depth Image&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://developer.download.nvidia.com/presentations/2008/SIGGRAPH/HBAO_SIG08b.pdf&quot;&gt;Siggraph 2008 : Image-Space Horizon-Based Ambient Occlusion&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h1&gt;참조 자료&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://docs.nvidia.com/gameworks/content/gameworkslibrary/visualfx/hbao/index.html&quot;&gt;NVidia HBAO+&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://developer.download.nvidia.com/presentations/2008/SIGGRAPH/HBAO_SIG08b.pdf&quot;&gt;Image-Space Horizon Based Ambient Occlusion&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Gaussian_filter&quot;&gt;Wikipedia : Gaussian Filter&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Bilateral_filter&quot;&gt;Wikipedia : Bilateral Filter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="render" />
      
        <category term="analysis" />
      
        <category term="hbao+" />
      
        <category term="bilateral_filter" />
      

      

      
        <summary type="html">HBAO+ 3.1 버젼을 기준으로 글이 작성되었습니다. 이전 hbao plus analysis 1 글에서 HBAO+ 에서 Linearize Depth 와 Deinterleaved Texturing 에 대해서 알아보았다. 이번 글에서는 HBAO+ 의 핵심 알고리즘인 Horizon Based Ambient Occlusion 와 AO 블러에 사용되는 Cross Bilateral Filter 에 대해서 알아볼것이다. Horizon Based Ambient Occlusion Horizon Based Ambient Occlusion 은 xy 평면과(horizon) Depth 값을 사용해서 AO 를 계산한다. 슬라이드에서 가져온 일부를 보자. 출처 : Siggraph 2008 : Image-Space Horizon-Based Ambient Occlusion 해당 슬라이드에서는 xy평면을 단순하게 1차원인 x축만으로 나타냈다. HBAO 는 그림에 나오는 horizon angle 을 사용하여 AO 값을 구한다. 자세한 방법은 아래 슬라이드를 보자. 출처 : Siggraph 2008 : Image-Space Horizon-Based Ambient Occlusion 슬라이드에서는 표면의 접선을 나타내는 Tangent 벡터와 Horizon 벡터를 사용해서 sin 의 차이로 AO 를 계산한다고 설명되어 있다. Horizon 벡터는 Depth 와 화면의 좌표를 구해서 샘플링하는 위치값을 구하고 기준이 되는 위치값의 차이를 통해 구한다. HBAO+ 코드에서는 입력을 받은 Normal 벡터와 Horizon 벡터에 dot 을 사용해 cos 값을 구하고 변환해준다. 이렇게 한번 AO 값을 구한다. 보다 정확한 AO 값을 구하기 위해서는 전방위로 탐색할 필요가 있다. 정해진 방향으로 샘플링을 해도 오차가 생길 수 있고 완전히 랜덤하게 방향을 정해도 부정확한 결과를 얻을 수 있다. 그래서 HBAO 는 랜덤하게 방향을 정하나 그 방향 벡터를 정해진 각도로 돌려주어 그나마 정확한 결과를 얻으려 한다. 슬라이드를 보고 넘어가자. 출처 : Siggraph 2008 : Image-Space Horizon-Based Ambient Occlusion 핵심적인 개념은 모두 설명했지만 만족할만한 결과를 얻기 위해 여러가지 보정 방법들이 필요하다. 그래서 HBAO 에서는 두가지 보정을 해주는 개념을 설명한다. HBAO 는 방향을 설정해주고 해당 방향으로 한번만 샘플링 하는게 아니라 여러번 샘플링 한다. 그러므로 거리에 따른 감쇠(attenuation)가 필요하다. 방법은 간단하다. AO 를 계산할때 구했던 Horizon 벡터의 크기에 따라서 AO 값을 줄여준다. 나머지 한가지는 Horizon 벡터와 Tangent 벡터를 이용해 구하는 실질적인 AO 값에 Bias 로 낮은 AO 값들을 무시하는 방법이다. Bias 가 없이 AO 를 생성하게 되면 노이즈가 생기기 때문이다. 또한 Bias 로 생긴 수학적 오차는 코드에서 따로 보정해주기 때문에 크게 문제는 없다. Cross Bilateral Filter SSAO 의 결과에는 일반적으로 블러를 먹이게 된다. 대부분 근사에 기반한 계산이기 때문이다. HBAO 에서는 Depth 를 이용한 방법을 소개한다. 바로 Cross Bilateral Filter 다. Cross Bilateral Filter 은 Gaussian Filter 와 비슷한 필터로, Gaussian Filter 는 샘플링할 위치의 거리에 따라 점차 가중치가 줄어드는 필터라면, Bilateral Filter 는 위치에 따라 가중치가 줄어드는게 아닌 각 위치별로 가지고 있는 한개의 스칼라값에 차이에 따라서 가중치를 정하는 필터다. Cross 단어를 붙인 이유는 왼쪽과 오른쪽 방향의 필터와 위와 아래의 필터를 따로하기 때문에 Cross 라는 단어를 붙인 듯 하다. HBAO+ 코드에서도 X 축과 Y 축을 기준으로 하는 블러 소스가 나누어져 있다. HBAO 에는 한개의 스칼라 값을 Depth 를 기준으로 계산한다. 그래서 크게 튀는 부분의 결과는 많이 반영하지 않아 전체적으로 뿌옇게 바뀐다. 출처 : Siggraph 2008 : Image-Space Horizon-Based Ambient Occlusion 참조 자료 NVidia HBAO+ Image-Space Horizon Based Ambient Occlusion Wikipedia : Gaussian Filter Wikipedia : Bilateral Filter</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Hbao Plus Analysis 1</title>
      
      <link href="https://hrmrzizon.github.io/2017/11/15/hbao-plus-analysis-1/" rel="alternate" type="text/html" title="Hbao Plus Analysis 1" />
      <published>2017-11-15T00:00:00+00:00</published>
      <updated>2017-11-15T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2017/11/15/hbao-plus-analysis-1</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2017/11/15/hbao-plus-analysis-1/">&lt;p&gt;&lt;strong&gt;HBAO+ 3.1 버젼을 기준으로 글이 작성되었습니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이전 &lt;a href=&quot;/2017/11/15/hbao-plus-analysis-0/&quot;&gt;hbao plus analysis 0&lt;/a&gt; 글에서 &lt;em&gt;HBAO+&lt;/em&gt; 을 알기위한 기본적인 개념들에 대해서 살펴보았다. 이번 글에서는 &lt;em&gt;HBAO+&lt;/em&gt; 의 구조와 &lt;em&gt;Linearize Depth&lt;/em&gt; 와 &lt;em&gt;Deinterleaved Texturing&lt;/em&gt; 에 대해서 알아보겠다.&lt;/p&gt;

&lt;h2&gt;&lt;em&gt;HBAO+&lt;/em&gt; Pipeline&lt;/h2&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/hbao+_pipeline_with_input_normals.png&quot; alt=&quot;hbao+ with input normal&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://docs.nvidia.com/gameworks/content/gameworkslibrary/visualfx/hbao/index.html&quot;&gt;NVIDIA HBAO+&lt;/a&gt;
&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/hbao+_pipeline_without_input_normals.png&quot; alt=&quot;hbao+ without input normal&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://docs.nvidia.com/gameworks/content/gameworkslibrary/visualfx/hbao/index.html&quot;&gt;NVIDIA HBAO+&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;그림이 두개가 있다. 하나는 &lt;em&gt;GBuffer&lt;/em&gt; 를 사용할 시 &lt;em&gt;World-Space Normal&lt;/em&gt; 버퍼와 &lt;em&gt;Depth Buffer&lt;/em&gt; 를 넘겨주어 계산하는 방식과 입력으로 &lt;em&gt;Depth Buffer&lt;/em&gt; 만 넘겨서 &lt;em&gt;Normal&lt;/em&gt; 데이터를 계산하는 두가지 방식에 대한 파이프라인이다. 두가지의 차이는 &lt;em&gt;Normal&lt;/em&gt; 데이터에 대한 처리방식만 다르다. 나머지 계산은 다를게 없다.&lt;/p&gt;

&lt;h2&gt;Linearize Depths&lt;/h2&gt;

&lt;p&gt;코드를 보면 가장 처음에 시작하는 단계는 바로 &lt;em&gt;Linearize Depths&lt;/em&gt; 다. 이는 꽤나 알려진 방법이다. 하지만 필자는 &lt;em&gt;HBAO+&lt;/em&gt; 를 볼때 처음 봤기에 어느 정도의 설명을 해놓아야겠다. &lt;em&gt;Linearize Depths&lt;/em&gt; 를 알기 위해선 입력된 정점의 위치를 &lt;em&gt;Clipping-Space&lt;/em&gt; 로 변환하는 방법이 어떻게 이루어지는지 알고 있어야 한다.&lt;/p&gt;

&lt;p&gt;일반적인 오브젝트를 렌더링 할때는 &lt;em&gt;Shader&lt;/em&gt; 에 입력으로 들어오는 정점의 기준 공간은 &lt;em&gt;Model-Space&lt;/em&gt;(또는 &lt;em&gt;Local-Space&lt;/em&gt;) &lt;em&gt;Position&lt;/em&gt; 이다. 그래서 &lt;em&gt;MVP&lt;/em&gt; 변환을 통해 &lt;em&gt;Rasterizer&lt;/em&gt; 가 처리할 수 있도록 &lt;em&gt;Clipping-Space&lt;/em&gt; 로 &lt;em&gt;Rasterizer&lt;/em&gt; 로 넘어가기 전에 변환해주어야 한다.(전체적인 내용은 &lt;a href=&quot;https://docs.google.com/presentation/d/10VzsjfifKJlRTHDlBq7e8vNBTu4D5jOWUF87KYYGwlk/edit#slide=id.g25f88339be_0_0&quot;&gt;Model, View, Projection 변환&lt;/a&gt; 에서 확인할 수 있다.
) 그래서 &lt;em&gt;Pixel Shader&lt;/em&gt; 로 넘어간 데이터들은 픽셀별로 들어가고, 픽셀별로 들어간 정점들의 위치는 &lt;em&gt;Clipping-Space&lt;/em&gt; 로 되어있다. 여기까지 이해했으면 아래 그림을 보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/Graphics3D_ClipVolume.png&quot; alt=&quot;Frustom vs Clipping&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://www.ntu.edu.sg/home/ehchua/programming/opengl/CG_BasicsTheory.html&quot;&gt;3D Graphics with OpenGL Basic Theory&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위 그림은 &lt;em&gt;View Frustom&lt;/em&gt; 과 &lt;em&gt;Clipping Volume&lt;/em&gt; 을 보여준다. &lt;em&gt;View Frustom&lt;/em&gt; 은 &lt;em&gt;Perspective&lt;/em&gt; 방식으로 카메라가 실제로 보여주는 공간을 시각화 한것이고, &lt;em&gt;Clipping Volume&lt;/em&gt; 은 &lt;em&gt;MVP&lt;/em&gt; 변환에서 &lt;em&gt;Projection&lt;/em&gt; 행렬을 사용할시 &lt;em&gt;View Frustom&lt;/em&gt; 에서 &lt;em&gt;Clipping Volume&lt;/em&gt; 으로 변환되는 볼륨을 시각화 한것이다. &lt;em&gt;Projection&lt;/em&gt; 변환은 아래와 같다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/projection_matrix.png&quot; alt=&quot;perspective projection matrix&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://stackoverflow.com/questions/6652253/getting-the-true-z-value-from-the-depth-buffer
&quot;&gt;Stackoverflow : Getting the true z value from the depth buffer&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Perspective Projection&lt;/em&gt; 은 &lt;em&gt;Frustom&lt;/em&gt; 기준 위치를 &lt;em&gt;Cube&lt;/em&gt; 기준 위치로 바꾸는 연산이기 때문에 실제 좌표의 왜곡이 발생한다. 우리는 Z(Detph) 값이 어떤식으로 왜곡되는지 알아야 한다. 우선 &lt;em&gt;Clipping-Space&lt;/em&gt; 로 변환할때, &lt;em&gt;Perspective&lt;/em&gt; 형식의 &lt;em&gt;View Frustom&lt;/em&gt; 의 &lt;em&gt;zNear&lt;/em&gt;, &lt;em&gt;zFar&lt;/em&gt; 사이의 Z 값을 [0~1] 값으로 매핑한다. 그러면 &lt;em&gt;zNear&lt;/em&gt;, &lt;em&gt;zFar&lt;/em&gt; 값을에 따라서 실제 좌표가 바뀐다. 그리고 값 자체가 실제 Z 값과 선형적으로 매핑되지 않는다. 아래 그림을 보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/nonlinearDepth.png&quot; alt=&quot;non linear depth&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://computergraphics.stackexchange.com/questions/5116/how-am-i-able-to-perform-perspective-projection-without-a-near-plane&quot;&gt;Computer Graphics StackExchange : How am I able to perform perspective projection without a near plane?&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;그림이 조금 헷갈릴수도 있다. 세로축의 &lt;em&gt;d&lt;/em&gt; 값은 &lt;em&gt;Projection&lt;/em&gt; 을 한 Z, &lt;em&gt;Depth&lt;/em&gt; 값이고 가로축은 &lt;em&gt;World-Space&lt;/em&gt; 의 Z 값이다. 조금 헷갈릴수도 있는 부분은 세로축의 기준값이 윗부분이 0이고 아랫부분이 1이다. 이 부분은 신경써서 봐야한다. 이해했다면 변경된 &lt;em&gt;Depth&lt;/em&gt; 값은 실제 Z 값과 선형적인 관계가 아니고, 실제 Z 값으로 복원하려면 여러 연산을 해야하기에 &lt;em&gt;HBAO+&lt;/em&gt; 에서는 &lt;em&gt;Depth&lt;/em&gt; 값들을 &lt;em&gt;Linearize&lt;/em&gt; 하는 과정을 맨 처음에 넣은 것이다. 실제 Z 값으로 복원하는 이유는 간단하다. &lt;em&gt;Linear&lt;/em&gt; 하지 않은 &lt;em&gt;Depth&lt;/em&gt; 값을 연산시에 사용하면 보다 부정확한 결과가 나오기 때문이다. 특히 &lt;em&gt;SSAO&lt;/em&gt; 연산을 할때는 &lt;em&gt;Depth&lt;/em&gt; 값이 기본이 되기 때문에 해주어야 한다.&lt;/p&gt;

&lt;p&gt;이 단계에서의 결론은 간단하다. &lt;em&gt;Clipping-Space&lt;/em&gt; 의 &lt;em&gt;Depth&lt;/em&gt; 값을 &lt;em&gt;View-Space&lt;/em&gt; 의 Z 값으로 변환하는 단계다. 처리하는 코드는 다른 단계에 비해 짧다. 만약에 넘겨준 &lt;em&gt;Depth&lt;/em&gt; 데이터들이 &lt;em&gt;View-Space&lt;/em&gt; 인 경우에는 옵션을 통해 처리할 수 있다.&lt;/p&gt;

&lt;h2&gt;Deintereaved Texturing&lt;/h2&gt;

&lt;p&gt;위의 그림에는 &lt;em&gt;Generate HBAO+&lt;/em&gt; 라고 단순히 뭉뚱그려서 표현했지만 그 안에는 단순한 &lt;em&gt;Horizon based ambient occlusion(HBAO)&lt;/em&gt; 계산만 있지는 않다. &lt;em&gt;Deintereaved Texturing&lt;/em&gt; 이라는 테크닉과 함께 &lt;em&gt;HBAO&lt;/em&gt; 를 계산한다. &lt;em&gt;Computer Engineering&lt;/em&gt; 분야의 지식을 응용한 이론으로 개인적으로 이 이론을 접했을 떄 꽤나 충격이였다. 자세한 설명은 &lt;a href=&quot;https://developer.nvidia.com/sites/default/files/akamai/gamedev/docs/BAVOIL_ParticleShadowsAndCacheEfficientPost.pdf&quot;&gt;GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing&lt;/a&gt; 슬라이드의 몇장과 함께 보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/gdc2013_ParticleShadowsAndCacheEfficientPost_51.png&quot; alt=&quot;gdc2013_ParticleShadowsAndCacheEfficientPost_51&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://developer.nvidia.com/sites/default/files/akamai/gamedev/docs/BAVOIL_ParticleShadowsAndCacheEfficientPost.pdf&quot;&gt;GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Deintereaved Texturing&lt;/em&gt; 의 방법은 간단하다. 텍스쳐를 여러장으로 나누어 샘플링을 한 후 각각의 나눠진 텍스쳐를 샘플링한 결과를 하나로 합친다. 슬라이드에는 &lt;em&gt;Post-Processing&lt;/em&gt; 을 기준으로 설명이 되어있다. 이점은 생각하면서 보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/gdc2013_ParticleShadowsAndCacheEfficientPost_52.png&quot; alt=&quot;gdc2013_ParticleShadowsAndCacheEfficientPost_52&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://developer.nvidia.com/sites/default/files/akamai/gamedev/docs/BAVOIL_ParticleShadowsAndCacheEfficientPost.pdf&quot;&gt;GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;한 텍스쳐를 여러장으로 나누는건 &lt;em&gt;Multiple Render Target&lt;/em&gt; 을 사용해서 나눈다. 슬라이드는 4개를 기준으로 설명했지만 &lt;em&gt;DirectX10&lt;/em&gt; 부터는 최대 8개까지 지원하기 때문에 16개로 나누어 샘플링한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/gdc2013_ParticleShadowsAndCacheEfficientPost_53.png&quot; alt=&quot;gdc2013_ParticleShadowsAndCacheEfficientPost_53&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://developer.nvidia.com/sites/default/files/akamai/gamedev/docs/BAVOIL_ParticleShadowsAndCacheEfficientPost.pdf&quot;&gt;GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;다음은 나누어진 각각의 텍스쳐를 샘플링하여 원하는 알고리즘으로 결과를 낸다. 조각난 텍스쳐 한개당 한번 &lt;em&gt;DrawCall&lt;/em&gt; 을 걸어준다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/gdc2013_ParticleShadowsAndCacheEfficientPost_54.png&quot; alt=&quot;gdc2013_ParticleShadowsAndCacheEfficientPost_54&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://developer.nvidia.com/sites/default/files/akamai/gamedev/docs/BAVOIL_ParticleShadowsAndCacheEfficientPost.pdf&quot;&gt;GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Deintereave&lt;/em&gt; 를 하기전까지는 넓은 범위의 텍스쳐를 샘플링하여 캐시 효율이 많이 떨어졌지만 텍스쳐를 나누어 각각 할때마다 처리를 하게되니 캐시 효율의 이득을 얻었다. 또한 각각의 &lt;em&gt;DrawCall&lt;/em&gt; 마다 텍스쳐의 용량이 조금만 필요하게 되니 대역폭의 이득도 얻게 된다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/gdc2013_ParticleShadowsAndCacheEfficientPost_55.png&quot; alt=&quot;gdc2013_ParticleShadowsAndCacheEfficientPost_55&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://developer.nvidia.com/sites/default/files/akamai/gamedev/docs/BAVOIL_ParticleShadowsAndCacheEfficientPost.pdf&quot;&gt;GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;한번의 &lt;em&gt;DrawCall&lt;/em&gt; 로 나누어진 결과들을 합친다. &lt;em&gt;Deintereaved Texturing&lt;/em&gt; 은 여기서 끝이다. 실제로 &lt;em&gt;HBAO+&lt;/em&gt; 는 16개의 텍스쳐로 나누어 샘플링한다. &lt;em&gt;Multiple Render Target&lt;/em&gt; 이 8개까지 지원되어 16개로 &lt;em&gt;Deintereave&lt;/em&gt; 하려면 2번 &lt;em&gt;DrawCall&lt;/em&gt; 을 해야한다. 또한 샘플링은 16번 &lt;em&gt;DrawCall&lt;/em&gt; 을 하여 계산한다. 그래서 한번 &lt;em&gt;Deintereaved Texturing&lt;/em&gt; 을 사용하여 &lt;em&gt;Post-Processing&lt;/em&gt; 처리하려면 약 20번의 &lt;em&gt;DrawCall&lt;/em&gt; 을 계산해야 한다. 절대적으로 큰 숫자가 아니기 때문에 크게 신경쓸 필요는 없어보인다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/gdc2013_ParticleShadowsAndCacheEfficientPost_62.png&quot; alt=&quot;gdc2013_ParticleShadowsAndCacheEfficientPost_62&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://developer.nvidia.com/sites/default/files/akamai/gamedev/docs/BAVOIL_ParticleShadowsAndCacheEfficientPost.pdf&quot;&gt;GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;엄청난 성과를 거둔게 보인다. 캐시 히트 확률이 굉장히 올라갔고, 시간도 많이 절약했다. &lt;em&gt;HBAO+&lt;/em&gt; 의 성능향상을 시켜준 것이 이 &lt;em&gt;Deinterleaved Texturing&lt;/em&gt; 인듯하다.&lt;/p&gt;

&lt;h2&gt;Reconstruction of Normal&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;HBAO+&lt;/em&gt; 는 기본적으로 &lt;em&gt;Depth&lt;/em&gt; 와 &lt;em&gt;Normal&lt;/em&gt; 을 통해서 계산한다. 그렇기 때문에 외부에서 &lt;em&gt;Normal&lt;/em&gt; 데이터를 넣어주거나 직접 만들어야 한다. 보통 &lt;em&gt;Deffered Rendering&lt;/em&gt; 을 차용하는 시스템들은 간단하게 &lt;em&gt;GBuffer&lt;/em&gt; 의 &lt;em&gt;Normal&lt;/em&gt; 데이터만 넣어주면 된다. &lt;em&gt;Normal&lt;/em&gt; 데이터를 가져오는 코드가 있으니 조금만 수정하여 사용하면 된다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Normal&lt;/em&gt; 데이터가 없는 경우에는 라이브러리 내에 직접 계산한다. 계산하는 픽셀을 기준으로 상하,좌우별로 &lt;em&gt;Depth&lt;/em&gt; 와 화면상의 좌표계를 이용하여 &lt;em&gt;View-Space&lt;/em&gt; 의 위치를 구한다음 위치가 상하, 좌우별로 가까운 픽셀의 위치 오프셋을 사용해 외적하여 &lt;em&gt;Normal&lt;/em&gt; 값을 구한다.&lt;/p&gt;

&lt;h1&gt;참조 자료&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://docs.nvidia.com/gameworks/content/gameworkslibrary/visualfx/hbao/index.html&quot;&gt;NVidia HBAO+&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://developer.nvidia.com/sites/default/files/akamai/gameworks/samples/DeinterleavedTexturing.pdf&quot;&gt;NVidia : Deintereaved Texturing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://developer.nvidia.com/sites/default/files/akamai/gamedev/docs/BAVOIL_ParticleShadowsAndCacheEfficientPost.pdf&quot;&gt;GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.gdcvault.com/play/1017623/Advanced-Visual-Effects-with-DirectX&quot;&gt;GDCVault : Particle Shadows &amp;amp; Cache-Efficient Post-Processing Video&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="render" />
      
        <category term="analysis" />
      
        <category term="hbao+" />
      
        <category term="linearize_depth" />
      
        <category term="deintereaved_texturing" />
      

      

      
        <summary type="html">HBAO+ 3.1 버젼을 기준으로 글이 작성되었습니다. 이전 hbao plus analysis 0 글에서 HBAO+ 을 알기위한 기본적인 개념들에 대해서 살펴보았다. 이번 글에서는 HBAO+ 의 구조와 Linearize Depth 와 Deinterleaved Texturing 에 대해서 알아보겠다. HBAO+ Pipeline 출처 : NVIDIA HBAO+ 출처 : NVIDIA HBAO+ 그림이 두개가 있다. 하나는 GBuffer 를 사용할 시 World-Space Normal 버퍼와 Depth Buffer 를 넘겨주어 계산하는 방식과 입력으로 Depth Buffer 만 넘겨서 Normal 데이터를 계산하는 두가지 방식에 대한 파이프라인이다. 두가지의 차이는 Normal 데이터에 대한 처리방식만 다르다. 나머지 계산은 다를게 없다. Linearize Depths 코드를 보면 가장 처음에 시작하는 단계는 바로 Linearize Depths 다. 이는 꽤나 알려진 방법이다. 하지만 필자는 HBAO+ 를 볼때 처음 봤기에 어느 정도의 설명을 해놓아야겠다. Linearize Depths 를 알기 위해선 입력된 정점의 위치를 Clipping-Space 로 변환하는 방법이 어떻게 이루어지는지 알고 있어야 한다. 일반적인 오브젝트를 렌더링 할때는 Shader 에 입력으로 들어오는 정점의 기준 공간은 Model-Space(또는 Local-Space) Position 이다. 그래서 MVP 변환을 통해 Rasterizer 가 처리할 수 있도록 Clipping-Space 로 Rasterizer 로 넘어가기 전에 변환해주어야 한다.(전체적인 내용은 Model, View, Projection 변환 에서 확인할 수 있다. ) 그래서 Pixel Shader 로 넘어간 데이터들은 픽셀별로 들어가고, 픽셀별로 들어간 정점들의 위치는 Clipping-Space 로 되어있다. 여기까지 이해했으면 아래 그림을 보자. 출처 : 3D Graphics with OpenGL Basic Theory 위 그림은 View Frustom 과 Clipping Volume 을 보여준다. View Frustom 은 Perspective 방식으로 카메라가 실제로 보여주는 공간을 시각화 한것이고, Clipping Volume 은 MVP 변환에서 Projection 행렬을 사용할시 View Frustom 에서 Clipping Volume 으로 변환되는 볼륨을 시각화 한것이다. Projection 변환은 아래와 같다. 출처 : Stackoverflow : Getting the true z value from the depth buffer Perspective Projection 은 Frustom 기준 위치를 Cube 기준 위치로 바꾸는 연산이기 때문에 실제 좌표의 왜곡이 발생한다. 우리는 Z(Detph) 값이 어떤식으로 왜곡되는지 알아야 한다. 우선 Clipping-Space 로 변환할때, Perspective 형식의 View Frustom 의 zNear, zFar 사이의 Z 값을 [0~1] 값으로 매핑한다. 그러면 zNear, zFar 값을에 따라서 실제 좌표가 바뀐다. 그리고 값 자체가 실제 Z 값과 선형적으로 매핑되지 않는다. 아래 그림을 보자. 출처 : Computer Graphics StackExchange : How am I able to perform perspective projection without a near plane? 그림이 조금 헷갈릴수도 있다. 세로축의 d 값은 Projection 을 한 Z, Depth 값이고 가로축은 World-Space 의 Z 값이다. 조금 헷갈릴수도 있는 부분은 세로축의 기준값이 윗부분이 0이고 아랫부분이 1이다. 이 부분은 신경써서 봐야한다. 이해했다면 변경된 Depth 값은 실제 Z 값과 선형적인 관계가 아니고, 실제 Z 값으로 복원하려면 여러 연산을 해야하기에 HBAO+ 에서는 Depth 값들을 Linearize 하는 과정을 맨 처음에 넣은 것이다. 실제 Z 값으로 복원하는 이유는 간단하다. Linear 하지 않은 Depth 값을 연산시에 사용하면 보다 부정확한 결과가 나오기 때문이다. 특히 SSAO 연산을 할때는 Depth 값이 기본이 되기 때문에 해주어야 한다. 이 단계에서의 결론은 간단하다. Clipping-Space 의 Depth 값을 View-Space 의 Z 값으로 변환하는 단계다. 처리하는 코드는 다른 단계에 비해 짧다. 만약에 넘겨준 Depth 데이터들이 View-Space 인 경우에는 옵션을 통해 처리할 수 있다. Deintereaved Texturing 위의 그림에는 Generate HBAO+ 라고 단순히 뭉뚱그려서 표현했지만 그 안에는 단순한 Horizon based ambient occlusion(HBAO) 계산만 있지는 않다. Deintereaved Texturing 이라는 테크닉과 함께 HBAO 를 계산한다. Computer Engineering 분야의 지식을 응용한 이론으로 개인적으로 이 이론을 접했을 떄 꽤나 충격이였다. 자세한 설명은 GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing 슬라이드의 몇장과 함께 보자. 출처 : GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing Deintereaved Texturing 의 방법은 간단하다. 텍스쳐를 여러장으로 나누어 샘플링을 한 후 각각의 나눠진 텍스쳐를 샘플링한 결과를 하나로 합친다. 슬라이드에는 Post-Processing 을 기준으로 설명이 되어있다. 이점은 생각하면서 보자. 출처 : GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing 한 텍스쳐를 여러장으로 나누는건 Multiple Render Target 을 사용해서 나눈다. 슬라이드는 4개를 기준으로 설명했지만 DirectX10 부터는 최대 8개까지 지원하기 때문에 16개로 나누어 샘플링한다. 출처 : GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing 다음은 나누어진 각각의 텍스쳐를 샘플링하여 원하는 알고리즘으로 결과를 낸다. 조각난 텍스쳐 한개당 한번 DrawCall 을 걸어준다. 출처 : GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing Deintereave 를 하기전까지는 넓은 범위의 텍스쳐를 샘플링하여 캐시 효율이 많이 떨어졌지만 텍스쳐를 나누어 각각 할때마다 처리를 하게되니 캐시 효율의 이득을 얻었다. 또한 각각의 DrawCall 마다 텍스쳐의 용량이 조금만 필요하게 되니 대역폭의 이득도 얻게 된다. 출처 : GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing 한번의 DrawCall 로 나누어진 결과들을 합친다. Deintereaved Texturing 은 여기서 끝이다. 실제로 HBAO+ 는 16개의 텍스쳐로 나누어 샘플링한다. Multiple Render Target 이 8개까지 지원되어 16개로 Deintereave 하려면 2번 DrawCall 을 해야한다. 또한 샘플링은 16번 DrawCall 을 하여 계산한다. 그래서 한번 Deintereaved Texturing 을 사용하여 Post-Processing 처리하려면 약 20번의 DrawCall 을 계산해야 한다. 절대적으로 큰 숫자가 아니기 때문에 크게 신경쓸 필요는 없어보인다. 출처 : GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing 엄청난 성과를 거둔게 보인다. 캐시 히트 확률이 굉장히 올라갔고, 시간도 많이 절약했다. HBAO+ 의 성능향상을 시켜준 것이 이 Deinterleaved Texturing 인듯하다. Reconstruction of Normal HBAO+ 는 기본적으로 Depth 와 Normal 을 통해서 계산한다. 그렇기 때문에 외부에서 Normal 데이터를 넣어주거나 직접 만들어야 한다. 보통 Deffered Rendering 을 차용하는 시스템들은 간단하게 GBuffer 의 Normal 데이터만 넣어주면 된다. Normal 데이터를 가져오는 코드가 있으니 조금만 수정하여 사용하면 된다. Normal 데이터가 없는 경우에는 라이브러리 내에 직접 계산한다. 계산하는 픽셀을 기준으로 상하,좌우별로 Depth 와 화면상의 좌표계를 이용하여 View-Space 의 위치를 구한다음 위치가 상하, 좌우별로 가까운 픽셀의 위치 오프셋을 사용해 외적하여 Normal 값을 구한다. 참조 자료 NVidia HBAO+ NVidia : Deintereaved Texturing GDC2013 : Particle Shadows &amp;amp; Cache-Efficient Post-Processing GDCVault : Particle Shadows &amp;amp; Cache-Efficient Post-Processing Video</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Hbao Plus Analysis 0</title>
      
      <link href="https://hrmrzizon.github.io/2017/11/15/hbao-plus-analysis-0/" rel="alternate" type="text/html" title="Hbao Plus Analysis 0" />
      <published>2017-11-15T00:00:00+00:00</published>
      <updated>2017-11-15T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2017/11/15/hbao-plus-analysis-0</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2017/11/15/hbao-plus-analysis-0/">&lt;p&gt;게임에서 쓰이는 실시간 렌더링에서 빛과 물체들의 상호작용을 완벽하게 현실적으로 표현하는 거의 불가능하다. 하지만 이를 위해 수십년동안 많은 엔지니어와 연구자들이 노력하여 부분적이고 제한된 환경에서의 빛과 물체의 상호작용을 현실 세계와 비슷하게 따라잡고 있다. 이번 글에서 살펴볼 것은 &lt;em&gt;Screen-Space Ambient Occlusion(SSAO)&lt;/em&gt; 기반의 &lt;em&gt;HBAO+&lt;/em&gt; 라는 라이브러리에 대해서 알아볼 것이다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;HBAO+&lt;/em&gt; 는 NVidia 에서 만든 라이브러리로써, 현재 &lt;a href=&quot;https://developer.nvidia.com/shadowworks&quot;&gt;&lt;em&gt;ShadowWorks&lt;/em&gt;&lt;/a&gt; 라는 프로젝트에 포함되어 있다. &lt;a href=&quot;https://developer.nvidia.com/shadowworks&quot;&gt;&lt;em&gt;ShadowWorks&lt;/em&gt;&lt;/a&gt; 에는 &lt;em&gt;HBAO+&lt;/em&gt; 뿐만 아니라 &lt;em&gt;ShadowLib&lt;/em&gt; 이라는 그림자 렌더링을 위한 라이브러리로써 HFTS, PCSS, CSM 등 많은 기능들을 포함하고 있는 라이브러리가 있다. 현재 &lt;em&gt;ShadowLib&lt;/em&gt; 은 오픈소스가 아니지만 이번에 알아볼 &lt;em&gt;HBAO+&lt;/em&gt; 는 &lt;em&gt;Github&lt;/em&gt; 에서 소스를 받을 수 있다. 이에 대한 자세한 사항은 &lt;a href=&quot;https://developer.nvidia.com/gameworks-source-github&quot;&gt;“Access GameWorks Source on Github”&lt;/a&gt; 에서 확인할 수 있다.&lt;/p&gt;

&lt;h3&gt;Ambient Occlusion?&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Ambient Occlusion&lt;/em&gt; 이란 말은 처음 들어본 사람에게는 생소한 말이지만 한번이라도 들어본 사람들에게는 꽤나 익숙한 말일 것이다. 빠른 이해를 위해 아래 그림을 보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/220px-AmbientOcclusion_German.jpg&quot; alt=&quot;Wikipedia : Ambient Occlusion Image&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://en.wikipedia.org/wiki/Ambient_occlusion&quot;&gt;Wikipedia : Ambient Occlusion&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;가장 위의 이미지를 보자. 단순한 렌더링이 아니라 반사를 구현해놓아서 꽤나 사실적이다. 하지만 여기서 더욱더 사실적으로 표현이 가능하다. 중간의 이미지를 보면 &lt;em&gt;Ambient Occlusion&lt;/em&gt; 이 무엇인지 쉽게 알 수 있다. 흰색 물체와 흰색 배경과 함께 아래 구석에 어두워진 것을 볼 수 있다. 이를 &lt;em&gt;Ambient Occlusion&lt;/em&gt; 이라고 한다.&lt;/p&gt;

&lt;p&gt;글로 간단하게 설명하자면 물체가 모이면 좁은 공간이 생기게 되고, 구석이면 구석일 수록 직접 들어오는 빛은 있으나 반사되어 오는 빛이 적어지므로 어두워진다. 이를 말하는 것이바로 &lt;em&gt;Ambient Occlusion&lt;/em&gt; 이다. &lt;em&gt;Ambient&lt;/em&gt; 는 주변을 뜻하고, &lt;em&gt;Occlusion&lt;/em&gt; 은 무언가 가리는 것을 뜻한다. 빛을 가려서 주변에 보이는 것이 어두워지는 것을 말하는 것이다.&lt;/p&gt;

&lt;p&gt;보통 &lt;em&gt;Ambient Occlusion&lt;/em&gt; 은 개발시에 미리 시간을 들여 계산해 미리 저장해놓은 다음 실시간으로 저장된 데이터를 읽어서 사용하는 &lt;em&gt;precomputed AO&lt;/em&gt; 방식으로 사용한다. 이유는 간단하다. 실시간으로 계산되기에는 요즘 컴퓨터로는 연산량을 버틸 수 없어서 그렇다. 월드가 복잡할수록, 넓을수록, 고퀄리티의 &lt;em&gt;AO&lt;/em&gt; 를 계산할수록 시간이 대폭 증가한다. 하지만 이는 물체와 물체간의 &lt;em&gt;AO&lt;/em&gt; 를 계산할때의 이야기이다.&lt;/p&gt;

&lt;h3&gt;Screen-Space AO?&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Deffered Rendering&lt;/em&gt; 이 많이 쓰이면서 이를 위한 많은 기술들이 연구되었다. &lt;em&gt;Deffered Rendering&lt;/em&gt; 의 대부분의 기술들은 이름앞에 보통 &lt;em&gt;Screen-Space&lt;/em&gt; 라는 단어를 달고 나왔다. &lt;em&gt;Screen-Space&lt;/em&gt; 란 &lt;em&gt;Geometry Stage&lt;/em&gt; 아닌 &lt;em&gt;Rasterizer Stage&lt;/em&gt; 로 넘어가서 처리되는 부분을 말한다. 말 뜻대로 해석한다면 2D 이미지의 공간이라는 뜻도 되겠다. 다만 &lt;em&gt;Pixel Shader&lt;/em&gt; 로 넘어간다고 해서 &lt;em&gt;Depth(Z value)&lt;/em&gt; 가 사라지지는 않기 때문에 이를 온전히 2D 공간이라고도 볼 수는 없다. 실제 좌표계는 &lt;em&gt;Clipping-Space&lt;/em&gt; 로 되어 있을 것이다.(x,y,z 가 -1 ~ 1 범위의 좌표로 되어있는 공간, Driver API 에 따라 조금씩 다르다.) 쉽게 이해하기 위해 아래 그림을 보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/DeferredLighting.jpg&quot; alt=&quot;Deffered Rendering&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;http://tower22.blogspot.kr/2010/11/from-deferred-to-inferred-part-uno.html&quot;&gt;From Deferred to Inferred, part uno&lt;/a&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위 그림은 &lt;em&gt;Deffered Rendering&lt;/em&gt; 의 중요한 특징을 나타내는 그림이다. 이 데이터들을 &lt;em&gt;GBuffer&lt;/em&gt; 라고 한다. 저 결과들은 &lt;em&gt;Multi Render Target&lt;/em&gt; 을 통해 한 &lt;em&gt;Pixel Shader&lt;/em&gt; 에서 나온 결과물들이다. 즉 한 장면을 렌더링해서 2D 버퍼안에 데이터들을 픽셀별로 저장한 것이다. 각각 저마다 필요한 정보들을 담고 있다. 우리가 알아볼 &lt;em&gt;Screen-Space AO&lt;/em&gt; 또한 이런식으로 데이터를 처리한다. 위 그림에는 나와있지 않지만 하드웨어 차원에서 &lt;em&gt;Depth Buffer&lt;/em&gt; 를 지원한다. &lt;em&gt;Pixel Shader&lt;/em&gt; 를 실행한 후 알아서 &lt;em&gt;Depth Buffer&lt;/em&gt; 에 Z 값을 저장해준다. &lt;em&gt;Screen-Space AO&lt;/em&gt; 는 &lt;em&gt;Depth Buffer&lt;/em&gt; 를 이용하여 계산한다. 물론 &lt;em&gt;Depth Buffer&lt;/em&gt; 뿐만아니라 &lt;em&gt;GBuffer&lt;/em&gt; 에서 &lt;em&gt;Normal&lt;/em&gt; 데이터까지 사용하여 할 수도 있다.&lt;/p&gt;

&lt;p&gt;하지만 &lt;em&gt;SSAO&lt;/em&gt; 는 정공법이 아니다. 모든 &lt;em&gt;AO&lt;/em&gt; 를 표현할 수 없으며 디테일한 &lt;em&gt;AO&lt;/em&gt; 밖에 표현하지 못한다. 그렇기에 이는 부가적인 방법으로 사용되어야 한다. &lt;em&gt;SSAO&lt;/em&gt; 를 활용하기 가장 좋은 장면은 화면에서 작게 표시되는 오브젝트들이 조밀하게 많이 있을 떄나 그려지는 오브젝트의 디테일이 많을 때다. 이럴때 &lt;em&gt;SSAO&lt;/em&gt; 를 표현하면 괜찮은 결과가 나온다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/0cf69542-8ff5-422a-8d01-f11bd65ab62e_scaled.jpg&quot; alt=&quot;Unity Technology : SSAO&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://forum.unity.com/threads/ssao-pro-high-quality-screen-space-ambient-occlusion.274003/page-5&quot;&gt;Unity Forum&lt;/a&gt;
&lt;/center&gt;
&lt;center&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;게임에서는 조금이라도 복잡한 메시를 쓸 수 밖에 없기 때문에 결국 왠만한 게임에는 &lt;em&gt;SSAO&lt;/em&gt; 를 넣는 것이 괜찮은 선택이 된것이다. 하지만 초기에 제안된 &lt;em&gt;SSAO&lt;/em&gt; 구현물들은 대부분 꽤나 시간을 잡아먹었었다. 그래서 시간에 따른 선택이 되었지만 &lt;em&gt;HBAO+&lt;/em&gt; 와 같은 개량된 기법이 여러개 등장하여 퍼포먼스를 다른 곳에 쓸 수 있게 되었다. &lt;a href=&quot;https://www.geforce.com/hardware/technology/hbao-plus/technology&quot;&gt;Geforce : HBAO+ Technology&lt;/a&gt; 에서 &lt;em&gt;SSAO&lt;/em&gt; 테크닉에 따른 벤치마킹을 볼 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/images/hbao_bench.png&quot; alt=&quot;benchmark&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;
&lt;center&gt;출처 : &lt;a href=&quot;https://www.geforce.com/hardware/technology/hbao-plus/technology&quot;&gt;Geforce : HBAO+ Technology&lt;/a&gt;
&lt;/center&gt;
&lt;center&gt;&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;우리가 살펴볼 것은 가장 아래에 있는 &lt;em&gt;HBAO+&lt;/em&gt; 에 대한 것들이다. &lt;em&gt;Resolution&lt;/em&gt; 은 &lt;em&gt;Depth Buffer&lt;/em&gt; 의 해상도를 뜻한다. 당연히 큰 사이즈여야 디테일한 것까지 표현할 수 있다. 가장 왼쪽에 있는 것은 시간이다. &lt;em&gt;HBAO+&lt;/em&gt; 가 2.4ms 로 조금 느리지만 그 오른쪽에 있는 &lt;em&gt;Occlusion Samples Per AO Pixel&lt;/em&gt; 의 숫자와 같이 비교하면 이야기가 다르다. 이는 한 픽셀별로 몇번 다른 텍스쳐의 데이터 샘플링 숫자다. &lt;em&gt;HBAO&lt;/em&gt; 는 4번만 하지만, &lt;em&gt;HBAO+&lt;/em&gt; 는 이의 9배인 36번이다. 샘플링을 많이하게 되면 더욱더 사실적인 &lt;em&gt;SSAO&lt;/em&gt; 를 표현할 수 있다. 샘플링 숫자에 비하면 시간은 내어줄 수 있는 자원인 것이다.&lt;/p&gt;

&lt;p&gt;이 글에서는 &lt;em&gt;HBAO+&lt;/em&gt; 를 분석하기 전 배경이 되는 개념에 대해서 알아보았다. 다음 &lt;a href=&quot;/2017/11/15/hbao-plus-analysis-1/&quot;&gt;hbao plus analysis 1&lt;/a&gt; 에서는 &lt;em&gt;HBAO+&lt;/em&gt; 라이브러리에 대한 본격적인 분석을 해보려고 한다.&lt;/p&gt;

&lt;h1&gt;참조 자료&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Ambient_occlusion&quot;&gt;Wikipedia : Ambient Occlusion&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://developer.nvidia.com/developer-program&quot;&gt;NVidia Developer Program&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.geforce.com/hardware/technology/hbao-plus/technology&quot;&gt;Geforce : HBAO+ Technology&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://forums.geforce.co.kr/index.php?document_srl=12616&amp;amp;mid=geforce&quot;&gt;한국 지포스 포럼 : HBAO+&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="render" />
      
        <category term="analysis" />
      
        <category term="hbao+" />
      

      

      
        <summary type="html">게임에서 쓰이는 실시간 렌더링에서 빛과 물체들의 상호작용을 완벽하게 현실적으로 표현하는 거의 불가능하다. 하지만 이를 위해 수십년동안 많은 엔지니어와 연구자들이 노력하여 부분적이고 제한된 환경에서의 빛과 물체의 상호작용을 현실 세계와 비슷하게 따라잡고 있다. 이번 글에서 살펴볼 것은 Screen-Space Ambient Occlusion(SSAO) 기반의 HBAO+ 라는 라이브러리에 대해서 알아볼 것이다. HBAO+ 는 NVidia 에서 만든 라이브러리로써, 현재 ShadowWorks 라는 프로젝트에 포함되어 있다. ShadowWorks 에는 HBAO+ 뿐만 아니라 ShadowLib 이라는 그림자 렌더링을 위한 라이브러리로써 HFTS, PCSS, CSM 등 많은 기능들을 포함하고 있는 라이브러리가 있다. 현재 ShadowLib 은 오픈소스가 아니지만 이번에 알아볼 HBAO+ 는 Github 에서 소스를 받을 수 있다. 이에 대한 자세한 사항은 “Access GameWorks Source on Github” 에서 확인할 수 있다. Ambient Occlusion? Ambient Occlusion 이란 말은 처음 들어본 사람에게는 생소한 말이지만 한번이라도 들어본 사람들에게는 꽤나 익숙한 말일 것이다. 빠른 이해를 위해 아래 그림을 보자. 출처 : Wikipedia : Ambient Occlusion 가장 위의 이미지를 보자. 단순한 렌더링이 아니라 반사를 구현해놓아서 꽤나 사실적이다. 하지만 여기서 더욱더 사실적으로 표현이 가능하다. 중간의 이미지를 보면 Ambient Occlusion 이 무엇인지 쉽게 알 수 있다. 흰색 물체와 흰색 배경과 함께 아래 구석에 어두워진 것을 볼 수 있다. 이를 Ambient Occlusion 이라고 한다. 글로 간단하게 설명하자면 물체가 모이면 좁은 공간이 생기게 되고, 구석이면 구석일 수록 직접 들어오는 빛은 있으나 반사되어 오는 빛이 적어지므로 어두워진다. 이를 말하는 것이바로 Ambient Occlusion 이다. Ambient 는 주변을 뜻하고, Occlusion 은 무언가 가리는 것을 뜻한다. 빛을 가려서 주변에 보이는 것이 어두워지는 것을 말하는 것이다. 보통 Ambient Occlusion 은 개발시에 미리 시간을 들여 계산해 미리 저장해놓은 다음 실시간으로 저장된 데이터를 읽어서 사용하는 precomputed AO 방식으로 사용한다. 이유는 간단하다. 실시간으로 계산되기에는 요즘 컴퓨터로는 연산량을 버틸 수 없어서 그렇다. 월드가 복잡할수록, 넓을수록, 고퀄리티의 AO 를 계산할수록 시간이 대폭 증가한다. 하지만 이는 물체와 물체간의 AO 를 계산할때의 이야기이다. Screen-Space AO? Deffered Rendering 이 많이 쓰이면서 이를 위한 많은 기술들이 연구되었다. Deffered Rendering 의 대부분의 기술들은 이름앞에 보통 Screen-Space 라는 단어를 달고 나왔다. Screen-Space 란 Geometry Stage 아닌 Rasterizer Stage 로 넘어가서 처리되는 부분을 말한다. 말 뜻대로 해석한다면 2D 이미지의 공간이라는 뜻도 되겠다. 다만 Pixel Shader 로 넘어간다고 해서 Depth(Z value) 가 사라지지는 않기 때문에 이를 온전히 2D 공간이라고도 볼 수는 없다. 실제 좌표계는 Clipping-Space 로 되어 있을 것이다.(x,y,z 가 -1 ~ 1 범위의 좌표로 되어있는 공간, Driver API 에 따라 조금씩 다르다.) 쉽게 이해하기 위해 아래 그림을 보자. 출처 : From Deferred to Inferred, part uno 위 그림은 Deffered Rendering 의 중요한 특징을 나타내는 그림이다. 이 데이터들을 GBuffer 라고 한다. 저 결과들은 Multi Render Target 을 통해 한 Pixel Shader 에서 나온 결과물들이다. 즉 한 장면을 렌더링해서 2D 버퍼안에 데이터들을 픽셀별로 저장한 것이다. 각각 저마다 필요한 정보들을 담고 있다. 우리가 알아볼 Screen-Space AO 또한 이런식으로 데이터를 처리한다. 위 그림에는 나와있지 않지만 하드웨어 차원에서 Depth Buffer 를 지원한다. Pixel Shader 를 실행한 후 알아서 Depth Buffer 에 Z 값을 저장해준다. Screen-Space AO 는 Depth Buffer 를 이용하여 계산한다. 물론 Depth Buffer 뿐만아니라 GBuffer 에서 Normal 데이터까지 사용하여 할 수도 있다. 하지만 SSAO 는 정공법이 아니다. 모든 AO 를 표현할 수 없으며 디테일한 AO 밖에 표현하지 못한다. 그렇기에 이는 부가적인 방법으로 사용되어야 한다. SSAO 를 활용하기 가장 좋은 장면은 화면에서 작게 표시되는 오브젝트들이 조밀하게 많이 있을 떄나 그려지는 오브젝트의 디테일이 많을 때다. 이럴때 SSAO 를 표현하면 괜찮은 결과가 나온다. 출처 : Unity Forum 게임에서는 조금이라도 복잡한 메시를 쓸 수 밖에 없기 때문에 결국 왠만한 게임에는 SSAO 를 넣는 것이 괜찮은 선택이 된것이다. 하지만 초기에 제안된 SSAO 구현물들은 대부분 꽤나 시간을 잡아먹었었다. 그래서 시간에 따른 선택이 되었지만 HBAO+ 와 같은 개량된 기법이 여러개 등장하여 퍼포먼스를 다른 곳에 쓸 수 있게 되었다. Geforce : HBAO+ Technology 에서 SSAO 테크닉에 따른 벤치마킹을 볼 수 있다. 출처 : Geforce : HBAO+ Technology 우리가 살펴볼 것은 가장 아래에 있는 HBAO+ 에 대한 것들이다. Resolution 은 Depth Buffer 의 해상도를 뜻한다. 당연히 큰 사이즈여야 디테일한 것까지 표현할 수 있다. 가장 왼쪽에 있는 것은 시간이다. HBAO+ 가 2.4ms 로 조금 느리지만 그 오른쪽에 있는 Occlusion Samples Per AO Pixel 의 숫자와 같이 비교하면 이야기가 다르다. 이는 한 픽셀별로 몇번 다른 텍스쳐의 데이터 샘플링 숫자다. HBAO 는 4번만 하지만, HBAO+ 는 이의 9배인 36번이다. 샘플링을 많이하게 되면 더욱더 사실적인 SSAO 를 표현할 수 있다. 샘플링 숫자에 비하면 시간은 내어줄 수 있는 자원인 것이다. 이 글에서는 HBAO+ 를 분석하기 전 배경이 되는 개념에 대해서 알아보았다. 다음 hbao plus analysis 1 에서는 HBAO+ 라이브러리에 대한 본격적인 분석을 해보려고 한다. 참조 자료 Wikipedia : Ambient Occlusion NVidia Developer Program Geforce : HBAO+ Technology 한국 지포스 포럼 : HBAO+</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Shader Pipeline 4 Geometry Shader</title>
      
      <link href="https://hrmrzizon.github.io/2017/10/31/shader-pipeline-4-geometry-shader/" rel="alternate" type="text/html" title="Shader Pipeline 4 Geometry Shader" />
      <published>2017-10-31T00:00:00+00:00</published>
      <updated>2017-10-31T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2017/10/31/shader-pipeline-4-geometry-shader</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2017/10/31/shader-pipeline-4-geometry-shader/">&lt;p&gt;“&lt;a href=&quot;/2017/10/31/shader-pipeline-3-fragment-shader/&quot;&gt;Fragemnt Shader&lt;/a&gt;” 에서 &lt;em&gt;Fragment Shader&lt;/em&gt; 에 대해 알아보았다. 다음은 &lt;em&gt;Geometry Shader&lt;/em&gt; 에 대해서 써보려 한다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Geometry Shader&lt;/em&gt; 는 쉐이더 파이프라인에서 &lt;em&gt;Rasterizer Stage&lt;/em&gt; 넘어가기 전의 &lt;em&gt;Geometry Stage&lt;/em&gt; 의 마지막 단계로써 이전 쉐이더에서 넘긴  &lt;em&gt;Primitive&lt;/em&gt; 데이터(point, line, triangle..)를 프로그래머가 원하는 복수의 &lt;em&gt;Primitive&lt;/em&gt; 데이터로 변환할 수 있다. 삼각형을 삼각형의 중심을 나타내는 점으로 변환하는 쉐이더를 보자.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C&quot;&gt;[maxvertexcount(1)]
void geom(vertexOutput input[3], inout PointStream&amp;lt;geometryOutput&amp;gt; pointStream)
{
    geometryShaderOutput o;

    o.vertex = (input[0].vertex + input[1].vertex + input[2].vertex) / 3;

    pointStream.Append(o);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;매우 간단한 코드다. 간략하게 설명하자면, 맨 윗줄의 &lt;em&gt;maxvertexcount&lt;/em&gt; 는 해당 지오메트리 쉐이더에서 &lt;em&gt;Stream&lt;/em&gt; 으로 넘길 정점별 데이터의 갯수를 뜻한다. &lt;em&gt;Geometry Shader&lt;/em&gt; 한번당 &lt;em&gt;Stream&lt;/em&gt; 으로 넘길 &lt;em&gt;maxvertexcount&lt;/em&gt; 의 한계는 정해지지 않았지만 크기는 1024 바이트로 정해져 있기 때문에 적절하게 사용해야 겠다. 그 다음줄의 인자들에 대해서 설명하면, 첫번째 &lt;em&gt;vertexOutput input[3]&lt;/em&gt; 은 정해진 프리미티브의 값들을 뜻한다. 여기서는 삼각형을 기준으로 만들었기 때문에 정점별 정보가 3개가 있다. _inout PointStream&lt;geometryOutput&gt; pointStream_ 은 _Geometry Shader_ 의 최종 출력을 해주는 오브젝트다. _PointStream_ 은 점 프리미티브의 데이터를 받는 _Stream_ 으로써, 프리미티브가 다르면 각자 다른것을 사용할 수 있다.([MSDN : Getting Started with the Stream-Output Stage](https://msdn.microsoft.com/en-us/library/windows/desktop/bb205122.aspx)) 부등호 안에 있는 것은 일반적으로 알려진 제너릭이나 템플릿의 형태와 같으니 안에 출력으로 넘길 구조체를 넘겨주면 된다. 함수의 내용은 삼각형을 구성하는 각 정점의 위치의 평균을 구해 하나의 정점 정보만 _Stream_ 에 넘긴다.&lt;/geometryOutput&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Stream&lt;/em&gt; 은 총 두가지의 역할을 한다. 하나는 &lt;em&gt;Rasterizer&lt;/em&gt; 단계로 넘겨서 쉐이더에서 처리를 할 수 있게 하는 통로 역할을 하고, 다른 하나는 드라이버 레벨에서 데이터를 출력해주는 통로 역할을 한다. 두가지의 일을 하기 때문에 &lt;em&gt;Stream&lt;/em&gt; 의 개념으로 추상화한 것인가 싶다. 그리고 하나의 &lt;em&gt;Geometry Shader&lt;/em&gt; 에서 여러개의 &lt;em&gt;Stream&lt;/em&gt; 으로 출력이 가능하긴 하다. 최대 4개의 &lt;em&gt;Stream&lt;/em&gt; 을 사용할 수 있다. &lt;em&gt;Stream&lt;/em&gt; 을 선택해서 데이터를 받아올 수도 있으며, &lt;em&gt;Rasterizer&lt;/em&gt; 로 보낼수도 있다. 자세한 사항은 &lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/ff471424.aspx&quot;&gt;MSDN : How To: Index Multiple Output Streams &lt;/a&gt; 에서 확인하면 되겠다.&lt;/p&gt;

&lt;p&gt;활용할 수 있는 다른 기능이 하나 더 있다. &lt;em&gt;instance&lt;/em&gt; 기능이다. 아래 코드를 보자.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C&quot;&gt;[instance(3)]
[maxvertexcount(1)]
void geom(vertexOutput input[3], uint InstanceID : SV_GSInstanceID, inout PointStream&amp;lt;geometryOutput&amp;gt; pointStream)
{
    geometryShaderOutput o;

    o.vertex = input[InstanceID].vertex;

    pointStream.Append(o);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;해당 코드는 삼각형의 세개의 정점 위치를 넘기는 코드다. 달라진 것은 &lt;em&gt;instance(3)&lt;/em&gt; 코드가 붙고, &lt;em&gt;uint InstanceID : SV_GSInstanceID&lt;/em&gt; 파라미터가 생겨 코드 안에서 이를 활용한다. &lt;em&gt;instance(x)&lt;/em&gt; 에 들어가는 x 는 반복하는 횟수를 뜻하고, &lt;em&gt;InstanceID&lt;/em&gt; 파라미터는 반복하는 인덱스를 뜻한다. 같은 입력을 여러번 받아서 일정한 수만큼 반복하는 것이다.  &lt;em&gt;instance&lt;/em&gt; 속성에 들어가는 숫자의 한계는 32까지다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Geometry Shader&lt;/em&gt; 는 &lt;em&gt;Shader Model 4.0&lt;/em&gt; 에서 추가되었으며 뒤에 추가적으로 알아본 &lt;em&gt;multiple stream&lt;/em&gt; 과 &lt;em&gt;instance&lt;/em&gt; 키워드는 &lt;em&gt;Shader Model 5.0&lt;/em&gt; 에서 확장된 기능들이다.&lt;/p&gt;

&lt;h1&gt;참조 자료&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://msdn.microsoft.com/ko-kr/library/windows/desktop/bb509609.aspx&quot;&gt;MSDN : Geometry-Shader Object&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/bb205122.aspx&quot;&gt;MSDN : Getting Started with the Stream-Output Stage&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/ff471424.aspx&quot;&gt;MSDN : How To: Index Multiple Output Streams&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/ff471425.aspx&quot;&gt;MSDN : How To: Instance a Geometry Shader&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.gamedev.net/forums/topic/600141-limit-on-maxvertexcount-gs/&quot;&gt;GameDev : limit on maxvertexcount() GS&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="render" />
      
        <category term="shader" />
      
        <category term="hlsl" />
      

      

      
        <summary type="html">“Fragemnt Shader” 에서 Fragment Shader 에 대해 알아보았다. 다음은 Geometry Shader 에 대해서 써보려 한다. Geometry Shader 는 쉐이더 파이프라인에서 Rasterizer Stage 넘어가기 전의 Geometry Stage 의 마지막 단계로써 이전 쉐이더에서 넘긴 Primitive 데이터(point, line, triangle..)를 프로그래머가 원하는 복수의 Primitive 데이터로 변환할 수 있다. 삼각형을 삼각형의 중심을 나타내는 점으로 변환하는 쉐이더를 보자. [maxvertexcount(1)] void geom(vertexOutput input[3], inout PointStream&amp;lt;geometryOutput&amp;gt; pointStream) { geometryShaderOutput o; o.vertex = (input[0].vertex + input[1].vertex + input[2].vertex) / 3; pointStream.Append(o); } 매우 간단한 코드다. 간략하게 설명하자면, 맨 윗줄의 maxvertexcount 는 해당 지오메트리 쉐이더에서 Stream 으로 넘길 정점별 데이터의 갯수를 뜻한다. Geometry Shader 한번당 Stream 으로 넘길 maxvertexcount 의 한계는 정해지지 않았지만 크기는 1024 바이트로 정해져 있기 때문에 적절하게 사용해야 겠다. 그 다음줄의 인자들에 대해서 설명하면, 첫번째 vertexOutput input[3] 은 정해진 프리미티브의 값들을 뜻한다. 여기서는 삼각형을 기준으로 만들었기 때문에 정점별 정보가 3개가 있다. _inout PointStream pointStream_ 은 _Geometry Shader_ 의 최종 출력을 해주는 오브젝트다. _PointStream_ 은 점 프리미티브의 데이터를 받는 _Stream_ 으로써, 프리미티브가 다르면 각자 다른것을 사용할 수 있다.([MSDN : Getting Started with the Stream-Output Stage](https://msdn.microsoft.com/en-us/library/windows/desktop/bb205122.aspx)) 부등호 안에 있는 것은 일반적으로 알려진 제너릭이나 템플릿의 형태와 같으니 안에 출력으로 넘길 구조체를 넘겨주면 된다. 함수의 내용은 삼각형을 구성하는 각 정점의 위치의 평균을 구해 하나의 정점 정보만 _Stream_ 에 넘긴다. Stream 은 총 두가지의 역할을 한다. 하나는 Rasterizer 단계로 넘겨서 쉐이더에서 처리를 할 수 있게 하는 통로 역할을 하고, 다른 하나는 드라이버 레벨에서 데이터를 출력해주는 통로 역할을 한다. 두가지의 일을 하기 때문에 Stream 의 개념으로 추상화한 것인가 싶다. 그리고 하나의 Geometry Shader 에서 여러개의 Stream 으로 출력이 가능하긴 하다. 최대 4개의 Stream 을 사용할 수 있다. Stream 을 선택해서 데이터를 받아올 수도 있으며, Rasterizer 로 보낼수도 있다. 자세한 사항은 MSDN : How To: Index Multiple Output Streams 에서 확인하면 되겠다. 활용할 수 있는 다른 기능이 하나 더 있다. instance 기능이다. 아래 코드를 보자. [instance(3)] [maxvertexcount(1)] void geom(vertexOutput input[3], uint InstanceID : SV_GSInstanceID, inout PointStream&amp;lt;geometryOutput&amp;gt; pointStream) { geometryShaderOutput o; o.vertex = input[InstanceID].vertex; pointStream.Append(o); } 해당 코드는 삼각형의 세개의 정점 위치를 넘기는 코드다. 달라진 것은 instance(3) 코드가 붙고, uint InstanceID : SV_GSInstanceID 파라미터가 생겨 코드 안에서 이를 활용한다. instance(x) 에 들어가는 x 는 반복하는 횟수를 뜻하고, InstanceID 파라미터는 반복하는 인덱스를 뜻한다. 같은 입력을 여러번 받아서 일정한 수만큼 반복하는 것이다. instance 속성에 들어가는 숫자의 한계는 32까지다. Geometry Shader 는 Shader Model 4.0 에서 추가되었으며 뒤에 추가적으로 알아본 multiple stream 과 instance 키워드는 Shader Model 5.0 에서 확장된 기능들이다. 참조 자료 MSDN : Geometry-Shader Object MSDN : Getting Started with the Stream-Output Stage MSDN : How To: Index Multiple Output Streams MSDN : How To: Instance a Geometry Shader GameDev : limit on maxvertexcount() GS</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Shader Pipeline 3 Fragment Shader</title>
      
      <link href="https://hrmrzizon.github.io/2017/10/31/shader-pipeline-3-fragment-shader/" rel="alternate" type="text/html" title="Shader Pipeline 3 Fragment Shader" />
      <published>2017-10-31T00:00:00+00:00</published>
      <updated>2017-10-31T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2017/10/31/shader-pipeline-3-fragment-shader</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2017/10/31/shader-pipeline-3-fragment-shader/">&lt;p&gt;이전 글 : “&lt;a href=&quot;/2017/10/31/shader-pipeline-2-rasterizer/&quot;&gt;Rasterizer&lt;/a&gt;” 에서 &lt;em&gt;Rasterizer&lt;/em&gt; 에 대해 알아보았다. 이번에는 &lt;em&gt;Fragment Shader&lt;/em&gt; 을 알아보자.&lt;/p&gt;

&lt;p&gt;쉐이더 파이프라인에서 &lt;em&gt;Rasterizer&lt;/em&gt; 다음에 실행되는 것은 &lt;em&gt;Programmable Shader&lt;/em&gt; 중에서 &lt;em&gt;Fragment Shader&lt;/em&gt; 이다. &lt;em&gt;Fragment Shader&lt;/em&gt; 은 &lt;em&gt;Pixel Shader&lt;/em&gt; 라고도 불리는데, 이전에 &lt;em&gt;Rasterizer&lt;/em&gt; 에서 조각낸 픽셀들을 단위로 실행되기 때문에 &lt;em&gt;Pixel Shader&lt;/em&gt; 라고 불리기도 한다. 또한 각 픽셀은 조각난 단위이기 때문에 조각의 뜻을 가진 &lt;em&gt;Fragment Shader&lt;/em&gt; 라고도 불린다. 이 글에서는 &lt;em&gt;Fragment Shader&lt;/em&gt; 로 사용할 것이다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Fragment Shader&lt;/em&gt; 의 역할은 굉장히 단순하다. &lt;em&gt;Geometry Stage&lt;/em&gt; 에서 넘어와 &lt;em&gt;Rasterizer&lt;/em&gt; 단계에서 정리된 파라미터를 받고, 해당 픽셀의 색을 반환하면 끝난다. 역할은 단순하지만 그만큼 중요한 것이 &lt;em&gt;Fragment Shader&lt;/em&gt; 다. 마지막으로 픽셀 단위로 보여주는 색을 바꿀 수 있는 &lt;em&gt;Programmable Shader&lt;/em&gt; 로써 반복하는 비용이 꽤나 많아 일반적으로 오래 걸린다고 평가되지만 색을 바꿀 수 있어 그만큼 잘쓰면 굉장한 효과를 낼 수 있는 &lt;em&gt;Programmable Shader&lt;/em&gt; 다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/fragment_shader.jpg&quot; alt=&quot;Fragment Shader&quot; /&gt;&lt;/p&gt;

&lt;p&gt;가장 단순한 형태의 Unity 에서 사용하는 CG/HLSL 쉐이더를 보자.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C&quot;&gt;#pragma fragment frag

struct v2f
{
    float4 vertex : SV_POSITION;
    float4 tangent : TANGENT;
    float3 normal : NORMAL;
    float4 texcoord : TEXCOORD0;
}

/*
  다른 코드 들..
*/

fixed4 frag(v2f i)
{
  return float4(1,1,1,1);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;해당 쉐이더는 단순하게 흰색만 출력해주는 쉐이더다. 그만큼 매우 단순하고 쉽다. 하지만 많은 것들을 표현하려면 &lt;em&gt;frag&lt;/em&gt; 함수의 코드는 점점 길어질 것이다.&lt;/p&gt;

&lt;p&gt;또한 &lt;em&gt;Fragment Shader&lt;/em&gt; 가 실행되는 시점에서 하드웨어, 드라이버 단계에서 지원하는 기능들도 있다. 일반적인 것들에 대해서 이야기 하자면 &lt;em&gt;Depth Buffer&lt;/em&gt; 와 &lt;em&gt;Stencil Buffer&lt;/em&gt; 가 있다. 두가지의 공통점은 각 픽셀 단위별로 데이터를 저장하는 버퍼들이다. &lt;em&gt;Depth Buffer&lt;/em&gt; 는 &lt;em&gt;Clip-Space&lt;/em&gt; 로 변환된 정점 값의 Z 값을 저장하는 용도로 쓰이는 버퍼로, 요즘 개발되거나 쓰이는 기술들은 &lt;em&gt;Depth Buffer&lt;/em&gt; 를 엄청 많이 쓴다. 대표적으로 &lt;em&gt;Depth Pre-Pass&lt;/em&gt; 가 있다. &lt;em&gt;Stencil Buffer&lt;/em&gt; 는 픽셀별로 정수 데이터를 저장해서 사용하는 버퍼로써, 대부분 마스킹을 할 때 쓰인다.&lt;/p&gt;

&lt;h1&gt;참조 자료&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.slideshare.net/kyruie/everything-about-earlyz&quot;&gt;SlideShader : Everything about Early-Z&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="render" />
      
        <category term="shader" />
      
        <category term="hlsl" />
      

      

      
        <summary type="html">이전 글 : “Rasterizer” 에서 Rasterizer 에 대해 알아보았다. 이번에는 Fragment Shader 을 알아보자. 쉐이더 파이프라인에서 Rasterizer 다음에 실행되는 것은 Programmable Shader 중에서 Fragment Shader 이다. Fragment Shader 은 Pixel Shader 라고도 불리는데, 이전에 Rasterizer 에서 조각낸 픽셀들을 단위로 실행되기 때문에 Pixel Shader 라고 불리기도 한다. 또한 각 픽셀은 조각난 단위이기 때문에 조각의 뜻을 가진 Fragment Shader 라고도 불린다. 이 글에서는 Fragment Shader 로 사용할 것이다. Fragment Shader 의 역할은 굉장히 단순하다. Geometry Stage 에서 넘어와 Rasterizer 단계에서 정리된 파라미터를 받고, 해당 픽셀의 색을 반환하면 끝난다. 역할은 단순하지만 그만큼 중요한 것이 Fragment Shader 다. 마지막으로 픽셀 단위로 보여주는 색을 바꿀 수 있는 Programmable Shader 로써 반복하는 비용이 꽤나 많아 일반적으로 오래 걸린다고 평가되지만 색을 바꿀 수 있어 그만큼 잘쓰면 굉장한 효과를 낼 수 있는 Programmable Shader 다. 가장 단순한 형태의 Unity 에서 사용하는 CG/HLSL 쉐이더를 보자. #pragma fragment frag struct v2f { float4 vertex : SV_POSITION; float4 tangent : TANGENT; float3 normal : NORMAL; float4 texcoord : TEXCOORD0; } /* 다른 코드 들.. */ fixed4 frag(v2f i) { return float4(1,1,1,1); } 해당 쉐이더는 단순하게 흰색만 출력해주는 쉐이더다. 그만큼 매우 단순하고 쉽다. 하지만 많은 것들을 표현하려면 frag 함수의 코드는 점점 길어질 것이다. 또한 Fragment Shader 가 실행되는 시점에서 하드웨어, 드라이버 단계에서 지원하는 기능들도 있다. 일반적인 것들에 대해서 이야기 하자면 Depth Buffer 와 Stencil Buffer 가 있다. 두가지의 공통점은 각 픽셀 단위별로 데이터를 저장하는 버퍼들이다. Depth Buffer 는 Clip-Space 로 변환된 정점 값의 Z 값을 저장하는 용도로 쓰이는 버퍼로, 요즘 개발되거나 쓰이는 기술들은 Depth Buffer 를 엄청 많이 쓴다. 대표적으로 Depth Pre-Pass 가 있다. Stencil Buffer 는 픽셀별로 정수 데이터를 저장해서 사용하는 버퍼로써, 대부분 마스킹을 할 때 쓰인다. 참조 자료 SlideShader : Everything about Early-Z</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Shader Pipeline 2 Rasterizer</title>
      
      <link href="https://hrmrzizon.github.io/2017/10/31/shader-pipeline-2-rasterizer/" rel="alternate" type="text/html" title="Shader Pipeline 2 Rasterizer" />
      <published>2017-10-31T00:00:00+00:00</published>
      <updated>2017-10-31T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2017/10/31/shader-pipeline-2-rasterizer</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2017/10/31/shader-pipeline-2-rasterizer/">&lt;p&gt;이전 포스트 “&lt;a href=&quot;/2017/10/30/shader-pipeline-1-vertex-shader/&quot;&gt;Vertex Shader&lt;/a&gt;” 에서 &lt;em&gt;Vertex Shader&lt;/em&gt; 에 대해 간단히 알아보았다. 이번 글에서는 &lt;em&gt;Rasterizer&lt;/em&gt; 에 대해서 간단히 써보려 한다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Rasterizer&lt;/em&gt; 는 쉐이더 파이프라인에 존재하는 고정 기능 단계이다. 간단하게 정의하면, &lt;em&gt;Rasterizer&lt;/em&gt; 이전 단계를 거쳐 나온 Geometry 데이터들(vertex, mesh, …)을 정해진 해상도에 맞춰 픽셀별로 조각내어 주는 단계다. 아래 그림을 보자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i-msdn.sec.s-msft.com/dynimg/IC520311.png&quot; alt=&quot;MSDN : Traignel Rasterization Rule&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이는 일반적인 삼각형 폴리곤을 &lt;em&gt;Rasterizer&lt;/em&gt; 단계에서 어떻게 픽셀로 변환하는지 한눈에 알게 해놓은 사진이다. 겹치는 정도에 따라 검은색에 가깝게 해놓은 것을 볼 수있다. &lt;a href=&quot;https://msdn.microsoft.com/ko-kr/library/windows/desktop/cc627092.aspx&quot;&gt;MSDN : Rasterization Rules&lt;/a&gt; 에서 다른 프리미티브의 &lt;em&gt;Rasterize&lt;/em&gt; 과정도 살펴볼 수 있다.&lt;/p&gt;

&lt;p&gt;쉐이더 파이프라인에서 &lt;em&gt;Rasterizer&lt;/em&gt; 가 가지는 의미도 조금 특별하다. 이는 3차원 메시 데이터를 2차원 이미지 데이터로 바꿔주는 과정이기 때문에 &lt;em&gt;Geometry Stage&lt;/em&gt; 에서 &lt;em&gt;Rasterizer Stage&lt;/em&gt; 로 넘어가는 관문이다.&lt;/p&gt;

&lt;h1&gt;참조 자료&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://msdn.microsoft.com/ko-kr/library/windows/desktop/cc627092.aspx&quot;&gt;MSDN : Rasterization Rules&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="render" />
      
        <category term="hlsl" />
      

      

      
        <summary type="html">이전 포스트 “Vertex Shader” 에서 Vertex Shader 에 대해 간단히 알아보았다. 이번 글에서는 Rasterizer 에 대해서 간단히 써보려 한다. Rasterizer 는 쉐이더 파이프라인에 존재하는 고정 기능 단계이다. 간단하게 정의하면, Rasterizer 이전 단계를 거쳐 나온 Geometry 데이터들(vertex, mesh, …)을 정해진 해상도에 맞춰 픽셀별로 조각내어 주는 단계다. 아래 그림을 보자. 이는 일반적인 삼각형 폴리곤을 Rasterizer 단계에서 어떻게 픽셀로 변환하는지 한눈에 알게 해놓은 사진이다. 겹치는 정도에 따라 검은색에 가깝게 해놓은 것을 볼 수있다. MSDN : Rasterization Rules 에서 다른 프리미티브의 Rasterize 과정도 살펴볼 수 있다. 쉐이더 파이프라인에서 Rasterizer 가 가지는 의미도 조금 특별하다. 이는 3차원 메시 데이터를 2차원 이미지 데이터로 바꿔주는 과정이기 때문에 Geometry Stage 에서 Rasterizer Stage 로 넘어가는 관문이다. 참조 자료 MSDN : Rasterization Rules</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Shader Pipeline 1 Vertex Shader</title>
      
      <link href="https://hrmrzizon.github.io/2017/10/30/shader-pipeline-1-vertex-shader/" rel="alternate" type="text/html" title="Shader Pipeline 1 Vertex Shader" />
      <published>2017-10-30T00:00:00+00:00</published>
      <updated>2017-10-30T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2017/10/30/shader-pipeline-1-vertex-shader</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2017/10/30/shader-pipeline-1-vertex-shader/">&lt;p&gt;이번 글에서 언급할 쉐이더는 &lt;em&gt;Vertex Shader&lt;/em&gt; 다. 한글로는 &lt;em&gt;정점 쉐이더&lt;/em&gt; 라고 보통 말한다. &lt;em&gt;Vertex Shader&lt;/em&gt; 에서 할 수 있는 것은, 정점별로 들어온 정보들을 코딩을 해서 프로그래머가 원하는대로 바꾸어 다음 쉐이더에서 처리할 수 있도록 해주는 &lt;em&gt;Shader&lt;/em&gt; 다.&lt;/p&gt;

&lt;p&gt;Unity 에서의 CG/HLSL 일반적인 &lt;em&gt;Vertex Shader&lt;/em&gt; 코드는 아래와 같다.&lt;/p&gt;

&lt;div class=&quot;language-c highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;cp&quot;&gt;#pragma vertex vert
#include &quot;UnityCG.cginc&quot;
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;appdata_tan&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;float4&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vertex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;POSITION&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;float4&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tangent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TANGENT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;float3&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NORMAL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;float4&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;texcoord&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TEXCOORD0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;struct&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v2f&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;float4&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vertex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SV_POSITION&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;float4&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tangent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TANGENT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;float3&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;NORMAL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;float4&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;texcoord&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TEXCOORD0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;v2f&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;appdata_tan&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;v2f&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vertex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;UNITY_MATRIX_MVP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vertex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tangent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tangent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;texcoord&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;texcoord&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;o&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;cm&quot;&gt;/*
  기타 코드들..
*/&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;굉장히 단순한 &lt;em&gt;Vertex Shader&lt;/em&gt; 코드다. 코드가 단순한 만큼 이 &lt;em&gt;Shader&lt;/em&gt; 는 최소한의 역할만 하고 있다. &lt;em&gt;model-space&lt;/em&gt; 에 있는 정점을 &lt;em&gt;clipping-space&lt;/em&gt; 의 정점으로 변환 시켜 다음으로(fragment shader) 넘긴다. 위에서 위치 데이터를 바꿀 수 있다고 언급했는데, 이 변환은 정상적인 메커니즘을 통해 오브젝트를 출력하려면 &lt;em&gt;Rasterizer Stage&lt;/em&gt; 로 넘어가기전에 반드시 정점값에 적용시켜주어야 하는 변환이다. 해당 변환에 대해서는 &lt;a href=&quot;https://docs.google.com/presentation/d/10VzsjfifKJlRTHDlBq7e8vNBTu4D5jOWUF87KYYGwlk/edit?usp=sharing&quot;&gt;Model, View, Projection&lt;/a&gt;에 설명해 놓았으니 간단하게 참고하길 바란다.&lt;/p&gt;

&lt;p&gt;위 코드에서 보여준 것들은 최소한의 것들이다. 코드를 짜는 것은 프로그래머의 역량이기 때문에 더 창의적인 것들을 할 수 있다. 쉬운 것들 중에서는 표면에서 웨이브를 주어 표면이 일렁이는 것처럼 보이게 할 수 있다. 이는 시간을 키값으로 두어 삼각함수를 이용해 할 수 있겠다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C&quot;&gt;float time;

struct appdata
{
    float4 vertex : POSITION;
};

struct v2f
{
    float4 vertex : SV_POSITION;
}

v2f vert(appdata i)
{
    v2f o;

    i.vertex = i.vertex + i.normal * sin(time + i.vertex.x + i.vertex.z);
    o.vertex = mul(UNITY_MATRIX_MVP, i.vertex);

    return o;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;메쉬는 여러개의 정사각형 모양으로 잘라진 평평한 판의 형태의 메쉬를 준비하고, 간단하게 x 좌표와 z 좌표를 기준으로 오브젝트가 일렁이는 것을 만들어 보았다. 이렇게 &lt;em&gt;Vertex Shader&lt;/em&gt; 를 응용해서 정점 데이터를 프로그래머가 원하는데로 움직일 수 있다. 정점 쉐이더는 사용하기에 크게 어려운점은 없기에 &lt;em&gt;Shader&lt;/em&gt; 를 처음 다룰 때 가지고 놀만하다. 또한 &lt;em&gt;Vertex Shader&lt;/em&gt; 가 가장 응용되기 쉬운 것은 바로 &lt;em&gt;skinning&lt;/em&gt; 이다. &lt;em&gt;skinning&lt;/em&gt; 자체가 정점 데이터들을 움직이고 움직임을 기반으로 바꾸는 것이기 때문에 &lt;em&gt;Vertex Shader&lt;/em&gt; 의 형태가 &lt;em&gt;skinning&lt;/em&gt; 을 적용하기 가장 알맞다.&lt;/p&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="render" />
      
        <category term="shader" />
      
        <category term="cg" />
      
        <category term="hlsl" />
      
        <category term="unity" />
      

      

      
        <summary type="html">이번 글에서 언급할 쉐이더는 Vertex Shader 다. 한글로는 정점 쉐이더 라고 보통 말한다. Vertex Shader 에서 할 수 있는 것은, 정점별로 들어온 정보들을 코딩을 해서 프로그래머가 원하는대로 바꾸어 다음 쉐이더에서 처리할 수 있도록 해주는 Shader 다. Unity 에서의 CG/HLSL 일반적인 Vertex Shader 코드는 아래와 같다. #pragma vertex vert #include &quot;UnityCG.cginc&quot; struct appdata_tan { float4 vertex : POSITION; float4 tangent : TANGENT; float3 normal : NORMAL; float4 texcoord : TEXCOORD0; }; struct v2f { float4 vertex : SV_POSITION; float4 tangent : TANGENT; float3 normal : NORMAL; float4 texcoord : TEXCOORD0; } v2f vert(appdata_tan i) { v2f o; o.vertex = mul(UNITY_MATRIX_MVP, i.vertex); o.tangent = i.tangent; o.normal = i.normal; o.texcoord = i.texcoord; return o; } /* 기타 코드들.. */ 굉장히 단순한 Vertex Shader 코드다. 코드가 단순한 만큼 이 Shader 는 최소한의 역할만 하고 있다. model-space 에 있는 정점을 clipping-space 의 정점으로 변환 시켜 다음으로(fragment shader) 넘긴다. 위에서 위치 데이터를 바꿀 수 있다고 언급했는데, 이 변환은 정상적인 메커니즘을 통해 오브젝트를 출력하려면 Rasterizer Stage 로 넘어가기전에 반드시 정점값에 적용시켜주어야 하는 변환이다. 해당 변환에 대해서는 Model, View, Projection에 설명해 놓았으니 간단하게 참고하길 바란다. 위 코드에서 보여준 것들은 최소한의 것들이다. 코드를 짜는 것은 프로그래머의 역량이기 때문에 더 창의적인 것들을 할 수 있다. 쉬운 것들 중에서는 표면에서 웨이브를 주어 표면이 일렁이는 것처럼 보이게 할 수 있다. 이는 시간을 키값으로 두어 삼각함수를 이용해 할 수 있겠다. float time; struct appdata { float4 vertex : POSITION; }; struct v2f { float4 vertex : SV_POSITION; } v2f vert(appdata i) { v2f o; i.vertex = i.vertex + i.normal * sin(time + i.vertex.x + i.vertex.z); o.vertex = mul(UNITY_MATRIX_MVP, i.vertex); return o; } 메쉬는 여러개의 정사각형 모양으로 잘라진 평평한 판의 형태의 메쉬를 준비하고, 간단하게 x 좌표와 z 좌표를 기준으로 오브젝트가 일렁이는 것을 만들어 보았다. 이렇게 Vertex Shader 를 응용해서 정점 데이터를 프로그래머가 원하는데로 움직일 수 있다. 정점 쉐이더는 사용하기에 크게 어려운점은 없기에 Shader 를 처음 다룰 때 가지고 놀만하다. 또한 Vertex Shader 가 가장 응용되기 쉬운 것은 바로 skinning 이다. skinning 자체가 정점 데이터들을 움직이고 움직임을 기반으로 바꾸는 것이기 때문에 Vertex Shader 의 형태가 skinning 을 적용하기 가장 알맞다.</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Shader Pipeline 0 Opening</title>
      
      <link href="https://hrmrzizon.github.io/2017/10/30/shader-pipeline-0-opening/" rel="alternate" type="text/html" title="Shader Pipeline 0 Opening" />
      <published>2017-10-30T00:00:00+00:00</published>
      <updated>2017-10-30T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2017/10/30/shader-pipeline-0-opening</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2017/10/30/shader-pipeline-0-opening/">&lt;h3&gt;쉐이더 프로그래밍 환경&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Programmable Shader&lt;/em&gt; 들을 정리하기 위해 각 쉐이더별로 한개씩 글을 써보기로 했다. 그 전에 미리 알아야될 것들에 대해 알아보려고 한다. 각각의 &lt;em&gt;Shader&lt;/em&gt; 들은 코더의 입장에서 바라보았을 때는 단지 몇개의 파라미터를 받고 값을 반환하는 함수들이다. 하지만 일반적으로 알고있는 함수들과는 조금 다르게 실행된다. 첫번째로 일반적인 바이너리들은 CPU 에서 직렬로 실행된다. 멀티 스레드 기능을 따로 쓰지 않는한 말이다. 하지만 &lt;em&gt;Shader&lt;/em&gt; 는 기본적으로 병렬로 실행된다. 아래 그림을 보자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/cpucore_vs_gpucore.jpg&quot; alt=&quot;CPU core vs GPU core&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CPU 와 GPU 의 차이를 간단하게 보여주는 그림이다. 다만 위 그림이 전부는 아니니 간단하게 알고 넘어가도록 하자. 우리가 주목해야 할 것은 바로 &lt;em&gt;core&lt;/em&gt; 갯수의 차이다. 요즘의 CPU 는 &lt;em&gt;core&lt;/em&gt; 의 갯수가 많지 않다. 최근에 나온 &lt;a href=&quot;https://ark.intel.com/ko/products/126684/Intel-Core-i7-8700K-Processor-12M-Cache-up-to-4_70-GHz&quot;&gt;i7-8700k&lt;/a&gt; 를 보면 코어의 갯수가 6개 인것을 확인할 수 있다. 다만 OS 스케줄링이 있어서 실질적으로 실행되는 것은 &lt;em&gt;core&lt;/em&gt; 의 갯수에 엄격하게 제한되지는 않는다. 중요한 것은 개인용 PC 에 들어가는 CPU 는 아직은 &lt;em&gt;core&lt;/em&gt; 의 갯수가 10개를 넘어가지 않는다는 것이다. 반면에 실제 GPU 의 코어의 갯수를 꽤나 많다. 그림에서는 &lt;em&gt;hundreds of cores&lt;/em&gt;, 몇백개의 &lt;em&gt;core&lt;/em&gt; 라고 하지만 요즘 개인용 PC 에 들어가는 GPU 코어는 몇천개나(&lt;a href=&quot;https://www.geforce.co.uk/hardware/desktop-gpus/geforce-gtx-1080/specifications&quot;&gt;gtx1080&lt;/a&gt;) 된다. GPU 의 코어가 많은 이유는 간단하다. CPU 에서 돌아가는 프로그램에 비해 간단한 프로그램 바이너리(쉐이더 혹은 GPGPU 프로그램)를 동시에 실행하는게 최근의 GPU 가 쓰이는 목적이기 때문이다.&lt;/p&gt;

&lt;p&gt;일반적으로 CPU 에서 코딩하는 프로그램과 다르게 GPU 에서 실행되는 프로그램들은 이러한 병렬적인 실행 환경 때문에 특수한 사항들과 제약사항들이 존재한다. 퍼포먼스를 염두하고 프로그램을 코딩하다 보면 처음 경험하는 프로그래머는 조금 당황스러울 수도 있다.&lt;/p&gt;

&lt;h3&gt;쉐이더 파이프라인&lt;/h3&gt;

&lt;p&gt;GPU 의 여태까지의 주요한 역할은 기하학적(geometry) 성격을 띄고있는 데이터들을(mesh, vertex …) 이차원 이미지로 계산하여 보여주는 일이였다. 그렇게 3D 에셋 저작툴이나(3dsmax, maya, …) 게임에서 GPU 를 활용해 보다 많은 것들을 표현할 수 있게 해주었다. 우리가 이번에 살펴볼 것은 &lt;em&gt;Shader Model 5.0&lt;/em&gt; 의 쉐이더가 실행되는 단계다. 이 단계는 위에서 언급한 기하학적 성격을 띄고 있는 데이터를 이차원 이미지로 계산하는 단계를 나타낸 것이다. 아래 그림을 보자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/sm_5-0_pipeline.jpg&quot; alt=&quot;Shader Model 5_0 pipeline&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 그림에는 여러가지 항목들이 있다. &lt;em&gt;Programmable Shader&lt;/em&gt; 를 제외하면 전부 고정된 기능을 가진 단계로써 프로그래머가 완전히 제어를 할 수 없는 단계다. 우리가 살펴볼 것은 이름 끝에 &lt;em&gt;Shader&lt;/em&gt; 가 붙은 것들이다. 차례대로 &lt;em&gt;Vertex Shader&lt;/em&gt;, &lt;em&gt;Hull Shader&lt;/em&gt;, &lt;em&gt;Domain Shader&lt;/em&gt;, &lt;em&gt;Geometry Shader&lt;/em&gt;, &lt;em&gt;Pixel Shader&lt;/em&gt; 가 있다. 위의 단계들은 두가지로 분류할 수 있다. &lt;em&gt;Geometry Stage&lt;/em&gt; 와 &lt;em&gt;Rasterizer Stage&lt;/em&gt; 다. &lt;em&gt;Geometry Stage&lt;/em&gt; 는 일반적인 3D 상의 위치나 벡터를 가지고 있는 데이터를 처리하는 단계를 말한다. 위 그림에서는 &lt;em&gt;Rasterizer&lt;/em&gt; 전 까지의 단계를 뜻한다. &lt;em&gt;Rasterizer Stage&lt;/em&gt; 는 2D 이미지로 처리된 상태에서 데이터를 처리하는 단계를 말한다. &lt;em&gt;Rasterizer&lt;/em&gt; 단계 부터 오른쪽 끝까지의 단계다. 각 단계에 대한 자세한 설명은 해당 글에서 하겠다.&lt;/p&gt;

&lt;p&gt;쉐이더 파이프라인을 알고 있어야 여러 이론들을 구현할 수 있다. 쉐이더를 다루려면 이 쉐이더 파이프라인을 아는 것은 필수라고 할 수 있겠다.&lt;/p&gt;

&lt;h3&gt;각종 버퍼들의 활용&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;Shader Model 4.0&lt;/em&gt; 과 &lt;em&gt;Shader Model 5.0&lt;/em&gt; 를 통해 여러 버퍼들을 사용하여 &lt;em&gt;Shader&lt;/em&gt; 안에서 돌고도는 &lt;em&gt;varying data&lt;/em&gt; (쉐이더들의 파라미터들) 와 함께 응용을 할 수 있게 되었다. 이는 함수 한개씩을 파라미터와 반환값만 바꾸면서 코딩하는 환경에서 참조할 전역 변수를 만들어주어 훨씬 더 많은 데이터들을 접근할 수 있게 해준것이다.&lt;/p&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="render" />
      
        <category term="shader" />
      
        <category term="hlsl" />
      

      

      
        <summary type="html">쉐이더 프로그래밍 환경 Programmable Shader 들을 정리하기 위해 각 쉐이더별로 한개씩 글을 써보기로 했다. 그 전에 미리 알아야될 것들에 대해 알아보려고 한다. 각각의 Shader 들은 코더의 입장에서 바라보았을 때는 단지 몇개의 파라미터를 받고 값을 반환하는 함수들이다. 하지만 일반적으로 알고있는 함수들과는 조금 다르게 실행된다. 첫번째로 일반적인 바이너리들은 CPU 에서 직렬로 실행된다. 멀티 스레드 기능을 따로 쓰지 않는한 말이다. 하지만 Shader 는 기본적으로 병렬로 실행된다. 아래 그림을 보자. CPU 와 GPU 의 차이를 간단하게 보여주는 그림이다. 다만 위 그림이 전부는 아니니 간단하게 알고 넘어가도록 하자. 우리가 주목해야 할 것은 바로 core 갯수의 차이다. 요즘의 CPU 는 core 의 갯수가 많지 않다. 최근에 나온 i7-8700k 를 보면 코어의 갯수가 6개 인것을 확인할 수 있다. 다만 OS 스케줄링이 있어서 실질적으로 실행되는 것은 core 의 갯수에 엄격하게 제한되지는 않는다. 중요한 것은 개인용 PC 에 들어가는 CPU 는 아직은 core 의 갯수가 10개를 넘어가지 않는다는 것이다. 반면에 실제 GPU 의 코어의 갯수를 꽤나 많다. 그림에서는 hundreds of cores, 몇백개의 core 라고 하지만 요즘 개인용 PC 에 들어가는 GPU 코어는 몇천개나(gtx1080) 된다. GPU 의 코어가 많은 이유는 간단하다. CPU 에서 돌아가는 프로그램에 비해 간단한 프로그램 바이너리(쉐이더 혹은 GPGPU 프로그램)를 동시에 실행하는게 최근의 GPU 가 쓰이는 목적이기 때문이다. 일반적으로 CPU 에서 코딩하는 프로그램과 다르게 GPU 에서 실행되는 프로그램들은 이러한 병렬적인 실행 환경 때문에 특수한 사항들과 제약사항들이 존재한다. 퍼포먼스를 염두하고 프로그램을 코딩하다 보면 처음 경험하는 프로그래머는 조금 당황스러울 수도 있다. 쉐이더 파이프라인 GPU 의 여태까지의 주요한 역할은 기하학적(geometry) 성격을 띄고있는 데이터들을(mesh, vertex …) 이차원 이미지로 계산하여 보여주는 일이였다. 그렇게 3D 에셋 저작툴이나(3dsmax, maya, …) 게임에서 GPU 를 활용해 보다 많은 것들을 표현할 수 있게 해주었다. 우리가 이번에 살펴볼 것은 Shader Model 5.0 의 쉐이더가 실행되는 단계다. 이 단계는 위에서 언급한 기하학적 성격을 띄고 있는 데이터를 이차원 이미지로 계산하는 단계를 나타낸 것이다. 아래 그림을 보자. 이 그림에는 여러가지 항목들이 있다. Programmable Shader 를 제외하면 전부 고정된 기능을 가진 단계로써 프로그래머가 완전히 제어를 할 수 없는 단계다. 우리가 살펴볼 것은 이름 끝에 Shader 가 붙은 것들이다. 차례대로 Vertex Shader, Hull Shader, Domain Shader, Geometry Shader, Pixel Shader 가 있다. 위의 단계들은 두가지로 분류할 수 있다. Geometry Stage 와 Rasterizer Stage 다. Geometry Stage 는 일반적인 3D 상의 위치나 벡터를 가지고 있는 데이터를 처리하는 단계를 말한다. 위 그림에서는 Rasterizer 전 까지의 단계를 뜻한다. Rasterizer Stage 는 2D 이미지로 처리된 상태에서 데이터를 처리하는 단계를 말한다. Rasterizer 단계 부터 오른쪽 끝까지의 단계다. 각 단계에 대한 자세한 설명은 해당 글에서 하겠다. 쉐이더 파이프라인을 알고 있어야 여러 이론들을 구현할 수 있다. 쉐이더를 다루려면 이 쉐이더 파이프라인을 아는 것은 필수라고 할 수 있겠다. 각종 버퍼들의 활용 Shader Model 4.0 과 Shader Model 5.0 를 통해 여러 버퍼들을 사용하여 Shader 안에서 돌고도는 varying data (쉐이더들의 파라미터들) 와 함께 응용을 할 수 있게 되었다. 이는 함수 한개씩을 파라미터와 반환값만 바꾸면서 코딩하는 환경에서 참조할 전역 변수를 만들어주어 훨씬 더 많은 데이터들을 접근할 수 있게 해준것이다.</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Loop Attributes For Dynamic Branching</title>
      
      <link href="https://hrmrzizon.github.io/2017/10/26/loop-attributes-for-dynamic-branching/" rel="alternate" type="text/html" title="Loop Attributes For Dynamic Branching" />
      <published>2017-10-26T00:00:00+00:00</published>
      <updated>2017-10-26T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2017/10/26/loop-attributes-for-dynamic-branching</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2017/10/26/loop-attributes-for-dynamic-branching/">&lt;p&gt;&lt;em&gt;Programmable Shader&lt;/em&gt; 를 작성할 때에는 한가지 유의해야 할 점이 있다. 이는 &lt;em&gt;Dynamic Branching&lt;/em&gt; 이라는 개념이다. &lt;em&gt;Dynamic Branching&lt;/em&gt; 은 조건 분기문이 &lt;em&gt;Programmable Shader&lt;/em&gt; 에서 사용될 때 나타나는 현상을 말한다. &lt;em&gt;Programmable Shader&lt;/em&gt; 는 직렬이 아닌 병렬로 실행되기 때문에 나타나는 특성이다. 반복문에서도 조건 분기를 사용한다. 간단한 아래 코드를 보자.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-hlsl&quot;&gt;int i = 0;
while(i &amp;lt; 5)
{
  i++;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;위 코드는 프로그래밍을 입문할때 볼 수 있는 코드다. 중요한 것은 &lt;em&gt;while&lt;/em&gt; 단어가 있는 줄에 있는 조건 식이다. &lt;em&gt;(i &amp;lt; 5)&lt;/em&gt; 조건식 때문에 &lt;em&gt;Dynamic Branching&lt;/em&gt; 이 발생한다. 이 &lt;em&gt;Dynamic Branching&lt;/em&gt; 을 명시적으로 없에거나 만들기 위해 &lt;em&gt;hlsl&lt;/em&gt; 에서 &lt;em&gt;attribute&lt;/em&gt; 를 지원한다. 아래를 보자.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-hlsl&quot;&gt;[Attribute] for ( Initializer; Conditional; Iterator )
{
  Statement Block;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;해당 구문은 &lt;a href=&quot;https://msdn.microsoft.com/ko-kr/library/windows/desktop/bb509602.aspx&quot;&gt;MSDN : for Statement&lt;/a&gt; 에서 가져왔다. 일반적으로 프로그래머이 정말 많이본 &lt;em&gt;for&lt;/em&gt; 반복문이다. 우리가 봐야할 것은 &lt;em&gt;for&lt;/em&gt; 구문 왼쪽의 &lt;em&gt;[Attribute]&lt;/em&gt; 라는 구문이다. 이 부분에는 총 4가지의 옵션을 넣을 수 있는데, 이 글에서 언급할 &lt;em&gt;[Attribute]&lt;/em&gt; 는 두가지다. &lt;em&gt;unroll&lt;/em&gt; 과 &lt;em&gt;loop&lt;/em&gt; 이 두가지이다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;hlsl&lt;/em&gt; 로 정상적인 반복문 실행을 하게되면, 매번 반복을 할때 마다 조건식을 검사하게 되고, 해당 반복문의 범위를 마음대로 조정하여 코딩을 할 수 있다. 다만 조건식의 범위가 매번 달라진다면 &lt;em&gt;Dynamic Branching&lt;/em&gt; 이 발생하게 된다. 그리고 반복문이 매번 &lt;em&gt;Programmable Shader&lt;/em&gt; 가 실행될 때 상수로 반복을 한다면 쉐이더 컴파일러는 최적화를 위해 특정한 행동을 하게 된다. 아래 코드를 보자.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-hlsl&quot;&gt;for(int i = 0; i &amp;lt; 5; i++)
{
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;위의 코드는 루프를 다섯번 실행시키는 코드다. 따로 안에 인덱스 &lt;em&gt;i&lt;/em&gt; 를 건드리지 않는다면 쉐이더 컴파일러는 컴파일 시점에 최적화를 한다. 이를 &lt;em&gt;unroll&lt;/em&gt; 이라고 부를 수 있는데, 실행할 반복문을 반복문으로 해석하는게 아닌 5번 연속해서 같은 행동을 하게 하는 것이다. 조건 자체도 없어지고 그저 인덱스를 풀어쓰게 된다. 이는 상수(constant)로 반복문을 제어하면 쉐이더 컴파일러가 알아서 해주기 때문에 신경써주지 않아도 된다. 다만 &lt;em&gt;unroll&lt;/em&gt; 이라는 키워드를 써서 바뀔 때는 변수를 사용해 반복문을 제어할 때다. 변수를 사용하면 컴파일 시점에서는 추측할 수 없기 때문에 암시적으로 &lt;em&gt;unroll&lt;/em&gt; 을 할 수 없다. 이 때 &lt;em&gt;unroll&lt;/em&gt; 키워드를 사용하여 제어할 수 있다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-hlsl&quot;&gt;int count = ...;
[unroll(5)]
for(int i = 0; i &amp;lt; count; i++)
{
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;또한 암시적으로 &lt;em&gt;unroll&lt;/em&gt; 된 반복문을 명시적으로 반복문으로 실행되게 할 수도 있다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-hlsl&quot;&gt;[loop]
for(int i = 0; i &amp;lt; 5; i++)
{
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h1&gt;참조 자료&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://msdn.microsoft.com/ko-kr/library/windows/desktop/bb509602.aspx&quot;&gt;MSDN : for Statement&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.gamedev.net/forums/topic/649408-can-someone-explain-loop-and-unroll-to-me/&quot;&gt;GameDev : Can someone explain [loop] and [unroll] to me?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.gamedev.net/forums/topic/543541-hlsl-warning-gradient-based-operations-must-be-moved-out-of-flow-control-to-prevent/&quot;&gt;GameDev : HLSL warning: Gradient-based operations must be moved out of flow control to prevent
 &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="render" />
      
        <category term="shader" />
      
        <category term="hlsl" />
      

      

      
        <summary type="html">Programmable Shader 를 작성할 때에는 한가지 유의해야 할 점이 있다. 이는 Dynamic Branching 이라는 개념이다. Dynamic Branching 은 조건 분기문이 Programmable Shader 에서 사용될 때 나타나는 현상을 말한다. Programmable Shader 는 직렬이 아닌 병렬로 실행되기 때문에 나타나는 특성이다. 반복문에서도 조건 분기를 사용한다. 간단한 아래 코드를 보자. int i = 0; while(i &amp;lt; 5) { i++; } 위 코드는 프로그래밍을 입문할때 볼 수 있는 코드다. 중요한 것은 while 단어가 있는 줄에 있는 조건 식이다. (i &amp;lt; 5) 조건식 때문에 Dynamic Branching 이 발생한다. 이 Dynamic Branching 을 명시적으로 없에거나 만들기 위해 hlsl 에서 attribute 를 지원한다. 아래를 보자. [Attribute] for ( Initializer; Conditional; Iterator ) { Statement Block; } 해당 구문은 MSDN : for Statement 에서 가져왔다. 일반적으로 프로그래머이 정말 많이본 for 반복문이다. 우리가 봐야할 것은 for 구문 왼쪽의 [Attribute] 라는 구문이다. 이 부분에는 총 4가지의 옵션을 넣을 수 있는데, 이 글에서 언급할 [Attribute] 는 두가지다. unroll 과 loop 이 두가지이다. hlsl 로 정상적인 반복문 실행을 하게되면, 매번 반복을 할때 마다 조건식을 검사하게 되고, 해당 반복문의 범위를 마음대로 조정하여 코딩을 할 수 있다. 다만 조건식의 범위가 매번 달라진다면 Dynamic Branching 이 발생하게 된다. 그리고 반복문이 매번 Programmable Shader 가 실행될 때 상수로 반복을 한다면 쉐이더 컴파일러는 최적화를 위해 특정한 행동을 하게 된다. 아래 코드를 보자. for(int i = 0; i &amp;lt; 5; i++) { } 위의 코드는 루프를 다섯번 실행시키는 코드다. 따로 안에 인덱스 i 를 건드리지 않는다면 쉐이더 컴파일러는 컴파일 시점에 최적화를 한다. 이를 unroll 이라고 부를 수 있는데, 실행할 반복문을 반복문으로 해석하는게 아닌 5번 연속해서 같은 행동을 하게 하는 것이다. 조건 자체도 없어지고 그저 인덱스를 풀어쓰게 된다. 이는 상수(constant)로 반복문을 제어하면 쉐이더 컴파일러가 알아서 해주기 때문에 신경써주지 않아도 된다. 다만 unroll 이라는 키워드를 써서 바뀔 때는 변수를 사용해 반복문을 제어할 때다. 변수를 사용하면 컴파일 시점에서는 추측할 수 없기 때문에 암시적으로 unroll 을 할 수 없다. 이 때 unroll 키워드를 사용하여 제어할 수 있다. int count = ...; [unroll(5)] for(int i = 0; i &amp;lt; count; i++) { } 또한 암시적으로 unroll 된 반복문을 명시적으로 반복문으로 실행되게 할 수도 있다. [loop] for(int i = 0; i &amp;lt; 5; i++) { } 참조 자료 MSDN : for Statement GameDev : Can someone explain [loop] and [unroll] to me? GameDev : HLSL warning: Gradient-based operations must be moved out of flow control to prevent</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Introduce Of Wave Programming</title>
      
      <link href="https://hrmrzizon.github.io/2017/10/16/introduce-of-wave-programming/" rel="alternate" type="text/html" title="Introduce Of Wave Programming" />
      <published>2017-10-16T00:00:00+00:00</published>
      <updated>2017-10-16T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2017/10/16/introduce-of-wave-programming</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2017/10/16/introduce-of-wave-programming/">&lt;p&gt;Windows 10 Fall Creators Update 가 나오면서 &lt;em&gt;Shader Model 6.0&lt;/em&gt; 이 추가되었다. 여태까지의 &lt;em&gt;Shader Model&lt;/em&gt; 업데이트는 대부분 DirectX 버젼이 올라가면서 같이 업데이트 된 경우가 많으나 이번의 &lt;em&gt;Shader Model 6.0&lt;/em&gt; 은 따로 업데이트 되었다. &lt;em&gt;Shader Model 6.0&lt;/em&gt; 에서의 가장 큰 기능 추가는 당연히 &lt;em&gt;Wave Intrisic&lt;/em&gt; 이라고 할 수 있겠다. &lt;em&gt;Wave Intrisic&lt;/em&gt; 을 제외하면 &lt;em&gt;Shader Model 6.0&lt;/em&gt; 은 바뀐게 없다.&lt;/p&gt;

&lt;p&gt;여태까지의 HLSL 을 사용한 쉐이더 작성은 거의 대부분 &lt;em&gt;Single-Threading&lt;/em&gt; 으로 작동되었다. &lt;em&gt;Pixel Shader&lt;/em&gt; 에서 ddx, ddy instrisic 을 사용하여 Gradient 데이터를 가져올 수 있긴 했지만 이 것을 제외하면 거의 없었다고 보면 되겠다. 그래서 &lt;em&gt;Shader Model 6.0&lt;/em&gt; 에서는 다른 &lt;em&gt;Thread&lt;/em&gt; 와 인터렉션 할 수 있는 &lt;em&gt;Wave Intrisic&lt;/em&gt; 을 지원한다. &lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/mt733232.aspx&quot;&gt;MSDN : HLSL Shader Model 6.0&lt;/a&gt; 을 살펴보면 알겠지만 단순한 API 들을 제공하는 것이다. 하지만 내부에서 동작하는 것은 조금 다르다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/mt733232.aspx&quot;&gt;MSDN : HLSL Shader Model 6.0&lt;/a&gt; 에서 나온 용어에 대한 설명이 필요하다. &lt;em&gt;Lane&lt;/em&gt; 은 일반적으로 생각되는 한개의 &lt;em&gt;Thread&lt;/em&gt; 가 실행되는 것이다. &lt;em&gt;Shader Model 6.0&lt;/em&gt; 이전의 쉐이더 모델은 단순히 &lt;em&gt;Lane&lt;/em&gt; 개념 안에서 코딩을 해야 했다. &lt;em&gt;Lane&lt;/em&gt; 은 상황에 따라 실행되고 있는 상태일 수도 있고, 쉬고 있는 상태일 수도 있다. &lt;em&gt;Wave Intrisic&lt;/em&gt; 을 사용해 이를 각각의 &lt;em&gt;Lane&lt;/em&gt; 에서도 알 수 있다. &lt;em&gt;Wave&lt;/em&gt; 는 GPU 에서 실행되는 &lt;em&gt;Lane&lt;/em&gt; 의 묶음을 뜻한다. 즉 여러개의 &lt;em&gt;Lane&lt;/em&gt; 이라고 할 수 있겠다. 같은 &lt;em&gt;Wave&lt;/em&gt; 안의 &lt;em&gt;Lane&lt;/em&gt; 들은 &lt;em&gt;Barrier&lt;/em&gt; 라는게 없다. 필자가 알고 있는 &lt;em&gt;Barrier&lt;/em&gt; 는 &lt;em&gt;Memory Barrier&lt;/em&gt; 인데, 이는 &lt;em&gt;Thread&lt;/em&gt;(&lt;em&gt;Lane&lt;/em&gt;)끼리의 같은 메모리에 접근하는 것에 대한 동기화를 위해 있는 개념이다. 동기화를 위한 &lt;em&gt;Barrier&lt;/em&gt; 는 속도를 늦출 수 밖에 없다. 하지만 &lt;em&gt;Wave&lt;/em&gt; 로 묶여진 &lt;em&gt;Lane&lt;/em&gt; 들은 서로 &lt;em&gt;Barrier&lt;/em&gt; 가 명시적으로 존재하지 않기 때문에 &lt;em&gt;Wave&lt;/em&gt; 별로 빠른 메모리 접근이 가능하다는 것이다. &lt;em&gt;Wave&lt;/em&gt; 는 &lt;em&gt;Warp&lt;/em&gt;, &lt;em&gt;WaveFront&lt;/em&gt; 라고도 불리울 수 있다고 한다.&lt;/p&gt;

&lt;p&gt;그리고 이 API 들을 통해 약간의 드라이버 내부를 엿볼 수 있다. &lt;em&gt;Pixel Shader&lt;/em&gt; 에서 &lt;em&gt;Render Lane&lt;/em&gt; 과 &lt;em&gt;Helper Lane&lt;/em&gt; 이 구분되어져 있는데, 이는 ddx,ddy 를 통해 픽셀의 Gradient 를 계산하는 것에 대한 보다 디테일한 개념을 생각할 수 있게 해준다. GPU 드라이버 시스템에서는 픽셀을 처리하기 위해 단순히 한개의 픽셀만 처리하는게 아닌 2x2 의 픽셀을 엮어 계산한다. 이를 MSDN 문서에서는 2x2 의 픽셀 뭉치를 &lt;em&gt;Quad&lt;/em&gt; 라고 명칭한다. &lt;em&gt;Quad&lt;/em&gt; 는 두가지 종류에 스레드가 실행된다. 하나는 우리가 잘 알고 있는 &lt;em&gt;Pixel Shader&lt;/em&gt; 를 실행하는 &lt;em&gt;Render Lane&lt;/em&gt; 이다. &lt;em&gt;Render Lane&lt;/em&gt; 은 화면에 보여주는 색을 결과로 내놓게 된다. 그리고 나머지 한가지는 &lt;em&gt;Helper Lane&lt;/em&gt; 인데, 이는 Pixel 별로 Gradient 를 계산하기 위해 실행되는 &lt;em&gt;Lane&lt;/em&gt; 으로써 아무런 결과를 내놓지 않고 단순히 계산을 위한 &lt;em&gt;Lane&lt;/em&gt; 이다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Shader Model 6.0&lt;/em&gt; 은 DirectX12 과 Vulkan 에서 지원한다. DirectX 에서는 &lt;em&gt;Pixel Shader&lt;/em&gt; 와 &lt;em&gt;Computer Shader&lt;/em&gt; 에서 지원한다. Vulkan 에서는 모든 쉐이더 단계에서 지원한다. 그래픽 카드 벤더별로 조금씩 다른게 있으니 &lt;a href=&quot;http://32ipi028l5q82yhj72224m8j.wpengine.netdna-cdn.com/wp-content/uploads/2017/07/GDC2017-Wave-Programming-D3D12-Vulkan.pdf&quot;&gt;GDCVault(GDC 2017) : Wave Programming D3D12 Vulkan &lt;/a&gt; 에서 참고 바란다.&lt;/p&gt;

&lt;p&gt;이 API 는 여러 쓰레드들 끼리 쉽게 협력하여 보다 효율적인 쉐이더 병렬 프로그래밍을 가능하게 해줄듯하다. 다만 &lt;em&gt;Shader Model 5.0&lt;/em&gt; 에서 소개된 &lt;em&gt;ComputeShader&lt;/em&gt; 만큼의 임팩트는 없다. 패러다임의 아주 큰 변화는 없다는 뜻이다. DirectX12 가 지향하는 드라이버 시스템에서의 부담을 줄이는 것과 &lt;em&gt;Shader Model 6.0&lt;/em&gt; 은 서로 방향이 비슷하다고 생각된다.&lt;/p&gt;

&lt;h1&gt;참조 자료&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.gdcvault.com/play/1024732/Advanced-Graphics-Tech-D3D12-and&quot;&gt;GDCVault(GDC 2017) : D3D12 &amp;amp; Vulkan Done Right&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://32ipi028l5q82yhj72224m8j.wpengine.netdna-cdn.com/wp-content/uploads/2017/07/GDC2017-Wave-Programming-D3D12-Vulkan.pdf&quot;&gt;GDCVault(GDC 2017) : Wave Programming D3D12 Vulkan &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://msdn.microsoft.com/en-us/library/windows/desktop/mt733232.aspx&quot;&gt;MSDN : HLSL Shader Model 6.0&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://optocrypto.com/2017/09/20/microsofts-program-shader-model-6-0-completed/&quot;&gt;Optocrypto : Microsoft’s first example program for shader model 6.0 was completed&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="render" />
      
        <category term="shader" />
      

      

      
        <summary type="html">Windows 10 Fall Creators Update 가 나오면서 Shader Model 6.0 이 추가되었다. 여태까지의 Shader Model 업데이트는 대부분 DirectX 버젼이 올라가면서 같이 업데이트 된 경우가 많으나 이번의 Shader Model 6.0 은 따로 업데이트 되었다. Shader Model 6.0 에서의 가장 큰 기능 추가는 당연히 Wave Intrisic 이라고 할 수 있겠다. Wave Intrisic 을 제외하면 Shader Model 6.0 은 바뀐게 없다. 여태까지의 HLSL 을 사용한 쉐이더 작성은 거의 대부분 Single-Threading 으로 작동되었다. Pixel Shader 에서 ddx, ddy instrisic 을 사용하여 Gradient 데이터를 가져올 수 있긴 했지만 이 것을 제외하면 거의 없었다고 보면 되겠다. 그래서 Shader Model 6.0 에서는 다른 Thread 와 인터렉션 할 수 있는 Wave Intrisic 을 지원한다. MSDN : HLSL Shader Model 6.0 을 살펴보면 알겠지만 단순한 API 들을 제공하는 것이다. 하지만 내부에서 동작하는 것은 조금 다르다. MSDN : HLSL Shader Model 6.0 에서 나온 용어에 대한 설명이 필요하다. Lane 은 일반적으로 생각되는 한개의 Thread 가 실행되는 것이다. Shader Model 6.0 이전의 쉐이더 모델은 단순히 Lane 개념 안에서 코딩을 해야 했다. Lane 은 상황에 따라 실행되고 있는 상태일 수도 있고, 쉬고 있는 상태일 수도 있다. Wave Intrisic 을 사용해 이를 각각의 Lane 에서도 알 수 있다. Wave 는 GPU 에서 실행되는 Lane 의 묶음을 뜻한다. 즉 여러개의 Lane 이라고 할 수 있겠다. 같은 Wave 안의 Lane 들은 Barrier 라는게 없다. 필자가 알고 있는 Barrier 는 Memory Barrier 인데, 이는 Thread(Lane)끼리의 같은 메모리에 접근하는 것에 대한 동기화를 위해 있는 개념이다. 동기화를 위한 Barrier 는 속도를 늦출 수 밖에 없다. 하지만 Wave 로 묶여진 Lane 들은 서로 Barrier 가 명시적으로 존재하지 않기 때문에 Wave 별로 빠른 메모리 접근이 가능하다는 것이다. Wave 는 Warp, WaveFront 라고도 불리울 수 있다고 한다. 그리고 이 API 들을 통해 약간의 드라이버 내부를 엿볼 수 있다. Pixel Shader 에서 Render Lane 과 Helper Lane 이 구분되어져 있는데, 이는 ddx,ddy 를 통해 픽셀의 Gradient 를 계산하는 것에 대한 보다 디테일한 개념을 생각할 수 있게 해준다. GPU 드라이버 시스템에서는 픽셀을 처리하기 위해 단순히 한개의 픽셀만 처리하는게 아닌 2x2 의 픽셀을 엮어 계산한다. 이를 MSDN 문서에서는 2x2 의 픽셀 뭉치를 Quad 라고 명칭한다. Quad 는 두가지 종류에 스레드가 실행된다. 하나는 우리가 잘 알고 있는 Pixel Shader 를 실행하는 Render Lane 이다. Render Lane 은 화면에 보여주는 색을 결과로 내놓게 된다. 그리고 나머지 한가지는 Helper Lane 인데, 이는 Pixel 별로 Gradient 를 계산하기 위해 실행되는 Lane 으로써 아무런 결과를 내놓지 않고 단순히 계산을 위한 Lane 이다. Shader Model 6.0 은 DirectX12 과 Vulkan 에서 지원한다. DirectX 에서는 Pixel Shader 와 Computer Shader 에서 지원한다. Vulkan 에서는 모든 쉐이더 단계에서 지원한다. 그래픽 카드 벤더별로 조금씩 다른게 있으니 GDCVault(GDC 2017) : Wave Programming D3D12 Vulkan 에서 참고 바란다. 이 API 는 여러 쓰레드들 끼리 쉽게 협력하여 보다 효율적인 쉐이더 병렬 프로그래밍을 가능하게 해줄듯하다. 다만 Shader Model 5.0 에서 소개된 ComputeShader 만큼의 임팩트는 없다. 패러다임의 아주 큰 변화는 없다는 뜻이다. DirectX12 가 지향하는 드라이버 시스템에서의 부담을 줄이는 것과 Shader Model 6.0 은 서로 방향이 비슷하다고 생각된다. 참조 자료 GDCVault(GDC 2017) : D3D12 &amp;amp; Vulkan Done Right GDCVault(GDC 2017) : Wave Programming D3D12 Vulkan MSDN : HLSL Shader Model 6.0 Optocrypto : Microsoft’s first example program for shader model 6.0 was completed</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Using Rendering To Cubemap</title>
      
      <link href="https://hrmrzizon.github.io/2017/09/29/using-rendering-to-cubemap/" rel="alternate" type="text/html" title="Using Rendering To Cubemap" />
      <published>2017-09-29T00:00:00+00:00</published>
      <updated>2017-09-29T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2017/09/29/using-rendering-to-cubemap</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2017/09/29/using-rendering-to-cubemap/">&lt;p&gt;&lt;a href=&quot;/2017/09/29/using-relplacement-shader/&quot;&gt;using replacement shader&lt;/a&gt; 에서 &lt;em&gt;Camera.RenderWithShader&lt;/em&gt; 와 같은 렌더링을 코드에서 직접해주면서 기능을 커스터마이징 할 수 있는 것을 살펴보았는데, 이 게시물에서는 비슷한 메서드인 &lt;em&gt;Camera.RenderToCubemap&lt;/em&gt; 에 대해서 알아볼 것이다.&lt;/p&gt;

&lt;p&gt;Unity 에서는 여러 렌더링 커스터마이징 기능을 제공하는데, 이 게시물에서는 그 중 하나인 &lt;em&gt;Camera.RenderToCubemap&lt;/em&gt; 에 대해서 알아볼 것이다. 일반적으로 &lt;em&gt;Cubemap&lt;/em&gt; 은 SkyBox 나 주변의 Irradiance 를 나타낼 때 쓴다. 다만 이를 직접 구현할 때의 문제점은 각 모서리별로 &lt;em&gt;Aliasing&lt;/em&gt; 이 일어나는 경우다. 매우 매끄러운 표면의 Specular 에서 &lt;em&gt;Aliasing&lt;/em&gt; 이 나타난 Irradiance 를 표현하면 굉장히 티가 많이 나기 때문에 이는 굉장히 신경써야할 문제다.&lt;/p&gt;

&lt;p&gt;그래서 Unity 에서는 &lt;em&gt;Cubemap&lt;/em&gt; 에 렌더링을 하는 기능인 &lt;em&gt;Camera.RenderToCubemap&lt;/em&gt; 을 지원한다. 이를 통해 할 수 있는 것은 실시간으로 &lt;em&gt;Cubemap&lt;/em&gt; 에 렌더링된 결과를 저장해 &lt;em&gt;Irradiance&lt;/em&gt; 의 소스로 쓰거나, 실시간으로 바뀌는 &lt;em&gt;Skybox&lt;/em&gt; 렌더링을 할 수도 있다. 사용 방법은 아래와 같다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C#&quot;&gt;RenderTexture cubmapRT = ...;
camera.RenderToCubemap(cubemapRT, 63);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Camera.RenderToCubemap&lt;/em&gt; 의 두번째로 들어가는 인자는 어떤 면을 그릴건지에 대한 비트마스크다. &lt;em&gt;Camera.RenderToCubemap&lt;/em&gt; 를 쓸때 주의할 점은 일부 하드웨어에서는 동작하지 않는 기능이라고 한다. 다만 특정한 하드웨어를 기술해 놓지않아서 추측하기는 어렵다. 단순히 추측할 수 있는 것은 MRT 를 지원하지 않거나 아니면 다른 &lt;em&gt;ComputeShader&lt;/em&gt; 같은 기능을 사용해 일부 하드웨어에서 안된다고 하는 정도 밖에 없다.&lt;/p&gt;

&lt;p&gt;위 예제에서는 RenderTexture 를 사용하였는데, 저렇게 코드에서 처리할 수도 있지만 CustomRenderTexture 를 통해 간편하게 처리할 수도 있다. CustomRenderTexture 는 업데이트 주기를 사용자 임의대로 정할 수 있으므로 꽤나 유용하게 쓰일 수 있다.&lt;/p&gt;

&lt;h1&gt;참조 자료&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.unity3d.com/kr/current/ScriptReference/Camera.RenderToCubemap.html&quot;&gt;Unity Reference : Camera.RenderToCubemap&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="unity" />
      
        <category term="render" />
      

      

      
        <summary type="html">using replacement shader 에서 Camera.RenderWithShader 와 같은 렌더링을 코드에서 직접해주면서 기능을 커스터마이징 할 수 있는 것을 살펴보았는데, 이 게시물에서는 비슷한 메서드인 Camera.RenderToCubemap 에 대해서 알아볼 것이다. Unity 에서는 여러 렌더링 커스터마이징 기능을 제공하는데, 이 게시물에서는 그 중 하나인 Camera.RenderToCubemap 에 대해서 알아볼 것이다. 일반적으로 Cubemap 은 SkyBox 나 주변의 Irradiance 를 나타낼 때 쓴다. 다만 이를 직접 구현할 때의 문제점은 각 모서리별로 Aliasing 이 일어나는 경우다. 매우 매끄러운 표면의 Specular 에서 Aliasing 이 나타난 Irradiance 를 표현하면 굉장히 티가 많이 나기 때문에 이는 굉장히 신경써야할 문제다. 그래서 Unity 에서는 Cubemap 에 렌더링을 하는 기능인 Camera.RenderToCubemap 을 지원한다. 이를 통해 할 수 있는 것은 실시간으로 Cubemap 에 렌더링된 결과를 저장해 Irradiance 의 소스로 쓰거나, 실시간으로 바뀌는 Skybox 렌더링을 할 수도 있다. 사용 방법은 아래와 같다. RenderTexture cubmapRT = ...; camera.RenderToCubemap(cubemapRT, 63); Camera.RenderToCubemap 의 두번째로 들어가는 인자는 어떤 면을 그릴건지에 대한 비트마스크다. Camera.RenderToCubemap 를 쓸때 주의할 점은 일부 하드웨어에서는 동작하지 않는 기능이라고 한다. 다만 특정한 하드웨어를 기술해 놓지않아서 추측하기는 어렵다. 단순히 추측할 수 있는 것은 MRT 를 지원하지 않거나 아니면 다른 ComputeShader 같은 기능을 사용해 일부 하드웨어에서 안된다고 하는 정도 밖에 없다. 위 예제에서는 RenderTexture 를 사용하였는데, 저렇게 코드에서 처리할 수도 있지만 CustomRenderTexture 를 통해 간편하게 처리할 수도 있다. CustomRenderTexture 는 업데이트 주기를 사용자 임의대로 정할 수 있으므로 꽤나 유용하게 쓰일 수 있다. 참조 자료 Unity Reference : Camera.RenderToCubemap</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Using Relplacement Shader</title>
      
      <link href="https://hrmrzizon.github.io/2017/09/29/using-relplacement-shader/" rel="alternate" type="text/html" title="Using Relplacement Shader" />
      <published>2017-09-29T00:00:00+00:00</published>
      <updated>2017-09-29T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2017/09/29/using-relplacement-shader</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2017/09/29/using-relplacement-shader/">&lt;p&gt;Unity 는 &lt;em&gt;Replacement Shader&lt;/em&gt; 라는 렌더링 기능을 지원한다. 이는 Unity 가 Rendering 기능에서 지원하는 약간 Hack 한 테크닉이며 이 기능을 잘 사용하면 쉐이더를 바꿔치기 해서 재미있는 것들을 할 수 있다. &lt;em&gt;Replacement Shader&lt;/em&gt; 는 렌더링할 MeshRenderer 들이 가지고 있는 &lt;strong&gt;Material&lt;/strong&gt; 의 Shader 를 사용자가 원하는 것으로 바꾸는 기능이다. 이 기능을 통해 그림자 같은 여러 부가적인 처리를 할 수 있다.&lt;/p&gt;

&lt;p&gt;사용하는 방법은 아래와 같다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C#&quot;&gt;Shader shader = Shader.Find(&quot;CustomShaderName&quot;);
string replacementTag = &quot;replace&quot;;

// tag is optional. if dont need tag, insert null.
camera.RenderWithShader(shader, replacementTag);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;위의 간단한 예제는 &lt;em&gt;Replacement Shader&lt;/em&gt; 를 사용해 한번 그려주는 예제다. 단순히 &lt;em&gt;Camera.RenderWithShader&lt;/em&gt; 를 사용하기 때문에 직접 값을 컨트롤할 때 사용하기 좋다. &lt;em&gt;Replacement Shader&lt;/em&gt; 를 영구적으로 세팅하여 자동으로 그려주면 아래와 같이 하면된다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C#&quot;&gt;Shader shader = Shader.Find(&quot;CustomShaderName&quot;);
string replacementTag = &quot;replace&quot;;

// tag is optional. if dont need tag, insert null.
camera.SetReplacementShader(shader, replacementTag);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;사용 방법은 굉장히 단순하다. 다만 이 &lt;em&gt;Replacement Shader&lt;/em&gt; 기능에서 중요한 것은 쉐이더를 단순히 치환하는 것만 포인트가 아니다. 치환된 쉐이더들은 기존 &lt;strong&gt;Material&lt;/strong&gt; 이 가지고 있던 데이터들과 쉐이더 코드에서 이름만 똑같이 맞추어주면 자동으로 데이터들이 쉐이더로 들어온다. 즉 쉐이더를 갈아치우지 않고도 데이터를 공유할 수 있는 것이다. 이는 Unity 의 렌더링에서 굉장히 강력한 시스템으로 초기에는 이해하기도 힘들고 잔머리가 필요하지만 이를 잘 사용만 한다면 굉장히 유용하게 쓰일 수 있다.&lt;/p&gt;

&lt;p&gt;필자는 Github 에서 OIT 예제를 보면서 처음 보았다. &lt;a href=&quot;https://github.com/candycat1992/OIT_Lab&quot;&gt;Github : OIT_Lab&lt;/a&gt; 에서 OIT 를 처리하는 코드에서 구경할 수 있다. 또한 일본 Unity 지사에서 일하는 유명한 keijiro 의 &lt;a href=&quot;https://github.com/keijiro/Skinner&quot;&gt;Skinner&lt;/a&gt; 에서 위치를 처리하는데 쓰이기도 한다.&lt;/p&gt;

&lt;h1&gt;참조 자료&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.unity3d.com/kr/current/Manual/SL-ShaderReplacement.html&quot;&gt;Unity Reference : Replaced Shaders 에서의 렌더링&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="unity" />
      
        <category term="render" />
      

      

      
        <summary type="html">Unity 는 Replacement Shader 라는 렌더링 기능을 지원한다. 이는 Unity 가 Rendering 기능에서 지원하는 약간 Hack 한 테크닉이며 이 기능을 잘 사용하면 쉐이더를 바꿔치기 해서 재미있는 것들을 할 수 있다. Replacement Shader 는 렌더링할 MeshRenderer 들이 가지고 있는 Material 의 Shader 를 사용자가 원하는 것으로 바꾸는 기능이다. 이 기능을 통해 그림자 같은 여러 부가적인 처리를 할 수 있다. 사용하는 방법은 아래와 같다. Shader shader = Shader.Find(&quot;CustomShaderName&quot;); string replacementTag = &quot;replace&quot;; // tag is optional. if dont need tag, insert null. camera.RenderWithShader(shader, replacementTag); 위의 간단한 예제는 Replacement Shader 를 사용해 한번 그려주는 예제다. 단순히 Camera.RenderWithShader 를 사용하기 때문에 직접 값을 컨트롤할 때 사용하기 좋다. Replacement Shader 를 영구적으로 세팅하여 자동으로 그려주면 아래와 같이 하면된다. Shader shader = Shader.Find(&quot;CustomShaderName&quot;); string replacementTag = &quot;replace&quot;; // tag is optional. if dont need tag, insert null. camera.SetReplacementShader(shader, replacementTag); 사용 방법은 굉장히 단순하다. 다만 이 Replacement Shader 기능에서 중요한 것은 쉐이더를 단순히 치환하는 것만 포인트가 아니다. 치환된 쉐이더들은 기존 Material 이 가지고 있던 데이터들과 쉐이더 코드에서 이름만 똑같이 맞추어주면 자동으로 데이터들이 쉐이더로 들어온다. 즉 쉐이더를 갈아치우지 않고도 데이터를 공유할 수 있는 것이다. 이는 Unity 의 렌더링에서 굉장히 강력한 시스템으로 초기에는 이해하기도 힘들고 잔머리가 필요하지만 이를 잘 사용만 한다면 굉장히 유용하게 쓰일 수 있다. 필자는 Github 에서 OIT 예제를 보면서 처음 보았다. Github : OIT_Lab 에서 OIT 를 처리하는 코드에서 구경할 수 있다. 또한 일본 Unity 지사에서 일하는 유명한 keijiro 의 Skinner 에서 위치를 처리하는데 쓰이기도 한다. 참조 자료 Unity Reference : Replaced Shaders 에서의 렌더링</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Using Compute Buffer In Unity</title>
      
      <link href="https://hrmrzizon.github.io/2017/08/01/using-compute-buffer-in-unity/" rel="alternate" type="text/html" title="Using Compute Buffer In Unity" />
      <published>2017-08-01T00:00:00+00:00</published>
      <updated>2017-08-01T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2017/08/01/using-compute-buffer-in-unity</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2017/08/01/using-compute-buffer-in-unity/">&lt;p&gt;Unity 에서의 확실한 GPU Instancing 은 &lt;strong&gt;ComputeBuffer&lt;/strong&gt; 라는 구현체에서 시작될 것이다. 이 구현체는 &lt;strong&gt;UnityEngine.ComputeBuffer&lt;/strong&gt; 라는 Unity 의 구현체이며 하는 역할은 GPU 메모리를 사용하게 해주는 역할을 한다. &lt;strong&gt;ComputeBuffer&lt;/strong&gt; 는 &lt;strong&gt;ComputeShader&lt;/strong&gt; 와 함께 등장했다. &lt;strong&gt;ComputeShader&lt;/strong&gt; 에서 데이터를 읽고 쓰는것을 요구하기 때문에 Unity 는 GPU 메모리를 사용하는 컨테이너로서 &lt;strong&gt;ComputeBuffer&lt;/strong&gt; 를 구현해 놓았다. 하지만 이 &lt;strong&gt;ComputeBuffer&lt;/strong&gt; 는 &lt;strong&gt;ComputeShader&lt;/strong&gt; 뿐만아니라 일반 쉐이더에서도 폭넓게 사용가능하다. 이 말의 뜻은 우리가 생각하는 Unity 에서 지원하는 일반적인 메쉬 데이터를 사용하지 않아도 사용자가 직접 메쉬 데이터를 커스터마이징해서 사용할 수 있다는 이야기이다. 지원하는 플랫폼은 일반적으로 말하는 &lt;em&gt;Shader Model 5.0&lt;/em&gt; 이상이다. PC 플랫폼에서는 당연히 사용 가능하다.&lt;/p&gt;

&lt;p&gt;사용하는 방법 자체는 어렵지 않다. 스크립트에서 &lt;em&gt;size&lt;/em&gt; 와 &lt;em&gt;stride&lt;/em&gt; 를 설정해주고, 데이터의 배열을 만들어 GPU 메모리 안에 있는 데이터를 읽거나 쓸 수 있다. 메모리 단위에서 하는것처럼 보이기 때문에 크기와 타입은 맞춰주어야 한다. C# 에서는 &lt;strong&gt;System.Array&lt;/strong&gt; 형으로 넣어주니 형태에 주의하기 바란다. 방법은 아래와 같다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C#&quot;&gt;int dataLen = ...;  // length of data
int[] dataArray = new int[dataLen];

// record data in dataArray..

ComputeShader computeShader = ...;
ComptueBuffer dataBuffer = new ComputeBuffer(dataLen, sizeof(int));
dataBuffer.SetData(dataArray);

computeShader.SetBuffer(&quot;dataBuffer&quot;, dataBuffer);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;위 코드는 &lt;strong&gt;ComputeShader&lt;/strong&gt; 에서 &lt;strong&gt;ComputeBuffer&lt;/strong&gt; 를 사용하기 위해 세팅하는 코드다. 가장 맨처음에는 초기에 세팅할 정수 배열을 만들고, 그 다음 &lt;strong&gt;ComputeBuffer&lt;/strong&gt; 인스턴스를 생성한다. 생성자에서 넣어주는 인자는 데이터의 길이(&lt;em&gt;length&lt;/em&gt;)와 각 데이터별 크기(&lt;em&gt;stride&lt;/em&gt;)이다. 그 다음 같은 크기의 배열의 데이터를 GPU 메모리로 쓴다.(&lt;em&gt;write&lt;/em&gt;) 그리고 마지막으로 데이터가 세팅된 &lt;strong&gt;ComputeBuffer&lt;/strong&gt; 를 &lt;strong&gt;ComputeShader&lt;/strong&gt; 에 연결해준다. 이러면 &lt;strong&gt;ComputeShader&lt;/strong&gt; 코드에서 &lt;em&gt;dataBuffer&lt;/em&gt; 라는 변수명을 가진 변수에 &lt;strong&gt;ComputeBuffer&lt;/strong&gt; 가 연결된다. 아래에 &lt;strong&gt;ComputeShader&lt;/strong&gt; 코드가 있다.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-HLSL&quot;&gt;StructuredBuffer&amp;lt;int&amp;gt; dataBuffer;

[numthreads(8,8,1)]
void Process (uint3 id : SV_DispatchThreadID)
{
  ...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;맨 처음에 있는 &lt;em&gt;dataBuffer&lt;/em&gt; 에 연결된다. &lt;a href=&quot;{ post_url 2017-07-06-structured-buffer-vs-constant-buffer }&quot;&gt;StructuredBuffer vs ConstantBuffer&lt;/a&gt; 에서본 &lt;em&gt;StructuredBuffer&lt;/em&gt; 타입이 가능하다. 또한 &lt;em&gt;RWStructuredBuffer&lt;/em&gt;, &lt;em&gt;ConsumeStructuredBuffer&lt;/em&gt;, &lt;em&gt;AppendStructuredBuffer&lt;/em&gt; 가능하다. 다른 렌더러 쉐이더 코드에서도 사용가능하다. 그래서 일반적으로 고려되는 파이프라인은 아래와 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/data-process-pipeline.png&quot; alt=&quot;data process&quot; class=&quot;center-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;앞의 두가지 &lt;strong&gt;ComputeBuffer&lt;/strong&gt; 를 세팅하고 &lt;strong&gt;ComputeShader&lt;/strong&gt; 를 실행하는 코드는 대충 보았다, 뒷 부분의 &lt;strong&gt;ComputeBuffer&lt;/strong&gt; 를 통해 렌더링을 하는 것은 그다지 어렵지 않다. 중요한 것은 참신하게, 효율적으로 렌더링하는 것이다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/hrmrzizon/CustomSkinningExample&quot;&gt;Github : CustomSkinningExample&lt;/a&gt; 에서 스키닝의 계산을 &lt;strong&gt;ComputeShader&lt;/strong&gt; 로 넘겨서 계산한다. 또한 메시 데이터 전체를 &lt;strong&gt;ComputeBuffer&lt;/strong&gt; 로 넘겨서 렌더링하기 때문에 꽤나 괜찮은 예가 될것이다.&lt;/p&gt;

&lt;h2&gt;참조&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.unity3d.com/ScriptReference/ComputeBuffer.html&quot;&gt;Unity Reference : ComptuteBuffer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="unity" />
      
        <category term="shader" />
      
        <category term="gpuinstancing" />
      

      

      
        <summary type="html">Unity 에서의 확실한 GPU Instancing 은 ComputeBuffer 라는 구현체에서 시작될 것이다. 이 구현체는 UnityEngine.ComputeBuffer 라는 Unity 의 구현체이며 하는 역할은 GPU 메모리를 사용하게 해주는 역할을 한다. ComputeBuffer 는 ComputeShader 와 함께 등장했다. ComputeShader 에서 데이터를 읽고 쓰는것을 요구하기 때문에 Unity 는 GPU 메모리를 사용하는 컨테이너로서 ComputeBuffer 를 구현해 놓았다. 하지만 이 ComputeBuffer 는 ComputeShader 뿐만아니라 일반 쉐이더에서도 폭넓게 사용가능하다. 이 말의 뜻은 우리가 생각하는 Unity 에서 지원하는 일반적인 메쉬 데이터를 사용하지 않아도 사용자가 직접 메쉬 데이터를 커스터마이징해서 사용할 수 있다는 이야기이다. 지원하는 플랫폼은 일반적으로 말하는 Shader Model 5.0 이상이다. PC 플랫폼에서는 당연히 사용 가능하다. 사용하는 방법 자체는 어렵지 않다. 스크립트에서 size 와 stride 를 설정해주고, 데이터의 배열을 만들어 GPU 메모리 안에 있는 데이터를 읽거나 쓸 수 있다. 메모리 단위에서 하는것처럼 보이기 때문에 크기와 타입은 맞춰주어야 한다. C# 에서는 System.Array 형으로 넣어주니 형태에 주의하기 바란다. 방법은 아래와 같다. int dataLen = ...; // length of data int[] dataArray = new int[dataLen]; // record data in dataArray.. ComputeShader computeShader = ...; ComptueBuffer dataBuffer = new ComputeBuffer(dataLen, sizeof(int)); dataBuffer.SetData(dataArray); computeShader.SetBuffer(&quot;dataBuffer&quot;, dataBuffer); 위 코드는 ComputeShader 에서 ComputeBuffer 를 사용하기 위해 세팅하는 코드다. 가장 맨처음에는 초기에 세팅할 정수 배열을 만들고, 그 다음 ComputeBuffer 인스턴스를 생성한다. 생성자에서 넣어주는 인자는 데이터의 길이(length)와 각 데이터별 크기(stride)이다. 그 다음 같은 크기의 배열의 데이터를 GPU 메모리로 쓴다.(write) 그리고 마지막으로 데이터가 세팅된 ComputeBuffer 를 ComputeShader 에 연결해준다. 이러면 ComputeShader 코드에서 dataBuffer 라는 변수명을 가진 변수에 ComputeBuffer 가 연결된다. 아래에 ComputeShader 코드가 있다. StructuredBuffer&amp;lt;int&amp;gt; dataBuffer; [numthreads(8,8,1)] void Process (uint3 id : SV_DispatchThreadID) { ... } 맨 처음에 있는 dataBuffer 에 연결된다. StructuredBuffer vs ConstantBuffer 에서본 StructuredBuffer 타입이 가능하다. 또한 RWStructuredBuffer, ConsumeStructuredBuffer, AppendStructuredBuffer 가능하다. 다른 렌더러 쉐이더 코드에서도 사용가능하다. 그래서 일반적으로 고려되는 파이프라인은 아래와 같다. 앞의 두가지 ComputeBuffer 를 세팅하고 ComputeShader 를 실행하는 코드는 대충 보았다, 뒷 부분의 ComputeBuffer 를 통해 렌더링을 하는 것은 그다지 어렵지 않다. 중요한 것은 참신하게, 효율적으로 렌더링하는 것이다. Github : CustomSkinningExample 에서 스키닝의 계산을 ComputeShader 로 넘겨서 계산한다. 또한 메시 데이터 전체를 ComputeBuffer 로 넘겨서 렌더링하기 때문에 꽤나 괜찮은 예가 될것이다. 참조 Unity Reference : ComptuteBuffer</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Darboux Frame</title>
      
      <link href="https://hrmrzizon.github.io/2017/08/01/darboux-frame/" rel="alternate" type="text/html" title="Darboux Frame" />
      <published>2017-08-01T00:00:00+00:00</published>
      <updated>2017-08-01T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2017/08/01/darboux-frame</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2017/08/01/darboux-frame/">&lt;p&gt;여러 공간 법선 벡터(&lt;em&gt;tangent space normal&lt;/em&gt;, &lt;em&gt;object space normal&lt;/em&gt;)에 대하여 알아보던 도중 모르는 것이 하나있어 정리해볼겸 포스팅해보려 한다. &lt;em&gt;darboux frame&lt;/em&gt; 이라는 놈이다.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;p&gt;우선 &lt;em&gt;tangent space normal&lt;/em&gt; 과 &lt;em&gt;object space normal&lt;/em&gt; 에 대해서 설명해야 한다. 그래픽스에서는 빛을 표현하기 위해 노말벡터를 사용한다. 처음에 나온식은 매우 간단하다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;(빛의 방향 벡터) * (노말 벡터) = (빛이 표현하는 색의 범위(-1~1))&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;위의 벡터곱은 내적을 뜻한다. 이 식의 결과값은 반사광을 표현하는데 쓰인다. 일반적으로 말하는 &lt;em&gt;Specular&lt;/em&gt; 를 뜻한다. 하여튼 빛을 표현하는 것은 그래픽스에서는 굉장히 중요한 일이기 때문에 이 노말벡터를 어떻게 관리하는지가 엄청나게 중요하다. 그래서 여러 방법이 있는데 제일 많이 쓰이는건 &lt;em&gt;tangent space normal&lt;/em&gt; 이다. 그런데 &lt;em&gt;object space normal&lt;/em&gt; 은 갑자기 왜 튀어나왔느냐? 이유는 간단하다. 두개가 가장 비교가 많이 되는 방법이기 때문이다. &lt;em&gt;object space normal&lt;/em&gt; 은 굉장히 간단하다. 저장된 메시 데이터의 노말 벡터값이다. 기본 단위가 저장된 한 개체의 메시의 노말이기 때문에 &lt;em&gt;object spoce&lt;/em&gt; 라는 접두사가 붙은 것이다. 그래서 그런지 아래 그림에서 나오는 &lt;em&gt;object spoce normal&lt;/em&gt; 이 저장된 텍스쳐는 색이 굉장히 다양하다. 하지만 옆에 &lt;em&gt;tangent space normal&lt;/em&gt; 이 저장된 텍스쳐는 색이 거의 일정하다. 왜 그럴까? 우선 앞의 &lt;em&gt;object space normal&lt;/em&gt; 은 그냥 오브젝트 기준의 좌표계에서의 정점별 노말값을 저장한 데이터다. 하지만 &lt;em&gt;tangent space normal&lt;/em&gt; 은 모델에서 추출한 &lt;em&gt;tangent&lt;/em&gt; 값을 통해 &lt;em&gt;normal&lt;/em&gt; 값을 구하는 방법이다. &lt;a href=&quot;/2017/07/30/normal-tangent-binormal/&quot;&gt;normal tangent binormal&lt;/a&gt; 에서 설명했었다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/ssloy/tinyrenderer/gh-pages/img/06b-tangent-space/nm_textures.jpg&quot; alt=&quot;tangent-space vs objcet-space&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그런데 &lt;em&gt;tangent space normal&lt;/em&gt; 에서의 &lt;em&gt;tangent&lt;/em&gt; 가 뜻하는 것은 표면의 접선 값이다. 이렇게 표면을 기준으로 하는 것을 &lt;a href=&quot;https://en.wikipedia.org/wiki/Darboux_frame&quot;&gt;&lt;em&gt;darboux frame&lt;/em&gt;&lt;/a&gt; 이라고 한다. 프랑스 사람의 이름이라 한글로 읽으면 &lt;em&gt;다르부-프레임&lt;/em&gt; 이다. 위키에서는 &lt;em&gt;“프레네-세레 프레임”&lt;/em&gt; 이 표면 기하학에서 적용된 것이라 한다. 그만큼 대부분의 정의들이 &lt;em&gt;“프레네-세레 프레임”&lt;/em&gt; 과 매우 비슷하다. 다른 점은 곡선에서 표면으로 확장시켰다는 점이다.&lt;/p&gt;

&lt;h2&gt;참조&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ssloy/tinyrenderer/wiki/Lesson-6bis:-tangent-space-normal-mapping&quot;&gt;Github : tinyrenderer Wiki - tangent-space-normal-mapping&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Darboux_frame&quot;&gt;Wikipedia : darvoux frame&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="math" />
      
        <category term="study" />
      

      

      
        <summary type="html">여러 공간 법선 벡터(tangent space normal, object space normal)에 대하여 알아보던 도중 모르는 것이 하나있어 정리해볼겸 포스팅해보려 한다. darboux frame 이라는 놈이다.</summary>
      

      
      
    </entry>
  
  
  
    <entry>
      
      <title type="html">Normal Tangent Binormal</title>
      
      <link href="https://hrmrzizon.github.io/2017/07/30/normal-tangent-binormal/" rel="alternate" type="text/html" title="Normal Tangent Binormal" />
      <published>2017-07-30T00:00:00+00:00</published>
      <updated>2017-07-30T00:00:00+00:00</updated>
      <id>https://hrmrzizon.github.io/2017/07/30/normal-tangent-binormal</id>
      <content type="html" xml:base="https://hrmrzizon.github.io/2017/07/30/normal-tangent-binormal/">&lt;p&gt;Graphics 를 공부하다보면 노말(normal), 탄젠트(tangent), 바이노말(binormal) 를 굉장히 많이보게 된다. 특히 노말이라는 단어는 꽤나 많이 보인다. 보통은 어떤 역할을 하는 벡터앞에 이름을 붙여서 말한다. 아래와 같이 정리된다.&lt;/p&gt;

&lt;!-- more --&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;노말 벡터&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;탄젠트 벡터&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;바이노말 벡터&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;법선 벡터&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;접선 벡터&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;이중법선(또는 종법선)&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;하지만 이게 뭔지, 어디서 나온지에 대해서 설명한건 그다지 많이 본적이 없다. 물론 필자는 인터넷에 있는 레퍼런스만 보고 공부해서 그럴 수도 있다. 그래서 간단하게 지식의 뿌리만 살펴보려고 한다.&lt;/p&gt;

&lt;p&gt;이 세가지는 이름도 무시무시한 &lt;em&gt;미분기하학&lt;/em&gt; 에서 소개되는 &lt;em&gt;“프레네-세레 공식”(Frene-seret formula)&lt;/em&gt; 에서 정의된 것들이다. 정식 이름은 &lt;em&gt;“프레네-세레 프레임”&lt;/em&gt; 이라고 한다.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“프레네-세레 공식”&lt;/em&gt; 은 일반적으로 우리가 알고있는 실수 공간의 곡률이 있는 곡선 &lt;em&gt;r(t)&lt;/em&gt; 에서 유도된다. 중간에 있는 &lt;em&gt;t&lt;/em&gt; 는 시간을 나타내며 이는 이동한 거리, 곡선의 호 &lt;em&gt;s&lt;/em&gt; 로 매개화 시킨다고 한다. 그래서 그 곡선 &lt;em&gt;r(t(s))&lt;/em&gt; 를 미분해서 방향을 나타내는 말들이 우리가 평상시에 많이 들어왔던 노말, 탄젠트, 바이노말인 것이다.&lt;/p&gt;

&lt;p&gt;탄젠트는 곡선 공식을 그대로 미분한 값. 우리가 알고있는 일반적인 순간 가속도를 뜻한다. 이게 결국 방향을 나타내기 때문에 한글로는 비슷하게 접선벡터 라고 하는 듯하다. 그리고 현재 방향의 수직을 나타내는 노말은 탄젠트를 미분한 값을 정규화시켜서 표현한다. 필자는 이 값이 수학적으로 어떤 것을 나타내는지 몰라서 직관적으로 수식을보고 법선벡터인지 모르겠다. 마지막으로 바이노말은 탄젠트와 노말을 외적해서 구한다.&lt;/p&gt;

&lt;p&gt;여기까지는 &lt;em&gt;“프레네-세레 공식”&lt;/em&gt; 을 위한 정의들이다. 사실 &lt;em&gt;“프레네-세레 공식”&lt;/em&gt; 보다는 앞에서 말한  &lt;em&gt;“프레네-세레 프레임”&lt;/em&gt; 의 정의가 훨씬 더 많이 알려져 있다. 빛을 나타내기 위한 노말과 탄젠트를 그래픽스 이론에서는 끊임없이 보기 때문이다. &lt;em&gt;“프레네-세레 공식”&lt;/em&gt; 에 대한 자세한건 이 글에서는 쓰지 않겠다. 이 글을 쓴 이유는 우리가 흔히 쓰는 용어의 뿌리를 찾기위함이였다. (자세한 설명은 &lt;a href=&quot;https://ko.wikipedia.org/wiki/%ED%94%84%EB%A0%88%EB%84%A4-%EC%84%B8%EB%A0%88_%EA%B3%B5%EC%8B%9D&quot;&gt;위키피디아 : 프레네-세레 공식&lt;/a&gt;을 참조)&lt;/p&gt;

&lt;h2&gt;참조&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Frenet%E2%80%93Serret_formulas&quot;&gt;Wikipedia : Frene-seret formula&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://ko.wikipedia.org/wiki/%ED%94%84%EB%A0%88%EB%84%A4-%EC%84%B8%EB%A0%88_%EA%B3%B5%EC%8B%9D&quot;&gt;위키피디아 : 프레네-세레 공식&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://gall.dcinside.com/board/view/?id=mathematics&amp;amp;no=134445&quot;&gt;디시인사이드 수학 갤러리&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      
        <author>
            <name>Su-Hyeok Kim</name>
          
          
        </author>
      

      
        <category term="math" />
      
        <category term="study" />
      

      

      
        <summary type="html">Graphics 를 공부하다보면 노말(normal), 탄젠트(tangent), 바이노말(binormal) 를 굉장히 많이보게 된다. 특히 노말이라는 단어는 꽤나 많이 보인다. 보통은 어떤 역할을 하는 벡터앞에 이름을 붙여서 말한다. 아래와 같이 정리된다.</summary>
      

      
      
    </entry>
  
  
</feed>
